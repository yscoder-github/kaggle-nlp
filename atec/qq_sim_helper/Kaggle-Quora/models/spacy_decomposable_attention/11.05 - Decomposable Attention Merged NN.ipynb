{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\"\n",
    "\n",
    "from __future__ import division, unicode_literals, print_function\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import spacy\n",
    "import plac\n",
    "import ujson as json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import en_core_web_md\n",
    "import en_vectors_glove_md\n",
    "from tqdm import tqdm\n",
    "\n",
    "from pathlib import Path\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from sklearn.model_selection import train_test_split\n",
    "try:\n",
    "    import cPickle as pickle\n",
    "except ImportError:\n",
    "    import pickle\n",
    "\n",
    "from spacy_hook import get_embeddings, get_word_ids\n",
    "from spacy_hook import create_similarity_pipeline\n",
    "from decomposable_merge import build_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def spacy_encode(df, settings, shape):\n",
    "    print('Encoding data according to following settings:', settings, '\\n', shape)\n",
    "    train_texts1, train_texts2 = df['question1'], df['question2']\n",
    "    print(\"Loading spaCy\")\n",
    "    nlp = en_core_web_md.load()\n",
    "    assert nlp.path is not None\n",
    "    print(\"Processing texts...\")\n",
    "    encoded_data = []\n",
    "    for texts in tqdm((train_texts1, train_texts2)):\n",
    "        encoded_data.append(get_word_ids(list(nlp.pipe(texts, n_threads=10, batch_size=5000)),\n",
    "                         max_length=shape[0],\n",
    "                         rnn_encode=settings['gru_encode'],\n",
    "                         tree_truncate=settings['tree_truncate']))\n",
    "    q1, q2 = encoded_data\n",
    "    return q1, q2\n",
    "\n",
    "def create_mergevalidset(data_1, data_2, datafeats, labels):\n",
    "    np.random.seed(1234)\n",
    "    perm = np.random.permutation(len(data_1))\n",
    "    idx_train = perm[:int(len(data_1)*(1-VALIDATION_SPLIT))]\n",
    "    idx_val = perm[int(len(data_1)*(1-VALIDATION_SPLIT)):]\n",
    "    \n",
    "    data_1_train = np.vstack((data_1[idx_train], data_2[idx_train]))\n",
    "    data_2_train = np.vstack((data_2[idx_train], data_1[idx_train]))\n",
    "    labels_train = np.concatenate((labels[idx_train], labels[idx_train]))\n",
    "    dataf_train = np.vstack((datafeats[idx_train], datafeats[idx_train]))\n",
    "    \n",
    "    data_1_val = np.vstack((data_1[idx_val], data_2[idx_val]))\n",
    "    data_2_val = np.vstack((data_2[idx_val], data_1[idx_val]))\n",
    "    labels_val = np.concatenate((labels[idx_val], labels[idx_val]))\n",
    "    dataf_val = np.vstack((datafeats[idx_val], datafeats[idx_val]))\n",
    "    return data_1_train, data_2_train, dataf_train, labels_train, data_1_val, data_2_val, dataf_val, labels_val\n",
    "\n",
    "def create_stratified_split(data_1, data_2, datafeats, labels):\n",
    "    data1_tr, data1_val, y1_tr, y1_val = train_test_split(data_1, labels, stratify = labels,\n",
    "                                                        test_size = 0.2, random_state = 111)\n",
    "    data2_tr, data2_val, y2_tr, y2_val = train_test_split(data_2, labels, stratify = labels,\n",
    "                                                        test_size = 0.2, random_state = 111)\n",
    "    dataf_train, dataf_val, yf_tr, yf_val = train_test_split(datafeats, labels, stratify = labels,\n",
    "                                                        test_size = 0.2, random_state = 111)\n",
    "    return data1_tr, data2_tr, dataf_train, yf_tr, data1_val, data2_val, dataf_val, yf_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "qsrc = '/media/w/1c392724-ecf3-4615-8f3c-79368ec36380/DS Projects/Kaggle/Quora/data/features/lemmatized_fullclean/'\n",
    "\n",
    "q1 = np.load(qsrc + 'q1train_spacylemmat_fullclean_170len_treetrunc.npy')\n",
    "q2 = np.load(qsrc + 'q2train_spacylemmat_fullclean_170len_treetrunc.npy')\n",
    "X = pd.read_pickle('../../X_train_618cols_02065.pkl')\n",
    "y = pd.read_pickle('../../y_train.pkl').is_duplicate.values\n",
    "\n",
    "nlp = en_core_web_md.load()\n",
    "ncols = X.shape[1]\n",
    "\n",
    "tr_q1, tr_q2, tr_feats, y_tr, val_q1, val_q2, val_feats, y_val = create_stratified_split(q1, q2, X.values, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "settings = {\n",
    "    'lr': 0.0005,\n",
    "    'dropout': 0.2,\n",
    "    'batch_size': 128,\n",
    "    'nr_epoch': 100,\n",
    "    'tree_truncate': True,\n",
    "    'gru_encode': False,\n",
    "    }\n",
    "\n",
    "max_length = 170\n",
    "nr_hidden = 256\n",
    "ncols = X.shape[1]\n",
    "shape = (max_length, nr_hidden, 2, ncols)\n",
    "\n",
    "re_weight = True\n",
    "if re_weight:\n",
    "    class_weight = {0: 1.309028344, 1: 0.472001959}\n",
    "else:\n",
    "    class_weight = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 323432 samples, validate on 80858 samples\n",
      "Epoch 1/100\n",
      "323432/323432 [==============================] - 245s - loss: 0.3929 - acc: 0.6991 - val_loss: 0.5004 - val_acc: 0.7439\n",
      "Epoch 2/100\n",
      "323432/323432 [==============================] - 253s - loss: 0.3107 - acc: 0.7529 - val_loss: 0.4015 - val_acc: 0.8113\n",
      "Epoch 3/100\n",
      "323432/323432 [==============================] - 254s - loss: 0.2802 - acc: 0.7844 - val_loss: 0.3968 - val_acc: 0.8157\n",
      "Epoch 4/100\n",
      "323432/323432 [==============================] - 255s - loss: 0.2604 - acc: 0.8071 - val_loss: 0.3906 - val_acc: 0.8280\n",
      "Epoch 5/100\n",
      "323432/323432 [==============================] - 254s - loss: 0.2455 - acc: 0.8211 - val_loss: 0.3828 - val_acc: 0.8342\n",
      "Epoch 6/100\n",
      "323432/323432 [==============================] - 244s - loss: 0.2330 - acc: 0.8332 - val_loss: 0.4043 - val_acc: 0.8317\n",
      "Epoch 7/100\n",
      "323432/323432 [==============================] - 255s - loss: 0.2225 - acc: 0.8424 - val_loss: 0.3790 - val_acc: 0.8400\n",
      "Epoch 8/100\n",
      "323432/323432 [==============================] - 254s - loss: 0.2135 - acc: 0.8510 - val_loss: 0.3787 - val_acc: 0.8396\n",
      "Epoch 9/100\n",
      "323432/323432 [==============================] - 244s - loss: 0.2057 - acc: 0.8583 - val_loss: 0.3816 - val_acc: 0.8395\n",
      "Epoch 10/100\n",
      "323432/323432 [==============================] - 244s - loss: 0.1974 - acc: 0.8644 - val_loss: 0.3901 - val_acc: 0.8405\n",
      "Epoch 11/100\n",
      "323432/323432 [==============================] - 244s - loss: 0.1914 - acc: 0.8701 - val_loss: 0.4287 - val_acc: 0.8272\n",
      "Epoch 12/100\n",
      "323432/323432 [==============================] - 244s - loss: 0.1862 - acc: 0.8742 - val_loss: 0.4090 - val_acc: 0.8393\n",
      "Epoch 13/100\n",
      "323432/323432 [==============================] - 244s - loss: 0.1800 - acc: 0.8790 - val_loss: 0.4173 - val_acc: 0.8353\n",
      "Epoch 14/100\n",
      "323432/323432 [==============================] - 243s - loss: 0.1758 - acc: 0.8813 - val_loss: 0.4092 - val_acc: 0.8425\n",
      "Epoch 15/100\n",
      "323432/323432 [==============================] - 244s - loss: 0.1701 - acc: 0.8870 - val_loss: 0.4149 - val_acc: 0.8424\n",
      "Epoch 16/100\n",
      "323432/323432 [==============================] - 248s - loss: 0.1669 - acc: 0.8894 - val_loss: 0.4164 - val_acc: 0.8403\n",
      "Epoch 17/100\n",
      "323432/323432 [==============================] - 247s - loss: 0.1636 - acc: 0.8918 - val_loss: 0.4210 - val_acc: 0.8423\n",
      "Epoch 18/100\n",
      "323432/323432 [==============================] - 248s - loss: 0.1594 - acc: 0.8952 - val_loss: 0.4216 - val_acc: 0.8444\n",
      "Epoch 19/100\n",
      "323432/323432 [==============================] - 247s - loss: 0.1564 - acc: 0.8972 - val_loss: 0.4533 - val_acc: 0.8403\n",
      "Epoch 00018: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f586c040080>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = build_model(get_embeddings(nlp.vocab), shape, settings)\n",
    "callbacks = [ModelCheckpoint('decomposable_merged.h5',\n",
    "                                    monitor='val_loss', \n",
    "                                    verbose = 0, save_best_only = True),\n",
    "                 EarlyStopping(monitor='val_loss', patience = 10, verbose = 1)]\n",
    "\n",
    "model.fit([tr_q1, tr_q2, tr_feats], y_tr,\n",
    "        validation_data=([val_q1, val_q2, val_feats], y_val), class_weight = class_weight,\n",
    "        nb_epoch=settings['nr_epoch'], batch_size=settings['batch_size'], callbacks = callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tr_preds = model.predict([tr_q1, tr_q2, tr_feats], batch_size = 16)\n",
    "val_preds = model.predict([val_q1, val_q2, val_feats], batch_size = 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('train_decomposable_merged_0.3787', tr_preds)\n",
    "np.save('val_decomposable_merged_0.3787', val_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
