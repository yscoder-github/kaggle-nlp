{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "\n",
    "from __future__ import division, unicode_literals, print_function\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import spacy\n",
    "import plac\n",
    "import ujson as json\n",
    "import numpy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import en_core_web_md\n",
    "import en_vectors_glove_md\n",
    "\n",
    "\n",
    "from pathlib import Path\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from sklearn.model_selection import train_test_split\n",
    "from spacy_hook import get_embeddings, get_word_ids\n",
    "from spacy_hook import create_similarity_pipeline\n",
    "from keras_decomposable_attention import build_model\n",
    "\n",
    "try:\n",
    "    import cPickle as pickle\n",
    "except ImportError:\n",
    "    import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_quora_data(src_train, src_test):\n",
    "    df_train = pd.read_csv(src_train)\n",
    "    df_train.fillna('NULL', inplace = True)\n",
    "    y_train = df_train['is_duplicate']\n",
    "    df_tr, df_val, y_train, y_val = train_test_split(df_train, y_train, stratify = y_train,\n",
    "                                                test_size = 0.2, random_state = 111)\n",
    "    return df_tr, df_val\n",
    "\n",
    "def evaluate(dev_loc):\n",
    "    dev_texts1, dev_texts2, dev_labels = read_snli(dev_loc)\n",
    "    nlp = spacy.load('en',\n",
    "            create_pipeline=create_similarity_pipeline)\n",
    "    total = 0.\n",
    "    correct = 0.\n",
    "    for text1, text2, label in zip(dev_texts1, dev_texts2, dev_labels):\n",
    "        doc1 = nlp(text1)\n",
    "        doc2 = nlp(text2)\n",
    "        sim = doc1.similarity(doc2)\n",
    "        if sim.argmax() == label.argmax():\n",
    "            correct += 1\n",
    "        total += 1\n",
    "    return correct, total\n",
    "\n",
    "def train_mine(shape, settings, savename):\n",
    "    train_texts1, train_texts2, train_labels = df_tr['question1'], df_tr['question2'], to_categorical(df_tr['is_duplicate'])\n",
    "    dev_texts1, dev_texts2, dev_labels = df_val['question1'], df_val['question2'], to_categorical(df_val['is_duplicate'])\n",
    "    \n",
    "    print(\"Loading spaCy\")\n",
    "    #nlp = spacy.load('en')\n",
    "    nlp = en_core_web_md.load()\n",
    "    #nlp = en_vectors_glove_md.load()\n",
    "    assert nlp.path is not None\n",
    "    \n",
    "    print(\"Compiling network\")\n",
    "    model = build_model(get_embeddings(nlp.vocab), shape, settings)\n",
    "    print(\"Processing texts...\")\n",
    "    Xs = []\n",
    "    for texts in (train_texts1, train_texts2, dev_texts1, dev_texts2):\n",
    "        Xs.append(get_word_ids(list(nlp.pipe(texts, n_threads=20, batch_size=20000)),\n",
    "                         max_length=shape[0],\n",
    "                         rnn_encode=settings['gru_encode'],\n",
    "                         tree_truncate=settings['tree_truncate']))\n",
    "    train_X1, train_X2, dev_X1, dev_X2 = Xs\n",
    "    print(settings)\n",
    "    callbacks = [ModelCheckpoint('{}.h5'.format(savename),\n",
    "                                        monitor='val_loss', \n",
    "                                        verbose = 0, save_best_only = True),\n",
    "                     EarlyStopping(monitor='val_loss', patience = 10, verbose = 1)]\n",
    "    model.fit(\n",
    "        [train_X1, train_X2],\n",
    "        train_labels,\n",
    "        validation_data=([dev_X1, dev_X2], dev_labels),\n",
    "        nb_epoch=settings['nr_epoch'],\n",
    "        batch_size=settings['batch_size'], callbacks = callbacks)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(170, 256, 2)\n"
     ]
    }
   ],
   "source": [
    "src_train_raw = '../../../data/train.csv'\n",
    "src_test_raw = '../../../data/test.csv'\n",
    "\n",
    "src_train = '../../../features/df_train_spacylemmat_fullclean.csv'\n",
    "src_test = '../../../features/df_test_spacylemmat_fullclean.csv'\n",
    "\n",
    "settings = {\n",
    "    'lr': 0.0005,\n",
    "    'dropout': 0.2,\n",
    "    'batch_size': 128,\n",
    "    'nr_epoch': 100,\n",
    "    'tree_truncate': True,\n",
    "    'gru_encode': False,\n",
    "    }\n",
    "\n",
    "max_length = 170\n",
    "nr_hidden = 256\n",
    "shape = (max_length, nr_hidden, 2)\n",
    "print(shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On fullclean data:\n",
    "\n",
    "settings = {\n",
    "    'lr': 0.0005,\n",
    "    'dropout': 0.2,\n",
    "    'batch_size': 128,\n",
    "    'nr_epoch': 100,\n",
    "    'tree_truncate': False,\n",
    "    'gru_encode': False,\n",
    "    }\n",
    "\n",
    "max_length = 170\n",
    "nr_hidden = 256\n",
    "fullclean\n",
    "\n",
    "val_loss: 0.3533 with treetrunc 0.3483"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_tr, df_val = get_quora_data(src_train, src_test)\n",
    "train_mine(shape, settings, 'decomposable_encoreweb_0.0005LR_treetrunc_170len_lemmatDF')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = en_core_web_md.load()\n",
    "model = build_model(get_embeddings(nlp.vocab), shape, settings)\n",
    "model.load_weights('decomposable_encoreweb_0.0005LR_treetrunc_170len_lemmatDF.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing texts...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-98a49aaa4aff>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mtr_preds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_X1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_X2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m128\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0mval_preds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdev_X1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdev_X2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m128\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'train_decomposable_0.3894'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtr_preds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'val_decomposable_0.3894'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_preds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "df_tr, df_val = get_quora_data(src_train, src_test)\n",
    "train_texts1, train_texts2, train_labels = df_tr['question1'], df_tr['question2'], to_categorical(df_tr['is_duplicate'])\n",
    "dev_texts1, dev_texts2, dev_labels = df_val['question1'], df_val['question2'], to_categorical(df_val['is_duplicate'])\n",
    "    \n",
    "nlp = en_core_web_md.load()\n",
    "print(\"Processing texts...\")\n",
    "Xs = []\n",
    "for texts in (train_texts1, train_texts2, dev_texts1, dev_texts2):\n",
    "    Xs.append(get_word_ids(list(nlp.pipe(texts, n_threads=20, batch_size=20000)),\n",
    "                     max_length=shape[0],\n",
    "                     rnn_encode=settings['gru_encode'],\n",
    "                     tree_truncate=settings['tree_truncate']))\n",
    "train_X1, train_X2, dev_X1, dev_X2 = Xs\n",
    "\n",
    "tr_preds = model.predict([train_X1, train_X2], batch_size = 128)\n",
    "val_preds = model.predict([dev_X1, dev_X2], batch_size = 128)\n",
    "np.save('train_decomposable_0.3894', tr_preds)\n",
    "np.save('val_decomposable_0.3894', val_preds)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
