{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\"\n",
    "\n",
    "import re\n",
    "import csv\n",
    "import codecs\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gc\n",
    "import sys\n",
    "import gensim\n",
    "import time\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "from string import punctuation\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import log_loss\n",
    "from gensim.models import KeyedVectors\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation\n",
    "from keras.layers.merge import concatenate\n",
    "from keras.models import Model\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.layers.advanced_activations import PReLU\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def merged_lstm():\n",
    "    embedding_layer = Embedding(nb_words,\n",
    "            embedding_dim,\n",
    "            weights=[word_embedding_matrix],\n",
    "            input_length=seq_length,\n",
    "            trainable=False)\n",
    "    \n",
    "    lstm_layer = LSTM(128, dropout=0.25, recurrent_dropout=0.2,\n",
    "                     go_backwards = False, implementation = 2)\n",
    "\n",
    "    sequence_1_input = Input(shape=(seq_length,), dtype='int32')\n",
    "    embedded_sequences_1 = embedding_layer(sequence_1_input)\n",
    "    x1 = lstm_layer(embedded_sequences_1)\n",
    "\n",
    "    sequence_2_input = Input(shape=(seq_length,), dtype='int32')\n",
    "    embedded_sequences_2 = embedding_layer(sequence_2_input)\n",
    "    y1 = lstm_layer(embedded_sequences_2)\n",
    "\n",
    "    dense_input = Input(shape = (ncols,))\n",
    "    d = Dense(256, kernel_initializer = 'he_normal')(dense_input)\n",
    "    d = PReLU()(d)\n",
    "    d = BatchNormalization()(d)\n",
    "    d = Dropout(0.4)(d)\n",
    "    \n",
    "    d2 = Dense(512, kernel_initializer = 'he_normal')(d)\n",
    "    d2 = PReLU()(d2)\n",
    "    d2 = BatchNormalization()(d2)\n",
    "    d2 = Dropout(0.2)(d2)\n",
    "    \n",
    "    d3 = Dense(512, kernel_initializer = 'he_normal')(d2)\n",
    "    d3 = PReLU()(d3)\n",
    "    d3 = Dropout(0.2)(d3)\n",
    "    \n",
    "    merged = concatenate([x1, y1, d3])\n",
    "    merged = Dropout(0.25)(merged)\n",
    "    merged = BatchNormalization()(merged)\n",
    "\n",
    "    merged = Dense(256)(merged)\n",
    "    merged = PReLU()(merged)\n",
    "    merged = Dropout(0.25)(merged)\n",
    "    merged = BatchNormalization()(merged)\n",
    "\n",
    "    preds = Dense(1, activation='sigmoid')(merged)\n",
    "    model = Model(inputs=[sequence_1_input, sequence_2_input, dense_input], outputs=preds)\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['acc'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "seq_length = 128\n",
    "embedding_dim = 300\n",
    "nb_words = 120594\n",
    "\n",
    "data_src = '/media/w/1c392724-ecf3-4615-8f3c-79368ec36380/DS Projects/Kaggle/Quora/data/transformed/keras_tokenizer/'\n",
    "word_embedding_matrix = np.load(data_src + 'embedding_matrix.npy')\n",
    "\n",
    "q_src = '/media/w/1c392724-ecf3-4615-8f3c-79368ec36380/DS Projects/Kaggle/Quora/data/features/NER/'\n",
    "feats_src = '/media/w/1c392724-ecf3-4615-8f3c-79368ec36380/DS Projects/Kaggle/Quora/data/features/uncleaned/'\n",
    "\n",
    "q1 = np.load(q_src + 'q1train_NER_128len.npy')\n",
    "q2 = np.load(q_src + 'q2train_NER_128len.npy')\n",
    "X_train = pd.read_pickle('Xtrain_866BestColsDropped.pkl')\n",
    "X_train = X_train.astype('float32')\n",
    "X_train = X_train.replace(np.nan, -999)\n",
    "X_train = X_train.replace(np.inf, 999)\n",
    "\n",
    "y = pd.read_csv(feats_src + '/the_1owl/owl_train.csv')['is_duplicate'].values\n",
    "\n",
    "test = False\n",
    "if test:\n",
    "    q1_te = np.load(q_src + 'q1test_NER_128len.npy')\n",
    "    q2_te = np.load(q_src + 'q2test_NER_128len.npy')\n",
    "    X_test = pd.read_pickle('Xtest_814colsBest.pkl', compression = 'bz2')\n",
    "    X_test = X_test.astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def lstm_foldrun(X, q1, q2, y, X_test = None, q1_test = None, q2_test = None, start_fold = 0,\n",
    "                name = 'LSTM_merged866cols_2ndpart', save = True):\n",
    "    \n",
    "    skf = StratifiedKFold(n_splits = 10, random_state = 111, shuffle = True)\n",
    "    if isinstance(X, pd.core.frame.DataFrame):\n",
    "        X = X.values\n",
    "    if isinstance(X_test, pd.core.frame.DataFrame):\n",
    "        X_test = X_test.values\n",
    "    if isinstance(y, pd.core.frame.DataFrame):\n",
    "        y = y.is_duplicate.values\n",
    "    if isinstance(y, pd.core.frame.Series):\n",
    "        y = y.values\n",
    "    \n",
    "    i = 0\n",
    "    losses = []\n",
    "    train_splits = []\n",
    "    val_splits = []\n",
    "    for tr_index, val_index in skf.split(X, y):\n",
    "        train_splits.append(tr_index)\n",
    "        val_splits.append(val_index)\n",
    "        \n",
    "    oof_train = np.zeros((404290))\n",
    "    oof_test = np.zeros((10, 2345796))\n",
    "    os.makedirs('saved_models/LSTM/SKF/{}'.format(name), exist_ok = True)\n",
    "    for i in range(start_fold, 10):\n",
    "        X_tr, X_val = X[train_splits[i]], X[val_splits[i]]\n",
    "        q1_tr, q1_val = q1[train_splits[i]], q1[val_splits[i]]\n",
    "        q2_tr, q2_val = q2[train_splits[i]], q2[val_splits[i]]\n",
    "        y_tr, y_val = y[train_splits[i]], y[val_splits[i]]\n",
    "\n",
    "        t = time.time()\n",
    "        print('Start training on fold: {}'.format(i))\n",
    "        callbacks = [ModelCheckpoint('saved_models/LSTM/SKF/{}/{}_fold{}.h5'.format(name, name, i),\n",
    "                                    monitor='val_loss', \n",
    "                                    verbose = 0, save_best_only = True),\n",
    "                 EarlyStopping(monitor='val_loss', patience = 7, verbose = 1)]\n",
    "        \n",
    "        model = merged_lstm()\n",
    "        model.fit([q1_tr, q2_tr, X_tr], y_tr, validation_data=([q1_val, q2_val, X_val], y_val),\n",
    "                epochs=200, batch_size=512, callbacks = callbacks)\n",
    "        \n",
    "        val_pred = model.predict([q1_val, q2_val, X_val], batch_size = 64)\n",
    "        oof_train[val_splits[i]] = val_pred\n",
    "        score = log_loss(y_val, val_pred)\n",
    "        losses.append(score)\n",
    "        print('Predicting training set.')\n",
    "        if X_test is not None:\n",
    "            print('Predicting test set.')\n",
    "            test_preds = model.predict([q1_te, q2_te, X_test], batch_size = 64)\n",
    "            oof_test[i, :] = test_preds\n",
    "        print('Final score for fold {} :'.format(i), score, '\\n',\n",
    "              'Time it took to train and predict on fold:', time.time() - t, '\\n')\n",
    "        del X_tr, X_val, q1_tr, q1_val, q2_tr, q2_val\n",
    "        gc.collect()\n",
    "        i += 1\n",
    "        \n",
    "    print('Mean logloss for model in 10-folds SKF:', np.array(losses).mean(axis = 0), '\\n')\n",
    "    oof_train = pd.DataFrame(oof_train)\n",
    "    oof_train.columns = ['{}_prob'.format(name)]\n",
    "    oof_test = oof_test.mean(axis = 0)\n",
    "    oof_test = pd.DataFrame(oof_test)\n",
    "    oof_test.columns = ['{}_prob'.format(name)]\n",
    "    if save:\n",
    "        oof_train.to_pickle('OOF_preds/train/train_preds_{}.pkl'.format(name))\n",
    "        oof_test.to_pickle('OOF_preds/test/test_preds_{}.pkl'.format(name))\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training on fold: 0\n",
      "Train on 363860 samples, validate on 40430 samples\n",
      "Epoch 1/200\n",
      "363860/363860 [==============================] - 171s - loss: 0.5136 - acc: 0.7237 - val_loss: 0.4453 - val_acc: 0.7678\n",
      "Epoch 2/200\n",
      "363860/363860 [==============================] - 171s - loss: 0.4356 - acc: 0.7752 - val_loss: 0.4206 - val_acc: 0.7786\n",
      "Epoch 3/200\n",
      "363860/363860 [==============================] - 171s - loss: 0.4013 - acc: 0.7976 - val_loss: 0.3699 - val_acc: 0.8184\n",
      "Epoch 4/200\n",
      "363860/363860 [==============================] - 171s - loss: 0.3831 - acc: 0.8084 - val_loss: 0.3610 - val_acc: 0.8250\n",
      "Epoch 5/200\n",
      "363860/363860 [==============================] - 171s - loss: 0.3700 - acc: 0.8157 - val_loss: 0.3584 - val_acc: 0.8171\n",
      "Epoch 6/200\n",
      "363860/363860 [==============================] - 171s - loss: 0.3622 - acc: 0.8207 - val_loss: 0.3381 - val_acc: 0.8333\n",
      "Epoch 7/200\n",
      "363860/363860 [==============================] - 171s - loss: 0.3554 - acc: 0.8240 - val_loss: 0.3523 - val_acc: 0.8235\n",
      "Epoch 8/200\n",
      "363860/363860 [==============================] - 171s - loss: 0.3509 - acc: 0.8266 - val_loss: 0.3270 - val_acc: 0.8370\n",
      "Epoch 9/200\n",
      "363860/363860 [==============================] - 170s - loss: 0.3469 - acc: 0.8284 - val_loss: 0.3353 - val_acc: 0.8369\n",
      "Epoch 10/200\n",
      "363860/363860 [==============================] - 171s - loss: 0.3420 - acc: 0.8308 - val_loss: 0.3257 - val_acc: 0.8395\n",
      "Epoch 11/200\n",
      "363860/363860 [==============================] - 171s - loss: 0.3374 - acc: 0.8340 - val_loss: 0.3426 - val_acc: 0.8240\n",
      "Epoch 12/200\n",
      "363860/363860 [==============================] - 172s - loss: 0.3362 - acc: 0.8337 - val_loss: 0.3185 - val_acc: 0.8467\n",
      "Epoch 13/200\n",
      "363860/363860 [==============================] - 172s - loss: 0.3340 - acc: 0.8358 - val_loss: 0.3177 - val_acc: 0.8443\n",
      "Epoch 14/200\n",
      "363860/363860 [==============================] - 169s - loss: 0.3303 - acc: 0.8374 - val_loss: 0.3190 - val_acc: 0.8462\n",
      "Epoch 15/200\n",
      "363860/363860 [==============================] - 168s - loss: 0.3292 - acc: 0.8379 - val_loss: 0.3126 - val_acc: 0.8467\n",
      "Epoch 16/200\n",
      "363860/363860 [==============================] - 167s - loss: 0.3255 - acc: 0.8403 - val_loss: 0.3166 - val_acc: 0.8456\n",
      "Epoch 17/200\n",
      "363860/363860 [==============================] - 166s - loss: 0.3249 - acc: 0.8405 - val_loss: 0.3129 - val_acc: 0.8489\n",
      "Epoch 18/200\n",
      "363860/363860 [==============================] - 166s - loss: 0.3230 - acc: 0.8412 - val_loss: 0.3054 - val_acc: 0.8520\n",
      "Epoch 19/200\n",
      "363860/363860 [==============================] - 164s - loss: 0.3219 - acc: 0.8416 - val_loss: 0.3129 - val_acc: 0.8483\n",
      "Epoch 20/200\n",
      "363860/363860 [==============================] - 165s - loss: 0.3184 - acc: 0.8433 - val_loss: 0.2986 - val_acc: 0.8545\n",
      "Epoch 21/200\n",
      "363860/363860 [==============================] - 165s - loss: 0.3172 - acc: 0.8448 - val_loss: 0.3040 - val_acc: 0.8520\n",
      "Epoch 22/200\n",
      "363860/363860 [==============================] - 164s - loss: 0.3175 - acc: 0.8444 - val_loss: 0.3188 - val_acc: 0.8394\n",
      "Epoch 23/200\n",
      "363860/363860 [==============================] - 165s - loss: 0.3140 - acc: 0.8462 - val_loss: 0.3021 - val_acc: 0.8561\n",
      "Epoch 24/200\n",
      "363860/363860 [==============================] - 165s - loss: 0.3141 - acc: 0.8459 - val_loss: 0.3076 - val_acc: 0.8465\n",
      "Epoch 25/200\n",
      "363860/363860 [==============================] - 165s - loss: 0.3124 - acc: 0.8472 - val_loss: 0.3051 - val_acc: 0.8530\n",
      "Epoch 26/200\n",
      "363860/363860 [==============================] - 165s - loss: 0.3107 - acc: 0.8479 - val_loss: 0.3008 - val_acc: 0.8557\n",
      "Epoch 27/200\n",
      "363860/363860 [==============================] - 166s - loss: 0.3137 - acc: 0.8457 - val_loss: 0.2956 - val_acc: 0.8556\n",
      "Epoch 28/200\n",
      "363860/363860 [==============================] - 166s - loss: 0.3111 - acc: 0.8479 - val_loss: 0.2923 - val_acc: 0.8595\n",
      "Epoch 29/200\n",
      "363860/363860 [==============================] - 165s - loss: 0.3104 - acc: 0.8487 - val_loss: 0.3013 - val_acc: 0.8518\n",
      "Epoch 30/200\n",
      "363860/363860 [==============================] - 164s - loss: 0.3090 - acc: 0.8488 - val_loss: 0.3028 - val_acc: 0.8496\n",
      "Epoch 31/200\n",
      "363860/363860 [==============================] - 166s - loss: 0.3073 - acc: 0.8502 - val_loss: 0.2872 - val_acc: 0.8615\n",
      "Epoch 32/200\n",
      "363860/363860 [==============================] - 165s - loss: 0.3052 - acc: 0.8517 - val_loss: 0.2913 - val_acc: 0.8588\n",
      "Epoch 33/200\n",
      "363860/363860 [==============================] - 165s - loss: 0.3050 - acc: 0.8516 - val_loss: 0.3027 - val_acc: 0.8601\n",
      "Epoch 34/200\n",
      "363860/363860 [==============================] - 165s - loss: 0.3032 - acc: 0.8527 - val_loss: 0.2951 - val_acc: 0.8617\n",
      "Epoch 35/200\n",
      "363860/363860 [==============================] - 164s - loss: 0.3031 - acc: 0.8535 - val_loss: 0.3008 - val_acc: 0.8513\n",
      "Epoch 36/200\n",
      "363860/363860 [==============================] - 165s - loss: 0.3015 - acc: 0.8537 - val_loss: 0.2872 - val_acc: 0.8635\n",
      "Epoch 37/200\n",
      "363860/363860 [==============================] - 165s - loss: 0.3011 - acc: 0.8538 - val_loss: 0.2928 - val_acc: 0.8616\n",
      "Epoch 38/200\n",
      "363860/363860 [==============================] - 164s - loss: 0.3027 - acc: 0.8528 - val_loss: 0.2965 - val_acc: 0.8575\n",
      "Epoch 39/200\n",
      "363860/363860 [==============================] - 165s - loss: 0.3001 - acc: 0.8546 - val_loss: 0.3017 - val_acc: 0.8553\n",
      "Epoch 40/200\n",
      "363860/363860 [==============================] - 165s - loss: 0.3020 - acc: 0.8530 - val_loss: 0.3182 - val_acc: 0.8372\n",
      "Epoch 41/200\n",
      "363860/363860 [==============================] - 165s - loss: 0.2997 - acc: 0.8551 - val_loss: 0.2874 - val_acc: 0.8641\n",
      "Epoch 42/200\n",
      "363860/363860 [==============================] - 165s - loss: 0.2996 - acc: 0.8548 - val_loss: 0.2878 - val_acc: 0.8637\n",
      "Epoch 43/200\n",
      "363860/363860 [==============================] - 165s - loss: 0.2983 - acc: 0.8558 - val_loss: 0.2962 - val_acc: 0.8595\n",
      "Epoch 44/200\n",
      "363860/363860 [==============================] - 165s - loss: 0.2974 - acc: 0.8554 - val_loss: 0.2832 - val_acc: 0.8640\n",
      "Epoch 45/200\n",
      "363860/363860 [==============================] - 165s - loss: 0.2975 - acc: 0.8561 - val_loss: 0.2908 - val_acc: 0.8626\n",
      "Epoch 46/200\n",
      "363860/363860 [==============================] - 165s - loss: 0.2981 - acc: 0.8557 - val_loss: 0.3517 - val_acc: 0.8169\n",
      "Epoch 47/200\n",
      "363860/363860 [==============================] - 164s - loss: 0.2977 - acc: 0.8558 - val_loss: 0.2865 - val_acc: 0.8644\n",
      "Epoch 48/200\n",
      "363860/363860 [==============================] - 165s - loss: 0.2951 - acc: 0.8576 - val_loss: 0.2845 - val_acc: 0.8641\n",
      "Epoch 49/200\n",
      "363860/363860 [==============================] - 165s - loss: 0.2947 - acc: 0.8578 - val_loss: 0.2792 - val_acc: 0.8671\n",
      "Epoch 50/200\n",
      "363860/363860 [==============================] - 166s - loss: 0.2940 - acc: 0.8579 - val_loss: 0.2801 - val_acc: 0.8650\n",
      "Epoch 51/200\n",
      "363860/363860 [==============================] - 165s - loss: 0.2939 - acc: 0.8587 - val_loss: 0.3034 - val_acc: 0.8524\n",
      "Epoch 52/200\n",
      "363860/363860 [==============================] - 165s - loss: 0.2925 - acc: 0.8588 - val_loss: 0.2799 - val_acc: 0.8647\n",
      "Epoch 53/200\n",
      "363860/363860 [==============================] - 166s - loss: 0.2925 - acc: 0.8594 - val_loss: 0.2806 - val_acc: 0.8658\n",
      "Epoch 54/200\n",
      "363860/363860 [==============================] - 165s - loss: 0.2917 - acc: 0.8591 - val_loss: 0.2886 - val_acc: 0.8649\n",
      "Epoch 55/200\n",
      "363860/363860 [==============================] - 167s - loss: 0.2924 - acc: 0.8585 - val_loss: 0.2782 - val_acc: 0.8665\n",
      "Epoch 56/200\n",
      "363860/363860 [==============================] - 166s - loss: 0.2918 - acc: 0.8595 - val_loss: 0.2791 - val_acc: 0.8660\n",
      "Epoch 57/200\n",
      "363860/363860 [==============================] - 165s - loss: 0.2908 - acc: 0.8593 - val_loss: 0.2833 - val_acc: 0.8662\n",
      "Epoch 58/200\n",
      "363860/363860 [==============================] - 164s - loss: 0.2914 - acc: 0.8594 - val_loss: 0.2993 - val_acc: 0.8519\n",
      "Epoch 59/200\n",
      "363860/363860 [==============================] - 164s - loss: 0.2913 - acc: 0.8598 - val_loss: 0.2849 - val_acc: 0.8639\n",
      "Epoch 60/200\n",
      "363860/363860 [==============================] - 164s - loss: 0.2914 - acc: 0.8596 - val_loss: 0.2827 - val_acc: 0.8653\n",
      "Epoch 61/200\n",
      "363860/363860 [==============================] - 165s - loss: 0.2900 - acc: 0.8601 - val_loss: 0.2824 - val_acc: 0.8666\n",
      "Epoch 62/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "363860/363860 [==============================] - 166s - loss: 0.2909 - acc: 0.8600 - val_loss: 0.2825 - val_acc: 0.8630\n",
      "Epoch 63/200\n",
      "363860/363860 [==============================] - 165s - loss: 0.2907 - acc: 0.8597 - val_loss: 0.2742 - val_acc: 0.8677\n",
      "Epoch 64/200\n",
      "363860/363860 [==============================] - 164s - loss: 0.2881 - acc: 0.8618 - val_loss: 0.2790 - val_acc: 0.8671\n",
      "Epoch 65/200\n",
      "363860/363860 [==============================] - 164s - loss: 0.2885 - acc: 0.8611 - val_loss: 0.2996 - val_acc: 0.8503\n",
      "Epoch 66/200\n",
      "363860/363860 [==============================] - 166s - loss: 0.2886 - acc: 0.8611 - val_loss: 0.2920 - val_acc: 0.8593\n",
      "Epoch 67/200\n",
      "363860/363860 [==============================] - 164s - loss: 0.2873 - acc: 0.8621 - val_loss: 0.2872 - val_acc: 0.8642\n",
      "Epoch 68/200\n",
      "363860/363860 [==============================] - 165s - loss: 0.2885 - acc: 0.8611 - val_loss: 0.2805 - val_acc: 0.8666\n",
      "Epoch 69/200\n",
      "363860/363860 [==============================] - 164s - loss: 0.2862 - acc: 0.8621 - val_loss: 0.2874 - val_acc: 0.8615\n",
      "Epoch 70/200\n",
      "363860/363860 [==============================] - 164s - loss: 0.2854 - acc: 0.8625 - val_loss: 0.2765 - val_acc: 0.8701\n",
      "Epoch 71/200\n",
      "363860/363860 [==============================] - 165s - loss: 0.2872 - acc: 0.8621 - val_loss: 0.2779 - val_acc: 0.8675\n",
      "Epoch 00070: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/w/anaconda3/envs/idp3/lib/python3.5/site-packages/ipykernel_launcher.py:43: DeprecationWarning: assignment will raise an error in the future, most likely because your index result shape does not match the value array shape. You can use `arr.flat[index] = values` to keep the old behaviour.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting training set.\n",
      "Final score for fold 0 : 0.277947695493 \n",
      " Time it took to train and predict on fold: 11881.0326089859 \n",
      "\n",
      "Start training on fold: 1\n",
      "Train on 363860 samples, validate on 40430 samples\n",
      "Epoch 1/200\n",
      "363860/363860 [==============================] - 169s - loss: 0.5152 - acc: 0.7242 - val_loss: 0.4546 - val_acc: 0.7623\n",
      "Epoch 2/200\n",
      "363860/363860 [==============================] - 168s - loss: 0.4349 - acc: 0.7751 - val_loss: 0.3968 - val_acc: 0.8005\n",
      "Epoch 3/200\n",
      "363860/363860 [==============================] - 169s - loss: 0.3987 - acc: 0.7990 - val_loss: 0.3696 - val_acc: 0.8148\n",
      "Epoch 4/200\n",
      "363860/363860 [==============================] - 168s - loss: 0.3822 - acc: 0.8098 - val_loss: 0.3577 - val_acc: 0.8246\n",
      "Epoch 5/200\n",
      "363860/363860 [==============================] - 169s - loss: 0.3730 - acc: 0.8151 - val_loss: 0.3476 - val_acc: 0.8271\n",
      "Epoch 6/200\n",
      "363860/363860 [==============================] - 207s - loss: 0.3628 - acc: 0.8209 - val_loss: 0.3525 - val_acc: 0.8235\n",
      "Epoch 7/200\n",
      "363860/363860 [==============================] - 211s - loss: 0.3560 - acc: 0.8240 - val_loss: 0.3394 - val_acc: 0.8347\n",
      "Epoch 8/200\n",
      "363860/363860 [==============================] - 209s - loss: 0.3503 - acc: 0.8269 - val_loss: 0.3706 - val_acc: 0.8088\n",
      "Epoch 9/200\n",
      "363860/363860 [==============================] - 208s - loss: 0.3436 - acc: 0.8306 - val_loss: 0.3405 - val_acc: 0.8341\n",
      "Epoch 10/200\n",
      "363860/363860 [==============================] - 210s - loss: 0.3413 - acc: 0.8318 - val_loss: 0.3266 - val_acc: 0.8412\n",
      "Epoch 11/200\n",
      "363860/363860 [==============================] - 210s - loss: 0.3364 - acc: 0.8347 - val_loss: 0.3314 - val_acc: 0.8365\n",
      "Epoch 12/200\n",
      "363860/363860 [==============================] - 208s - loss: 0.3370 - acc: 0.8341 - val_loss: 0.3276 - val_acc: 0.8402\n",
      "Epoch 13/200\n",
      "363860/363860 [==============================] - 209s - loss: 0.3321 - acc: 0.8370 - val_loss: 0.3192 - val_acc: 0.8428\n",
      "Epoch 14/200\n",
      "363860/363860 [==============================] - 210s - loss: 0.3317 - acc: 0.8368 - val_loss: 0.3198 - val_acc: 0.8464\n",
      "Epoch 15/200\n",
      "363860/363860 [==============================] - 210s - loss: 0.3312 - acc: 0.8372 - val_loss: 0.3161 - val_acc: 0.8461\n",
      "Epoch 16/200\n",
      "363860/363860 [==============================] - 211s - loss: 0.3274 - acc: 0.8391 - val_loss: 0.3101 - val_acc: 0.8489\n",
      "Epoch 17/200\n",
      "363860/363860 [==============================] - 206s - loss: 0.3238 - acc: 0.8412 - val_loss: 0.3096 - val_acc: 0.8493\n",
      "Epoch 18/200\n",
      "363860/363860 [==============================] - 207s - loss: 0.3226 - acc: 0.8418 - val_loss: 0.3664 - val_acc: 0.8106\n",
      "Epoch 19/200\n",
      "363860/363860 [==============================] - 209s - loss: 0.3215 - acc: 0.8427 - val_loss: 0.3108 - val_acc: 0.8491\n",
      "Epoch 20/200\n",
      "363860/363860 [==============================] - 209s - loss: 0.3200 - acc: 0.8432 - val_loss: 0.3220 - val_acc: 0.8396\n",
      "Epoch 21/200\n",
      "363860/363860 [==============================] - 206s - loss: 0.3179 - acc: 0.8446 - val_loss: 0.3108 - val_acc: 0.8455\n",
      "Epoch 22/200\n",
      "363860/363860 [==============================] - 208s - loss: 0.3170 - acc: 0.8452 - val_loss: 0.3019 - val_acc: 0.8525\n",
      "Epoch 23/200\n",
      "363860/363860 [==============================] - 207s - loss: 0.3139 - acc: 0.8471 - val_loss: 0.3246 - val_acc: 0.8389\n",
      "Epoch 24/200\n",
      "363860/363860 [==============================] - 197s - loss: 0.3155 - acc: 0.8462 - val_loss: 0.3269 - val_acc: 0.8375\n",
      "Epoch 25/200\n",
      "101376/363860 [=======>......................] - ETA: 140s - loss: 0.3109 - acc: 0.8489"
     ]
    }
   ],
   "source": [
    "ncols = X_train.shape[1]\n",
    "lstm_foldrun(X_train, q1, q2, y, start_fold = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
