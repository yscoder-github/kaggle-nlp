{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/w/anaconda3/envs/idp3/lib/python3.5/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from collections import Counter\n",
    "from sklearn.metrics import log_loss\n",
    "from scipy.optimize import minimize\n",
    "import multiprocessing\n",
    "import difflib\n",
    "import time\n",
    "\n",
    "import xgboost as xgb\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_train():\n",
    "    keras_q1 = np.load('../../data/transformed/keras_tokenizer/train_q1_transformed.npy')\n",
    "    keras_q2 = np.load('../../data/transformed/keras_tokenizer/train_q2_transformed.npy')\n",
    "    xgb_feats = pd.read_csv('../../data/features/the_1owl/owl_train.csv')\n",
    "    abhishek_feats = pd.read_csv('../../data/features/abhishek/train_features.csv',\n",
    "                              encoding = 'ISO-8859-1').iloc[:, 2:]\n",
    "    text_feats = pd.read_csv('../../data/features/other_features/text_features_train.csv',\n",
    "                            encoding = 'ISO-8859-1')\n",
    "    img_feats = pd.read_csv('../../data/features/other_features/img_features_train.csv')\n",
    "    srk_feats = pd.read_csv('../../data/features/srk/SRK_grams_features_train.csv')\n",
    "\n",
    "    xgb_feats.drop(['z_len1', 'z_len2', 'z_word_len1', 'z_word_len2'], axis = 1, inplace = True)\n",
    "    y_train = xgb_feats['is_duplicate']\n",
    "    xgb_feats = xgb_feats.iloc[:, 8:]\n",
    "    \n",
    "    X_train2 = np.concatenate([xgb_feats, abhishek_feats], axis = 1)\n",
    "    #X_train2 = np.concatenate([keras_q1, keras_q2, xgb_feats, abhishek_feats, text_feats], axis = 1)\n",
    "    for i in range(X_train2.shape[1]):\n",
    "        if np.sum(X_train2[:, i] == y_train.values) == X_train2.shape[0]:\n",
    "            print('LEAK FOUND')\n",
    "    \n",
    "    X_train2 = X_train2.astype('float32')\n",
    "    X_train2 = pd.DataFrame(X_train2)\n",
    "    X_train2['is_duplicate'] = y_train\n",
    "    print('Training data shape:', X_train2.shape)\n",
    "    return X_train2, y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv('../../data/features/the_1owl/owl_train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pos_train = train[train['is_duplicate'] == 1]\n",
    "neg_train = train[train['is_duplicate'] == 0]\n",
    "p = 0.165\n",
    "scale = ((len(pos_train) / (len(pos_train) + len(neg_train))) / p) - 1\n",
    "while scale > 1:\n",
    "    neg_train = pd.concat([neg_train, neg_train])\n",
    "    scale -=1\n",
    "neg_train = pd.concat([neg_train, neg_train[:int(scale * len(neg_train))]])\n",
    "train = pd.concat([pos_train, neg_train])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-logloss:0.683096\tvalid-logloss:0.683143\n",
      "Multiple eval metrics have been passed: 'valid-logloss' will be used for early stopping.\n",
      "\n",
      "Will train until valid-logloss hasn't improved in 50 rounds.\n",
      "[100]\ttrain-logloss:0.388841\tvalid-logloss:0.390304\n",
      "[200]\ttrain-logloss:0.359606\tvalid-logloss:0.361565\n",
      "[300]\ttrain-logloss:0.353011\tvalid-logloss:0.355308\n",
      "[400]\ttrain-logloss:0.349802\tvalid-logloss:0.352352\n"
     ]
    }
   ],
   "source": [
    "x_train, x_valid, y_train, y_valid = train_test_split(train.iloc[:, 8:], train['is_duplicate'], \n",
    "                                                      test_size=0.2, random_state=0)\n",
    "\n",
    "params = {}\n",
    "params[\"objective\"] = \"binary:logistic\"\n",
    "params['eval_metric'] = 'logloss'\n",
    "params[\"eta\"] = 0.05\n",
    "params[\"subsample\"] = 0.7\n",
    "params[\"min_child_weight\"] = 1\n",
    "params[\"colsample_bytree\"] = 0.7\n",
    "params[\"max_depth\"] = 4\n",
    "params[\"silent\"] = 1\n",
    "params[\"seed\"] = 1632\n",
    "params['nthread'] = 6\n",
    "\n",
    "d_train = xgb.DMatrix(x_train, label=y_train)\n",
    "d_valid = xgb.DMatrix(x_valid, label=y_valid)\n",
    "watchlist = [(d_train, 'train'), (d_valid, 'valid')]\n",
    "bst = xgb.train(params, d_train, 10000, watchlist, early_stopping_rounds=100, verbose_eval=100)\n",
    "print(log_loss(train.is_duplicate, bst.predict(xgb.DMatrix(train[col]))))\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "pos_train = X_train[X_train['is_duplicate'] == 1]\n",
    "neg_train = X_train[X_train['is_duplicate'] == 0]\n",
    "p = 0.165\n",
    "scale = ((len(pos_train) / (len(pos_train) + len(neg_train))) / p) - 1\n",
    "while scale > 1:\n",
    "    neg_train = pd.concat([neg_train, neg_train])\n",
    "    scale -=1\n",
    "neg_train = pd.concat([neg_train, neg_train[:int(scale * len(neg_train))]])\n",
    "X_train2 = pd.concat([pos_train, neg_train])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def kappa(preds, y):\n",
    "    score = []\n",
    "    a = 0.165 / 0.37\n",
    "    b = (1 - 0.165) / (1 - 0.37)\n",
    "    for pp,yy in zip(preds, y.get_label()):\n",
    "        score.append(a * yy * np.log (pp) + b * (1 - yy) * np.log(1-pp))\n",
    "    score = -np.sum(score) / len(score)\n",
    "    return 'kappa', score\n",
    "\n",
    "params = {\n",
    "    'seed': 1337,\n",
    "    'colsample_bytree': 0.7,\n",
    "    'silent': 1,\n",
    "    'subsample': 0.7,\n",
    "    'eta': 0.05,\n",
    "    'objective': 'binary:logistic',\n",
    "    'eval_metric': 'logloss',\n",
    "    'max_depth': 4,\n",
    "    'min_child_weight': 1,\n",
    "    'nthread': 6,\n",
    "    }\n",
    "\n",
    "X_tr, X_val, y_tr, y_val = train_test_split(X_train2.iloc[:, 8:], X_train2['is_duplicate'], \n",
    "                                            test_size = 0.2, random_state = 111)\n",
    "\n",
    "dtrain = xgb.DMatrix(X_tr, label = y_tr)\n",
    "dval = xgb.DMatrix(X_val, label = y_val)\n",
    "watchlist = [(dtrain, 'train'), (dval, 'valid')]\n",
    "\n",
    "bst = xgb.train(params, dtrain, 100000, watchlist, early_stopping_rounds=100, verbose_eval=50)\n",
    "#                feval = kappa)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
