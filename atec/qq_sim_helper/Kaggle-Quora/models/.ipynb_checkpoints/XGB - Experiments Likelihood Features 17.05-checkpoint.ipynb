{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import multiprocessing\n",
    "import difflib\n",
    "import time\n",
    "import gc\n",
    "import xgboost as xgb\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "import category_encoders as ce\n",
    "\n",
    "from collections import Counter\n",
    "from sklearn.metrics import log_loss\n",
    "from scipy.optimize import minimize\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.cross_validation import StratifiedKFold\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from scipy.spatial.distance import cosine, correlation, canberra, chebyshev, minkowski, jaccard, euclidean\n",
    "\n",
    "from models_utils_xgb import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_train():\n",
    "    feats_src = '/media/w/1c392724-ecf3-4615-8f3c-79368ec36380/DS Projects/Kaggle/Quora/data/features/uncleaned/'\n",
    "    keras_q1 = np.load(feats_src + 'train_q1_transformed.npy')\n",
    "    keras_q2 = np.load(feats_src + 'train_q2_transformed.npy')\n",
    "    \n",
    "    feats_src2 = '/media/w/1c392724-ecf3-4615-8f3c-79368ec36380/DS Projects/Kaggle/Quora/data/features/NER/'\n",
    "    keras_q1 = np.load(feats_src2 + 'q1train_NER_128len.npy')\n",
    "    keras_q2 = np.load(feats_src2 + 'q2train_NER_128len.npy')\n",
    "    \n",
    "    xgb_feats = pd.read_csv(feats_src + '/the_1owl/owl_train.csv')\n",
    "    abhishek_feats = pd.read_csv(feats_src + 'abhishek/train_features.csv',\n",
    "                              encoding = 'ISO-8859-1').iloc[:, 2:]\n",
    "    text_feats = pd.read_csv(feats_src + 'other_features/text_features_train.csv',\n",
    "                            encoding = 'ISO-8859-1')\n",
    "    img_feats = pd.read_csv(feats_src + 'other_features/img_features_train.csv')\n",
    "    srk_feats = pd.read_csv(feats_src + 'srk/SRK_grams_features_train.csv')\n",
    "    \n",
    "    mephisto_feats = pd.read_csv('../../data/features/lemmatized_fullclean/train_mephistopeheles_features.csv').iloc[:, 6:]\n",
    "    turkewitz_feats = pd.read_csv('../../data/features/lemmatized_fullclean/train_turkewitz_features_fullcleanSTEMMED.csv')\n",
    "    #turkewitz_feats = pd.read_csv(feats_src + 'other_features/train_turkewitz_feats_orig.csv')\n",
    "    turkewitz_feats = turkewitz_feats[['q1_freq', 'q2_freq']]\n",
    "    turkewitz_feats['freq_sum'] = turkewitz_feats.q1_freq + turkewitz_feats.q2_freq\n",
    "    turkewitz_feats['freq_diff'] = turkewitz_feats.q1_freq - turkewitz_feats.q2_freq\n",
    "    turkewitz_feats['freq_mult'] = turkewitz_feats.q1_freq * turkewitz_feats.q2_freq\n",
    "    turkewitz_feats['freq_div'] = turkewitz_feats.q1_freq / turkewitz_feats.q2_freq\n",
    "    \n",
    "    xgb_feats.drop(['z_len1', 'z_len2', 'z_word_len1', 'z_word_len2'], axis = 1, inplace = True)\n",
    "    y_train = xgb_feats['is_duplicate']\n",
    "    xgb_feats = xgb_feats.iloc[:, 8:]\n",
    "    \n",
    "    df = pd.concat([xgb_feats, abhishek_feats, text_feats, img_feats, \n",
    "                               turkewitz_feats, mephisto_feats], axis = 1)\n",
    "    df = pd.DataFrame(df)\n",
    "    dfc = df.iloc[0:1000,:]\n",
    "    dfc = dfc.T.drop_duplicates().T\n",
    "    duplicate_cols = sorted(list(set(df.columns).difference(set(dfc.columns))))\n",
    "    print('Dropping duplicate columns:', duplicate_cols)\n",
    "    df.drop(duplicate_cols, axis = 1, inplace = True)\n",
    "    print('Final shape:', df.shape)\n",
    "    \n",
    "    keras_q1 = pd.DataFrame(keras_q1)\n",
    "    keras_q2 = pd.DataFrame(keras_q2)\n",
    "    keras_q1.columns = ['question1_{}'.format(i) for i in range(keras_q1.shape[1])]\n",
    "    keras_q2.columns = ['question2_{}'.format(i) for i in range(keras_q2.shape[1])]\n",
    "    X = pd.concat([keras_q1, keras_q2, df], axis = 1)\n",
    "    #X = X.astype('float32')\n",
    "    print('Training data shape:', X.shape)\n",
    "    return X, y_train\n",
    "\n",
    "def labelcount_encode(df2, cols):\n",
    "    df = df2.copy()\n",
    "    categorical_features = cols\n",
    "    new_df = pd.DataFrame()\n",
    "    for cat_feature in categorical_features:\n",
    "        cat_feature_value_counts = df[cat_feature].value_counts()\n",
    "        value_counts_list = cat_feature_value_counts.index.tolist()\n",
    "        value_counts_range_rev = list(reversed(range(len(cat_feature_value_counts)))) # for ascending ordering\n",
    "        value_counts_range = list(range(len(cat_feature_value_counts))) # for descending ordering\n",
    "        labelcount_dict = dict(zip(value_counts_list, value_counts_range))\n",
    "        new_df[cat_feature] = df[cat_feature].map(labelcount_dict)\n",
    "    return new_df\n",
    "\n",
    "def count_encode(df2, cols):\n",
    "    df = df2.copy()\n",
    "    categorical_features = cols\n",
    "    new_df = pd.DataFrame()\n",
    "    for i in categorical_features:\n",
    "        new_df[i] = df[i].astype('object').replace(df[i].value_counts())\n",
    "    return new_df\n",
    "\n",
    "def bin_numerical(df2, cols, step):\n",
    "    df = df2.copy()\n",
    "    numerical_features = cols\n",
    "    new_df = pd.DataFrame()\n",
    "    for i in numerical_features:\n",
    "        feature_range = np.arange(0, np.max(df[i]), step)\n",
    "        new_df[i] = np.digitize(df[i], feature_range, right=True)\n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def train_xgb(cv = False):\n",
    "    \n",
    "    t = time.time()\n",
    "    params = {\n",
    "    'seed': 1337,\n",
    "    'colsample_bytree': 0.48,\n",
    "    'silent': 1,\n",
    "    'subsample': 0.74,\n",
    "    'eta': 0.05,\n",
    "    'objective': 'binary:logistic',\n",
    "    'eval_metric': 'logloss',\n",
    "    'max_depth': 12,\n",
    "    'min_child_weight': 20,\n",
    "    'nthread': 8,\n",
    "    'tree_method': 'hist',\n",
    "    #'updater': 'grow_gpu',\n",
    "    }\n",
    "    \n",
    "    X_train, y_train = get_train()\n",
    "    \n",
    "    if cv:\n",
    "        dtrain = xgb.DMatrix(X_train, y_train)\n",
    "        hist = xgb.cv(params, dtrain, num_boost_round = 100000, nfold = 5,\n",
    "                      stratified = True, early_stopping_rounds = 350, verbose_eval = 250,\n",
    "                      seed = 1337)\n",
    "        del X_train, y_train\n",
    "        gc.collect()\n",
    "        print('Time it took to train in CV manner:', time.time() - t)\n",
    "        return hist\n",
    "    \n",
    "    else:\n",
    "        X_tr, X_val, y_tr, y_val = train_test_split(X_train, y_train, stratify = y_train,\n",
    "                                                    test_size = 0.2, random_state = 111)\n",
    "        del X_train, y_train\n",
    "        gc.collect()\n",
    "        dtrain = xgb.DMatrix(X_tr, label = y_tr)\n",
    "        dval = xgb.DMatrix(X_val, label = y_val)\n",
    "        watchlist = [(dtrain, 'train'), (dval, 'valid')]\n",
    "\n",
    "        print('Start training...')\n",
    "        gbm = xgb.train(params, dtrain, 100000, watchlist, \n",
    "                        early_stopping_rounds = 350, verbose_eval = 100)\n",
    "\n",
    "        print('Start predicting...')\n",
    "        val_pred = gbm.predict(xgb.DMatrix(X_val), ntree_limit=gbm.best_ntree_limit)\n",
    "        score = log_loss(y_val, val_pred)\n",
    "        print('Final score:', score, '\\n', 'Time it took to train and predict:', time.time() - t)\n",
    "        \n",
    "        del X_tr, X_val, y_tr, y_val\n",
    "        gc.collect()\n",
    "        return gbm\n",
    "    \n",
    "\n",
    "def run_xgb(model_name, train = True, test = False, cv = False):\n",
    "    if cv:\n",
    "        gbm_hist = train_xgb(True)\n",
    "        return gbm_hist\n",
    "    if train:\n",
    "        gbm = train_xgb()\n",
    "        gbm.save_model('saved_models/XGB/{}.txt'.format(model_name))\n",
    "        if test:\n",
    "            predict_test('{}'.format(model_name))\n",
    "        return gbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_transformations_features(transformations_src, mode = 'train'):\n",
    "    print('Adding features based on data transformations.')\n",
    "    lsa10tr_3grams_q1 = np.load(transformations_src + '{}_lsa10_3grams.npy'.format(mode))[0]\n",
    "    lsa10tr_3grams_q2 = np.load(transformations_src + '{}_lsa10_3grams.npy'.format(mode))[1]\n",
    "    \n",
    "    transforms_feats = pd.DataFrame()\n",
    "    transforms_feats['cosine'] = [cosine(x, y) for (x,y) in zip(lsa10tr_3grams_q1, lsa10tr_3grams_q2)]\n",
    "    transforms_feats['correlation'] = [correlation(x, y) for (x,y) in zip(lsa10tr_3grams_q1, lsa10tr_3grams_q2)]\n",
    "    transforms_feats['jaccard'] = [jaccard(x, y) for (x,y) in zip(lsa10tr_3grams_q1, lsa10tr_3grams_q2)]\n",
    "    transforms_feats['euclidean'] = [euclidean(x, y) for (x,y) in zip(lsa10tr_3grams_q1, lsa10tr_3grams_q2)]\n",
    "    transforms_feats['minkowski'] = [minkowski(x, y, 3) for (x,y) in zip(lsa10tr_3grams_q1, lsa10tr_3grams_q2)]\n",
    "    return transforms_feats\n",
    "\n",
    "def get_doc2vec_features(doc2vec_src, mode = 'train'):\n",
    "    print('Adding features based on Doc2Vec distances.')\n",
    "    doc2vec_pre_q1 = np.load(doc2vec_src + '{}_q1_doc2vec_vectors_pretrained.npy'.format(mode))\n",
    "    doc2vec_pre_q2 = np.load(doc2vec_src + '{}_q2_doc2vec_vectors_pretrained.npy'.format(mode))\n",
    "    doc2vec_quora_q1 = np.load(doc2vec_src + '{}_q1_doc2vec_vectors_trainquora.npy'.format(mode))\n",
    "    doc2vec_quora_q2 = np.load(doc2vec_src + '{}_q2_doc2vec_vectors_trainquora.npy'.format(mode))\n",
    "    \n",
    "    d2v_feats_pretrained = pd.DataFrame()\n",
    "    d2v_feats_pretrained['cosine'] = [cosine(x, y) for (x,y) in zip(doc2vec_pre_q1, doc2vec_pre_q2)]\n",
    "    d2v_feats_pretrained['correlation'] = [correlation(x, y) for (x,y) in zip(doc2vec_pre_q1, doc2vec_pre_q2)]\n",
    "    d2v_feats_pretrained['jaccard'] = [jaccard(x, y) for (x,y) in zip(doc2vec_pre_q1, doc2vec_pre_q2)]\n",
    "    d2v_feats_pretrained['euclidean'] = [euclidean(x, y) for (x,y) in zip(doc2vec_pre_q1, doc2vec_pre_q2)]\n",
    "    d2v_feats_pretrained['minkowski'] = [minkowski(x, y, 3) for (x,y) in zip(doc2vec_pre_q1, doc2vec_pre_q2)]\n",
    "    \n",
    "    d2v_feats_quora = pd.DataFrame()\n",
    "    d2v_feats_quora['cosine'] = [cosine(x, y) for (x,y) in zip(doc2vec_quora_q1, doc2vec_quora_q2)]\n",
    "    d2v_feats_quora['correlation'] = [correlation(x, y) for (x,y) in zip(doc2vec_quora_q1, doc2vec_quora_q2)]\n",
    "    d2v_feats_quora['jaccard'] = [jaccard(x, y) for (x,y) in zip(doc2vec_quora_q1, doc2vec_quora_q2)]\n",
    "    d2v_feats_quora['euclidean'] = [euclidean(x, y) for (x,y) in zip(doc2vec_quora_q1, doc2vec_quora_q2)]\n",
    "    d2v_feats_quora['minkowski'] = [minkowski(x, y, 3) for (x,y) in zip(doc2vec_quora_q1, doc2vec_quora_q2)]\n",
    "    return d2v_feats_pretrained, d2v_feats_quora\n",
    "\n",
    "def labelcount_encode(df2, cols):\n",
    "    df = df2.copy()\n",
    "    categorical_features = cols\n",
    "    new_df = pd.DataFrame()\n",
    "    for cat_feature in categorical_features:\n",
    "        cat_feature_value_counts = df[cat_feature].value_counts()\n",
    "        value_counts_list = cat_feature_value_counts.index.tolist()\n",
    "        value_counts_range_rev = list(reversed(range(len(cat_feature_value_counts)))) # for ascending ordering\n",
    "        value_counts_range = list(range(len(cat_feature_value_counts))) # for descending ordering\n",
    "        labelcount_dict = dict(zip(value_counts_list, value_counts_range))\n",
    "        new_df[cat_feature] = df[cat_feature].map(labelcount_dict)\n",
    "    return new_df\n",
    "\n",
    "def count_encode(df2, cols):\n",
    "    df = df2.copy()\n",
    "    categorical_features = cols\n",
    "    new_df = pd.DataFrame()\n",
    "    for i in categorical_features:\n",
    "        new_df[i] = df[i].astype('object').replace(df[i].value_counts())\n",
    "    return new_df\n",
    "\n",
    "def bin_numerical(df2, cols, step):\n",
    "    df = df2.copy()\n",
    "    numerical_features = cols\n",
    "    new_df = pd.DataFrame()\n",
    "    for i in numerical_features:\n",
    "        feature_range = np.arange(0, np.max(df[i]), step)\n",
    "        new_df[i] = np.digitize(df[i], feature_range, right=True)\n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#turkewitz_feats = pd.read_csv('/media/w/1c392724-ecf3-4615-8f3c-79368ec36380/DS Projects/Kaggle/Quora/data/features/uncleaned_data/other_features/train_turkewitz_feats_orig.csv')\n",
    "turkewitz_feats = pd.read_csv('/media/w/1c392724-ecf3-4615-8f3c-79368ec36380/DS Projects/Kaggle/Quora/data/features/lemmatized_fullclean/train_turkewitz_features_fullcleanSTEMMED.csv')\n",
    "turkewitz_feats = turkewitz_feats[['q1_freq', 'q2_freq']]\n",
    "\n",
    "ff1 = turkewitz_feats.groupby(['q2_freq'])['q1_freq'].transform('sum')\n",
    "ff2 = turkewitz_feats.groupby(['q1_freq'])['q2_freq'].transform('sum')\n",
    "ff1 = ff1 / np.max(ff1)\n",
    "ff2 = ff2 / np.max(ff2)\n",
    "ff1m = turkewitz_feats.groupby(['q2_freq'])['q1_freq'].transform('mean')\n",
    "ff2m = turkewitz_feats.groupby(['q1_freq'])['q2_freq'].transform('mean')\n",
    "ff1m = ff1m / np.max(ff1m)\n",
    "ff2m = ff2m / np.max(ff2m)\n",
    "gr_feats = pd.DataFrame()\n",
    "gr_feats['ff1'] = ff1\n",
    "gr_feats['ff2'] = ff2\n",
    "gr_feats['ff1m'] = ff1m\n",
    "gr_feats['ff2m'] = ff2m\n",
    "\n",
    "train_lc3 = labelcount_encode(turkewitz_feats, ['q1_freq', 'q2_freq'])\n",
    "train_c = count_encode(turkewitz_feats, ['q1_freq', 'q2_freq'])\n",
    "train_c.q1_freq = train_c.q1_freq / np.max(train_c.q1_freq)\n",
    "train_c.q2_freq = train_c.q2_freq / np.max(train_c.q2_freq)\n",
    "\n",
    "new_feats = pd.concat([train_c, gr_feats], axis = 1)\n",
    "new_feats.drop(['q1_freq', 'q2_freq'], axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# https://github.com/diefimov/santander_2016/blob/master/dmitry/santander_preprocess.py\n",
    "def add_likelihood_feature(fname, train_likeli, test_likeli, flist):\n",
    "    tt_likeli = pd.DataFrame()\n",
    "    np.random.seed(1232345)\n",
    "    skf = StratifiedKFold(train_likeli['TARGET'].values, n_folds=5, shuffle=True, random_state=111)\n",
    "    for train_index, test_index in skf:\n",
    "        ids = train_likeli['ID'].values[train_index]\n",
    "        train_fold = train_likeli.loc[train_likeli['ID'].isin(ids)].copy()\n",
    "        test_fold = train_likeli.loc[~train_likeli['ID'].isin(ids)].copy()\n",
    "        global_avg = np.mean(train_fold['TARGET'].values)\n",
    "        feats_likeli = train_fold.groupby(fname)['TARGET'].agg({'sum': np.sum, 'count': len}).reset_index()\n",
    "        feats_likeli[fname + '_likeli'] = (feats_likeli['sum'] + 30.0*global_avg)/(feats_likeli['count']+30.0)\n",
    "        test_fold = pd.merge(test_fold, feats_likeli[[fname, fname + '_likeli']], on=fname, how='left')\n",
    "        test_fold[fname + '_likeli'] = test_fold[fname + '_likeli'].fillna(global_avg)\n",
    "        tt_likeli = tt_likeli.append(test_fold[['ID', fname + '_likeli']], ignore_index=True)\n",
    "    train_likeli = pd.merge(train_likeli, tt_likeli, on='ID', how='left')\n",
    "    \n",
    "    global_avg = np.mean(train_likeli['TARGET'].values)\n",
    "    feats_likeli = train_likeli.groupby(fname)['TARGET'].agg({'sum': np.sum, 'count': len}).reset_index()\n",
    "    feats_likeli[fname + '_likeli'] = (feats_likeli['sum'] + 30.0*global_avg)/(feats_likeli['count']+30.0)\n",
    "    test_likeli = pd.merge(test_likeli, feats_likeli[[fname, fname + '_likeli']], on=fname, how='left')\n",
    "    test_likeli[fname + '_likeli'] = test_likeli[fname + '_likeli'].fillna(global_avg)\n",
    "    return train_likeli[fname + '_likeli'], test_likeli[fname + '_likeli'], flist + [fname + '_likeli']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropping duplicate columns: ['duplicated', 'len1', 'len2', 'm_q1_q2_tf_svd1']\n",
      "Final shape: (404290, 203)\n",
      "Training data shape: (404290, 459)\n"
     ]
    }
   ],
   "source": [
    "X_train, y_train = get_train()\n",
    "X_train = pd.concat([X_train, new_feats], axis = 1)\n",
    "X_train['TARGET'] = y_train.values\n",
    "X_train['ID'] = y_train.index\n",
    "\n",
    "X_tr, X_val, y_tr, y_val = train_test_split(X_train, y_train, stratify = y_train.values,\n",
    "                                            test_size = 0.2, random_state = 111)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "trs = []\n",
    "vals = []\n",
    "\n",
    "for i in turkewitz_feats.columns.values:\n",
    "    tr_likeli, val_likeli, name = add_likelihood_feature(i, X_tr, X_val, new_feats.columns.values)\n",
    "    trs.append(tr_likeli)\n",
    "    vals.append(val_likeli)\n",
    "    \n",
    "trs = pd.DataFrame(trs).T\n",
    "vals = pd.DataFrame(vals).T\n",
    "trs.index = X_tr.index\n",
    "vals.index = X_val.index\n",
    "    \n",
    "X_tr.drop(['TARGET', 'ID'], axis = 1, inplace = True)\n",
    "X_val.drop(['TARGET', 'ID'], axis = 1, inplace = True)\n",
    "\n",
    "X_tr = pd.concat([X_tr, trs], axis = 1, ignore_index = True)\n",
    "X_val = pd.concat([X_val, vals], axis = 1, ignore_index = True)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "d2v_pre = np.load('train_doc2vec_pretrained_distances.npy')\n",
    "d2v_quora = np.load('train_doc2vec_quoratrain_distances.npy')\n",
    "transforms = np.load('train_transformations_distances.npy')\n",
    "\n",
    "X_train = np.concatenate([X_train, d2v_pre, d2v_quora, transforms, new_feats], axis = 1)\n",
    "X_train = X_train.astype('float32')\n",
    "print('Final shape:', X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training...\n",
      "[0]\ttrain-logloss:0.665886\tvalid-logloss:0.666463\n",
      "Multiple eval metrics have been passed: 'valid-logloss' will be used for early stopping.\n",
      "\n",
      "Will train until valid-logloss hasn't improved in 150 rounds.\n",
      "[100]\ttrain-logloss:0.26042\tvalid-logloss:0.298651\n",
      "[200]\ttrain-logloss:0.22209\tvalid-logloss:0.287471\n",
      "[300]\ttrain-logloss:0.195836\tvalid-logloss:0.283156\n",
      "[400]\ttrain-logloss:0.174747\tvalid-logloss:0.280649\n",
      "[500]\ttrain-logloss:0.156199\tvalid-logloss:0.279128\n",
      "[600]\ttrain-logloss:0.140765\tvalid-logloss:0.278161\n",
      "[700]\ttrain-logloss:0.126656\tvalid-logloss:0.277331\n",
      "[800]\ttrain-logloss:0.115246\tvalid-logloss:0.276911\n",
      "[900]\ttrain-logloss:0.104378\tvalid-logloss:0.276798\n",
      "[1000]\ttrain-logloss:0.095558\tvalid-logloss:0.276557\n",
      "[1100]\ttrain-logloss:0.087241\tvalid-logloss:0.276677\n",
      "Stopping. Best iteration:\n",
      "[963]\ttrain-logloss:0.098768\tvalid-logloss:0.276512\n",
      "\n",
      "Start predicting...\n",
      "Final score: 0.27651184703 \n",
      " Time it took to train and predict: 577.9848575592041\n"
     ]
    }
   ],
   "source": [
    "params = {\n",
    "    'seed': 1337,\n",
    "    'colsample_bytree': 0.48,\n",
    "    'silent': 1,\n",
    "    'subsample': 0.74,\n",
    "    'eta': 0.05,\n",
    "    'objective': 'binary:logistic',\n",
    "    'eval_metric': 'logloss',\n",
    "    'max_depth': 12,\n",
    "    'min_child_weight': 20,\n",
    "    'nthread': 8,\n",
    "    'tree_method': 'hist',\n",
    "    }\n",
    "\n",
    "t = time.time()\n",
    "\n",
    "dtrain = xgb.DMatrix(X_tr, label = y_tr)\n",
    "dval = xgb.DMatrix(X_val, label = y_val)\n",
    "watchlist = [(dtrain, 'train'), (dval, 'valid')]\n",
    "\n",
    "print('Start training...')\n",
    "gbm = xgb.train(params, dtrain, 100000, watchlist, \n",
    "                early_stopping_rounds = 150, verbose_eval = 100)\n",
    "\n",
    "print('Start predicting...')\n",
    "val_pred = gbm.predict(xgb.DMatrix(X_val), ntree_limit=gbm.best_ntree_limit)\n",
    "score = log_loss(y_val, val_pred)\n",
    "print('Final score:', score, '\\n', 'Time it took to train and predict:', time.time() - t)\n",
    "gbm.save_model('saved_models/XGB/XGB_new_EncodingExperiments.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
