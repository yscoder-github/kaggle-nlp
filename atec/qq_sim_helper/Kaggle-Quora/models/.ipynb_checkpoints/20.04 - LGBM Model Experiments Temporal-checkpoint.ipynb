{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/w/anaconda3/envs/idp3/lib/python3.5/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from collections import Counter\n",
    "from sklearn.metrics import log_loss\n",
    "from scipy.optimize import minimize\n",
    "import multiprocessing\n",
    "import difflib\n",
    "import time\n",
    "import gc\n",
    "\n",
    "import xgboost as xgb\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "import lightgbm as lgb\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_train():\n",
    "    keras_q1 = np.load('../../data/transformed/keras_tokenizer/train_q1_transformed.npy')\n",
    "    keras_q2 = np.load('../../data/transformed/keras_tokenizer/train_q2_transformed.npy')\n",
    "    xgb_feats = pd.read_csv('../../data/features/the_1owl/owl_train.csv')\n",
    "    abhishek_feats = pd.read_csv('../../data/features/abhishek/train_features.csv',\n",
    "                              encoding = 'ISO-8859-1').iloc[:, 2:]\n",
    "    text_feats = pd.read_csv('../../data/features/other_features/text_features_train.csv',\n",
    "                            encoding = 'ISO-8859-1')\n",
    "    img_feats = pd.read_csv('../../data/features/other_features/img_features_train.csv')\n",
    "    srk_feats = pd.read_csv('../../data/features/srk/SRK_grams_features_train.csv')\n",
    "\n",
    "    xgb_feats.drop(['z_len1', 'z_len2', 'z_word_len1', 'z_word_len2'], axis = 1, inplace = True)\n",
    "    y_train = xgb_feats['is_duplicate']\n",
    "    xgb_feats = xgb_feats.iloc[:, 8:]\n",
    "    \n",
    "    X_train2 = np.concatenate([keras_q1, keras_q2, xgb_feats, abhishek_feats, text_feats, img_feats], axis = 1)\n",
    "    #X_train2 = np.concatenate([xgb_feats, abhishek_feats, text_feats, img_feats], axis = 1)\n",
    "    #X_train2 = np.concatenate([xgb_feats], axis = 1)\n",
    "    for i in range(X_train2.shape[1]):\n",
    "        if np.sum(X_train2[:, i] == y_train.values) == X_train2.shape[0]:\n",
    "            print('LEAK FOUND')\n",
    "    \n",
    "    X_train2 = X_train2.astype('float32')\n",
    "    X_train2 = pd.DataFrame(X_train2)\n",
    "    X_train2['is_duplicate'] = y_train\n",
    "    print('Training data shape:', X_train2.shape)\n",
    "    return X_train2, y_train\n",
    "\n",
    "def get_test():\n",
    "    keras_q1 = np.load('../../data/transformed/keras_tokenizer/test_q1_transformed.npy')\n",
    "    keras_q2 = np.load('../../data/transformed/keras_tokenizer/test_q2_transformed.npy')\n",
    "    xgb_feats = pd.read_csv('../../data/features/the_1owl/owl_test.csv')\n",
    "    abhishek_feats = pd.read_csv('../../data/features/abhishek/test_features.csv',\n",
    "                              encoding = 'ISO-8859-1').iloc[:, 2:]\n",
    "    text_feats = pd.read_csv('../../data/features/other_features/text_features_test.csv',\n",
    "                            encoding = 'ISO-8859-1')\n",
    "    img_feats = pd.read_csv('../../data/features/other_features/img_features_test.csv')\n",
    "    srk_feats = pd.read_csv('../../data/features/srk/SRK_grams_features_test.csv')\n",
    "\n",
    "    xgb_feats.drop(['z_len1', 'z_len2', 'z_word_len1', 'z_word_len2'], axis = 1, inplace = True)\n",
    "    xgb_feats = xgb_feats.iloc[:, 5:]\n",
    "    \n",
    "    X_test2 = np.concatenate([keras_q1, keras_q2, xgb_feats, abhishek_feats, text_feats, img_feats], axis = 1)\n",
    "    #X_test2 = np.concatenate([keras_q1, keras_q2, xgb_feats, abhishek_feats, text_feats], axis = 1)\n",
    "    \n",
    "    X_test2 = X_test2.astype('float32')\n",
    "    X_test2 = pd.DataFrame(X_test2)\n",
    "    print('Test data shape:', X_test2.shape)\n",
    "    return X_test2\n",
    "\n",
    "def predict_test(model_name):\n",
    "    X_test = get_test()\n",
    "    gbm = lgb.Booster(model_file='saved_models/LGBM/{}.txt'.format(model_name))\n",
    "    test_preds = gbm.predict(lgb.Dataset(X_test))\n",
    "\n",
    "    sub_src = '/media/w/1c392724-ecf3-4615-8f3c-79368ec36380/DS Projects/Kaggle/Quora/submissions/'\n",
    "    sample_sub = pd.read_csv(sub_src + 'sample_submission.csv')\n",
    "    sample_sub['is_duplicate'] = test_preds\n",
    "    sample_sub.to_csv(sub_src + '{}.csv'.format(model_name), index = False)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def oversample(X_train, y_train):\n",
    "    print('Oversampling negative y according to anokas method')\n",
    "    pos_train = X_train[X_train['is_duplicate'] == 1]\n",
    "    neg_train = X_train[X_train['is_duplicate'] == 0]\n",
    "    p = 0.165\n",
    "    scale = ((len(pos_train) / (len(pos_train) + len(neg_train))) / p) - 1\n",
    "    while scale > 1:\n",
    "        neg_train = pd.concat([neg_train, neg_train])\n",
    "        scale -=1\n",
    "    neg_train = pd.concat([neg_train, neg_train[:int(scale * len(neg_train))]])\n",
    "    X_train = pd.concat([pos_train, neg_train])\n",
    "    y_train = (np.zeros(len(pos_train)) + 1).tolist() + np.zeros(len(neg_train)).tolist()\n",
    "\n",
    "    X_train = X_train.astype('float32')\n",
    "    X_train.drop(['is_duplicate'], axis = 1, inplace = True)\n",
    "    return X_train, y_train\n",
    "\n",
    "def oversample2(X_train):\n",
    "    print('Oversampling negative y according to SRK method')\n",
    "    y_train = np.array(X_train[\"is_duplicate\"])\n",
    "    X_train.drop(['is_duplicate'], axis = 1, inplace = True)\n",
    "    X_train_dup = X_train[y_train==1]\n",
    "    X_train_non_dup = X_train[y_train==0]\n",
    "\n",
    "    X_train = np.vstack([X_train_non_dup, X_train_dup, X_train_non_dup, X_train_non_dup])\n",
    "    y_train = np.array([0]*X_train_non_dup.shape[0] + [1]*X_train_dup.shape[0] + [0]*X_train_non_dup.shape[0] + [0]*X_train_non_dup.shape[0])\n",
    "    del X_train_dup\n",
    "    del X_train_non_dup\n",
    "    print(\"Mean target rate : \",y_train.mean())\n",
    "    X_train = X_train.astype('float32')\n",
    "    return X_train, y_train\n",
    "\n",
    "def kappa(preds, y):\n",
    "    score = []\n",
    "    a = 0.165 / 0.37\n",
    "    b = (1 - 0.165) / (1 - 0.37)\n",
    "    for pp,yy in zip(preds, y.get_label()):\n",
    "        score.append(a * yy * np.log (pp) + b * (1 - yy) * np.log(1-pp))\n",
    "    score = -np.sum(score) / len(score)\n",
    "    return 'kappa', score\n",
    "\n",
    "def get_temporal_pattern(df2):\n",
    "    df = df2.copy()\n",
    "    df[\"qmax\"] = df.apply( lambda row: max(row[\"qid1\"], row[\"qid2\"]), axis=1 )\n",
    "    df = df.sort_values(by=[\"qmax\"], ascending=True)\n",
    "    df[\"dupe_rate\"] = df.is_duplicate.rolling(window=500, min_periods=500).mean()\n",
    "    df[\"timeline\"] = np.arange(df.shape[0]) / float(df.shape[0])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def train_lgb(cv = False):\n",
    "    \n",
    "    t = time.time()\n",
    "    params = {\n",
    "        'task' : 'train',\n",
    "        'boosting_type' : 'gbdt',\n",
    "        'objective' : 'binary',\n",
    "        'metric' : {'binary_logloss'},\n",
    "        'learning_rate' : 0.05,\n",
    "        'feature_fraction' : 0.9,\n",
    "        'bagging_fraction': 0.8,\n",
    "        'bagging_freq': 100,\n",
    "        'num_leaves' : 200,\n",
    "        'max_depth': 4,\n",
    "        'min_data_in_leaf': 1,\n",
    "        'subsample': 0.7,\n",
    "        'colsample_bytree': 0.7,\n",
    "        'silent': 1,\n",
    "        'random_state': 1337,\n",
    "        'verbose': 1,\n",
    "        'nthread': 6,\n",
    "    }\n",
    "    \n",
    "    X_train, _ = get_train()\n",
    "    X_train, y_train = oversample2(X_train)\n",
    "    if cv:\n",
    "        lgb_train = lgb.Dataset(X_train, y_train)\n",
    "        hist = lgb.cv(params, lgb_train, num_boost_round = 100000, nfold = 5,\n",
    "                      stratified = True, early_stopping_rounds = 350, verbose_eval = 250,\n",
    "                      seed = 1337)\n",
    "        del X_train, y_train\n",
    "        gc.collect()\n",
    "        print('Time it took to train in CV manner:', time.time() - t)\n",
    "        return hist\n",
    "    \n",
    "    else:\n",
    "        X_tr, X_val, y_tr, y_val = train_test_split(X_train, y_train, stratify = y_train,\n",
    "                                                    test_size = 0.2, random_state = 111)\n",
    "        del X_train, y_train\n",
    "        gc.collect()\n",
    "        lgb_train = lgb.Dataset(X_tr, y_tr)\n",
    "        lgb_val = lgb.Dataset(X_val, y_val)\n",
    "\n",
    "        print('Start training...')\n",
    "        gbm = lgb.train(params, lgb_train, num_boost_round = 100000, valid_sets = lgb_val,\n",
    "                        early_stopping_rounds = 350, verbose_eval = 500)\n",
    "\n",
    "        print('Start predicting...')\n",
    "        val_pred = gbm.predict(lgb.Dataset(X_val), num_iteration=gbm.best_iteration)\n",
    "        score = log_loss(y_val, val_pred)\n",
    "        print('Final score:', score, '\\n', 'Time it took to train and predict:', time.time() - t)\n",
    "        \n",
    "        del X_tr, X_val, y_tr, y_val\n",
    "        gc.collect()\n",
    "        return gbm\n",
    "    \n",
    "\n",
    "def run_lgbm(model_name, train = True, test = False, cv = False):\n",
    "    if cv:\n",
    "        gbm_hist = train_lgb(True)\n",
    "        return gbm_hist\n",
    "    if train:\n",
    "        gbm = train_lgb()\n",
    "        gbm.save_model('saved_models/LGBM/{}.txt'.format(model_name))\n",
    "        if test:\n",
    "            predict_test('{}'.format(model_name))\n",
    "        return gbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "gbm = run_lgbm(train = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape: (404290, 247)\n"
     ]
    }
   ],
   "source": [
    "input_folder = '/media/w/1c392724-ecf3-4615-8f3c-79368ec36380/DS Projects/Kaggle/Quora/data/'\n",
    "\n",
    "df_train = pd.read_csv(input_folder + 'train.csv')\n",
    "X_train, y_train = get_train()\n",
    "\n",
    "X_train['qid1'] = df_train['qid1']\n",
    "X_train['qid2'] = df_train['qid2']\n",
    "X_traintemp = get_temporal_pattern(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/w/anaconda3/envs/idp3/lib/python3.5/site-packages/ipykernel_launcher.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  after removing the cwd from sys.path.\n",
      "/home/w/anaconda3/envs/idp3/lib/python3.5/site-packages/ipykernel_launcher.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Oversampling negative y according to SRK method\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/w/anaconda3/envs/idp3/lib/python3.5/site-packages/ipykernel_launcher.py:22: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean target rate :  0.175685821108\n"
     ]
    }
   ],
   "source": [
    "X_tr = X_traintemp.iloc[:360000, :]\n",
    "X_val = X_traintemp.iloc[:360000, :]\n",
    "\n",
    "X_tr.drop(['qid1', 'qid2', 'qmax', 'dupe_rate'], axis = 1, inplace = True)\n",
    "X_val.drop(['qid1', 'qid2', 'qmax', 'dupe_rate'], axis = 1, inplace = True)\n",
    "\n",
    "X_tr, y_tr = oversample2(X_tr)\n",
    "y_val = X_val['is_duplicate']\n",
    "X_val.drop(['is_duplicate'], axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "params = {\n",
    "        'task' : 'train',\n",
    "        'boosting_type' : 'gbdt',\n",
    "        'objective' : 'binary',\n",
    "        'metric' : {'binary_logloss'},\n",
    "        'learning_rate' : 0.05,\n",
    "        'feature_fraction' : 0.9,\n",
    "        'bagging_fraction': 0.8,\n",
    "        'bagging_freq': 100,\n",
    "        'num_leaves' : 200,\n",
    "        'max_depth': 4,\n",
    "        'min_data_in_leaf': 1,\n",
    "        'subsample': 0.7,\n",
    "        'colsample_bytree': 0.7,\n",
    "        'silent': 1,\n",
    "        'random_state': 1337,\n",
    "        'verbose': 1,\n",
    "        'nthread': 6,\n",
    "    }\n",
    "\n",
    "t = time.time()\n",
    "lgb_train = lgb.Dataset(X_tr, y_tr)\n",
    "lgb_val = lgb.Dataset(X_val, y_val)\n",
    "\n",
    "print('Start training...')\n",
    "gbm = lgb.train(params, lgb_train, num_boost_round = 100000, valid_sets = lgb_val,\n",
    "                early_stopping_rounds = 350, verbose_eval = 500)\n",
    "\n",
    "print('Start predicting...')\n",
    "val_pred = gbm.predict(lgb.Dataset(X_val), num_iteration=gbm.best_iteration)\n",
    "score = log_loss(y_val, val_pred)\n",
    "print('Final score:', score, '\\n', 'Time it took to train and predict:', time.time() - t)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
