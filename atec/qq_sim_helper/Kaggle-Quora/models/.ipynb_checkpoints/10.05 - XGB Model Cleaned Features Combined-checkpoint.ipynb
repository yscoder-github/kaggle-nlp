{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/w/anaconda3/envs/idp3/lib/python3.5/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from collections import Counter\n",
    "from sklearn.metrics import log_loss\n",
    "from scipy.optimize import minimize\n",
    "import multiprocessing\n",
    "import difflib\n",
    "import time\n",
    "import gc\n",
    "\n",
    "import xgboost as xgb\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from pandas.core.common import array_equivalent\n",
    "\n",
    "from xgb_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_train():\n",
    "    abhishek_feats = pd.read_csv('../../data/features/abhishek/train_features.csv',\n",
    "                      encoding = 'ISO-8859-1').iloc[:, 2:]\n",
    "    text_feats = pd.read_csv('../../data/features/spacylemmat_fullclean/train_whq_with_jaccard_feats.csv')\n",
    "    eda_feats = pd.read_csv('../../data/features/spacylemmat_fullclean/train_eda_features.csv')\n",
    "    mephisto_feats = pd.read_csv('../../data/features/spacylemmat_fullclean/train_mephistopeheles_features.csv')\n",
    "    turkewitz_feats = pd.read_csv('../../data/features/spacylemmat_fullclean/train_turkewitz_features.csv')\n",
    "    srk_feats = pd.read_csv('../../data/features/spacylemmat_fullclean/train_SRKgrams_features.csv')\n",
    "    turkewitz_feats = turkewitz_feats[['q1_freq', 'q2_freq']]\n",
    "    \n",
    "    q1 = np.load('../features/q1train_spacylemmat_fullclean_170len_treetrunc.npy')\n",
    "    q2 = np.load('../features/q2train_spacylemmat_fullclean_170len_treetrunc.npy')\n",
    "\n",
    "    df = pd.concat([mephisto_feats, abhishek_feats, turkewitz_feats], axis = 1)\n",
    "    df2 = pd.concat([eda_feats, text_feats, srk_feats], axis = 1)\n",
    "    df = df.merge(df2, on = 'id', how = 'left')\n",
    "    print('Original shape:', df.shape)\n",
    "    df.fillna(-999, inplace = True)\n",
    "    \n",
    "    y = df['is_duplicate_y']\n",
    "    \n",
    "    dfc = df.iloc[0:1000,:]\n",
    "    dfc = dfc.T.drop_duplicates().T\n",
    "    duplicate_cols = sorted(list(set(df.columns).difference(set(dfc.columns))))\n",
    "    print('Dropping duplicate columns:', duplicate_cols)\n",
    "    df.drop(duplicate_cols, axis = 1, inplace = True)\n",
    "    print('Final shape:', df.shape)\n",
    "    \n",
    "    df.drop(['is_duplicate_x',], axis = 1, inplace = True)\n",
    "    X = df.iloc[:, 6:]\n",
    "    X.drop(['question1_y','question2_y'], axis = 1, inplace = True)\n",
    "    X = np.concatenate([X.values, q1, q2], axis = 1)\n",
    "    print('Train data loaded.', '\\n', 'Training data shape:', X.shape)\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_test():\n",
    "    abhishek_feats = pd.read_csv('../../data/features/abhishek/test_features.csv',\n",
    "                      encoding = 'ISO-8859-1').iloc[:, 2:]\n",
    "    text_feats = pd.read_csv('../../data/features/spacylemmat_fullclean/test_whq_with_jaccard_feats.csv')\n",
    "    eda_feats = pd.read_csv('../../data/features/spacylemmat_fullclean/test_eda_features.csv')\n",
    "    mephisto_feats = pd.read_csv('../../data/features/spacylemmat_fullclean/test_mephistopeheles_features.csv')\n",
    "    turkewitz_feats = pd.read_csv('../../data/features/spacylemmat_fullclean/test_turkewitz_features.csv')\n",
    "    srk_feats = pd.read_csv('../../data/features/spacylemmat_fullclean/test_SRKgrams_features.csv')\n",
    "    turkewitz_feats = turkewitz_feats[['q1_freq', 'q2_freq']]\n",
    "\n",
    "    q1 = np.load('../../data/transformed/keras_tokenizer/test_q1_transformed.npy')\n",
    "    q2 = np.load('../../data/transformed/keras_tokenizer/test_q2_transformed.npy')\n",
    "    \n",
    "    df = pd.concat([mephisto_feats, abhishek_feats, turkewitz_feats], axis = 1)\n",
    "    df2 = pd.concat([eda_feats, text_feats, srk_feats], axis = 1)\n",
    "    df = df.merge(df2, on = 'test_id', how = 'left')\n",
    "    print('Original shape:', df.shape)\n",
    "    \n",
    "    dfc = df.iloc[0:1000,:]\n",
    "    dfc = dfc.T.drop_duplicates().T\n",
    "    duplicate_cols = sorted(list(set(df.columns).difference(set(dfc.columns))))\n",
    "    print('Dropping duplicate columns:', duplicate_cols)\n",
    "    df.drop(duplicate_cols, axis = 1, inplace = True)\n",
    "    print('Final shape:', df.shape)\n",
    "    \n",
    "    X = df.iloc[:, 4:]\n",
    "    X.drop(['question1_y','question2_y'], axis = 1, inplace = True)\n",
    "    X = np.concatenate([X.values, q1, q2], axis = 1)\n",
    "    print('Test data loaded.', '\\n', 'Test data shape:', X.shape)\n",
    "    return X\n",
    "\n",
    "\n",
    "def predict_test(model_name):\n",
    "    print('Predicting on test set.')\n",
    "    X_test = get_test()\n",
    "    gbm = xgb.Booster(model_file = 'saved_models/XGB/{}.txt'.format(model_name))\n",
    "    test_preds = gbm.predict(xgb.DMatrix(X_test))\n",
    "\n",
    "    sub_src = '/media/w/1c392724-ecf3-4615-8f3c-79368ec36380/DS Projects/Kaggle/Quora/submissions/'\n",
    "    sample_sub = pd.read_csv(sub_src + 'sample_submission.csv')\n",
    "    sample_sub['is_duplicate'] = test_preds\n",
    "    sample_sub.is_duplicate = sample_sub.is_duplicate.apply(transform)\n",
    "    sample_sub.to_csv(sub_src + '{}.csv'.format(model_name), index = False)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def train_xgb(cv = False):\n",
    "    \n",
    "    t = time.time()\n",
    "    params = {\n",
    "    'seed': 1337,\n",
    "    'colsample_bytree': 0.48,\n",
    "    'silent': 1,\n",
    "    'subsample': 0.74,\n",
    "    'eta': 0.05,\n",
    "    'objective': 'binary:logistic',\n",
    "    'eval_metric': 'logloss',\n",
    "    'max_depth': 12,\n",
    "    'min_child_weight': 20,\n",
    "    'nthread': 6,\n",
    "    'tree_method': 'hist',\n",
    "    #'updater': 'grow_gpu_hist',\n",
    "    #'gpu_id': 0,\n",
    "    }\n",
    "    \n",
    "    X_train, y_train = get_train()\n",
    "    X_train = X_train.astype('float32')\n",
    "    \n",
    "    if cv:\n",
    "        dtrain = xgb.DMatrix(X_train, y_train)\n",
    "        hist = xgb.cv(params, dtrain, num_boost_round = 100000, nfold = 5,\n",
    "                      stratified = True, early_stopping_rounds = 350, verbose_eval = 250,\n",
    "                      seed = 1337)\n",
    "        del X_train, y_train\n",
    "        gc.collect()\n",
    "        print('Time it took to train in CV manner:', time.time() - t)\n",
    "        return hist\n",
    "    \n",
    "    else:\n",
    "        X_tr, X_val, y_tr, y_val = train_test_split(X_train, y_train, stratify = y_train,\n",
    "                                                    test_size = 0.2, random_state = 111)\n",
    "        del X_train, y_train\n",
    "        gc.collect()\n",
    "        dtrain = xgb.DMatrix(X_tr, label = y_tr)\n",
    "        dval = xgb.DMatrix(X_val, label = y_val)\n",
    "        watchlist = [(dtrain, 'train'), (dval, 'valid')]\n",
    "\n",
    "        print('Start training...')\n",
    "        gbm = xgb.train(params, dtrain, 100000, watchlist, \n",
    "                        early_stopping_rounds = 350, verbose_eval = 250)\n",
    "\n",
    "        print('Start predicting...')\n",
    "        val_pred = gbm.predict(xgb.DMatrix(X_val), ntree_limit=gbm.best_ntree_limit)\n",
    "        score = log_loss(y_val, val_pred)\n",
    "        print('Final score:', score, '\\n', 'Time it took to train and predict:', time.time() - t)\n",
    "        \n",
    "        del X_tr, X_val, y_tr, y_val\n",
    "        gc.collect()\n",
    "        return gbm\n",
    "    \n",
    "\n",
    "def run_xgb(model_name, train = True, test = False, cv = False):\n",
    "    if cv:\n",
    "        gbm_hist = train_xgb(True)\n",
    "        return gbm_hist\n",
    "    if train:\n",
    "        gbm = train_xgb()\n",
    "        gbm.save_model('saved_models/XGB/{}.txt'.format(model_name))\n",
    "        if test:\n",
    "            predict_test('{}'.format(model_name))\n",
    "        return gbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "gbm = run_xgb('XGB_spacy_lemmat_combinedFeats_origEncoding', train = True, test = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "predict_test('XGB_spacy_lemmat_combinedFeats')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
