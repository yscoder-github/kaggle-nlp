{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/w/anaconda3/envs/idp3/lib/python3.5/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from collections import Counter\n",
    "from sklearn.metrics import log_loss\n",
    "from scipy.optimize import minimize\n",
    "import multiprocessing\n",
    "import difflib\n",
    "import time\n",
    "import gc\n",
    "\n",
    "import xgboost as xgb\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_train():\n",
    "    keras_q1 = np.load('../../data/transformed/keras_tokenizer/train_q1_transformed.npy')\n",
    "    keras_q2 = np.load('../../data/transformed/keras_tokenizer/train_q2_transformed.npy')\n",
    "    xgb_feats = pd.read_csv('../../data/features/the_1owl/owl_train.csv')\n",
    "    abhishek_feats = pd.read_csv('../../data/features/abhishek/train_features.csv',\n",
    "                              encoding = 'ISO-8859-1').iloc[:, 2:]\n",
    "    text_feats = pd.read_csv('../../data/features/other_features/text_features_train.csv',\n",
    "                            encoding = 'ISO-8859-1')\n",
    "    img_feats = pd.read_csv('../../data/features/other_features/img_features_train.csv')\n",
    "    srk_feats = pd.read_csv('../../data/features/srk/SRK_grams_features_train.csv')\n",
    "\n",
    "    xgb_feats.drop(['z_len1', 'z_len2', 'z_word_len1', 'z_word_len2'], axis = 1, inplace = True)\n",
    "    y_train = xgb_feats['is_duplicate']\n",
    "    xgb_feats = xgb_feats.iloc[:, 8:]\n",
    "    \n",
    "    X_train2 = np.concatenate([keras_q1, keras_q2, xgb_feats, abhishek_feats, text_feats, img_feats], axis = 1)\n",
    "    #X_train2 = np.concatenate([xgb_feats, abhishek_feats, text_feats, img_feats], axis = 1)\n",
    "    for i in range(X_train2.shape[1]):\n",
    "        if np.sum(X_train2[:, i] == y_train.values) == X_train2.shape[0]:\n",
    "            print('LEAK FOUND')\n",
    "    \n",
    "    X_train2 = X_train2.astype('float32')\n",
    "    X_train2 = pd.DataFrame(X_train2)\n",
    "    X_train2['is_duplicate'] = y_train\n",
    "    print('Training data shape:', X_train2.shape)\n",
    "    return X_train2, y_train\n",
    "\n",
    "def get_test():\n",
    "    keras_q1 = np.load('../../data/transformed/keras_tokenizer/test_q1_transformed.npy')\n",
    "    keras_q2 = np.load('../../data/transformed/keras_tokenizer/test_q2_transformed.npy')\n",
    "    xgb_feats = pd.read_csv('../../data/features/the_1owl/owl_test.csv')\n",
    "    abhishek_feats = pd.read_csv('../../data/features/abhishek/test_features.csv',\n",
    "                              encoding = 'ISO-8859-1').iloc[:, 2:]\n",
    "    text_feats = pd.read_csv('../../data/features/other_features/text_features_test.csv',\n",
    "                            encoding = 'ISO-8859-1')\n",
    "    img_feats = pd.read_csv('../../data/features/other_features/img_features_test.csv')\n",
    "    srk_feats = pd.read_csv('../../data/features/srk/SRK_grams_features_test.csv')\n",
    "\n",
    "    xgb_feats.drop(['z_len1', 'z_len2', 'z_word_len1', 'z_word_len2'], axis = 1, inplace = True)\n",
    "    xgb_feats = xgb_feats.iloc[:, 5:]\n",
    "    \n",
    "    X_test2 = np.concatenate([keras_q1, keras_q2, xgb_feats, abhishek_feats, text_feats, img_feats], axis = 1)\n",
    "    #X_test2 = np.concatenate([keras_q1, keras_q2, xgb_feats, abhishek_feats, text_feats], axis = 1)\n",
    "    \n",
    "    X_test2 = X_test2.astype('float32')\n",
    "    X_test2 = pd.DataFrame(X_test2)\n",
    "    print('Test data shape:', X_test2.shape)\n",
    "    return X_test2\n",
    "\n",
    "def predict_test(model_name):\n",
    "    X_test = get_test()\n",
    "    gbm = xgb.Booster(model_file = 'saved_models/XGB/{}.txt'.format(model_name))\n",
    "    test_preds = gbm.predict(xgb.DMatrix(X_test))\n",
    "\n",
    "    sub_src = '/media/w/1c392724-ecf3-4615-8f3c-79368ec36380/DS Projects/Kaggle/Quora/submissions/'\n",
    "    sample_sub = pd.read_csv(sub_src + 'sample_submission.csv')\n",
    "    sample_sub['is_duplicate'] = test_preds\n",
    "    sample_sub.to_csv(sub_src + '{}.csv'.format(model_name), index = False)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def oversample():\n",
    "    print('Oversampling negative y according to anokas method')\n",
    "    X_train, y_train = get_train()\n",
    "    pos_train = X_train[X_train['is_duplicate'] == 1]\n",
    "    neg_train = X_train[X_train['is_duplicate'] == 0]\n",
    "    p = 0.165\n",
    "    scale = ((len(pos_train) / (len(pos_train) + len(neg_train))) / p) - 1\n",
    "    while scale > 1:\n",
    "        neg_train = pd.concat([neg_train, neg_train])\n",
    "        scale -=1\n",
    "    neg_train = pd.concat([neg_train, neg_train[:int(scale * len(neg_train))]])\n",
    "    X_train = pd.concat([pos_train, neg_train])\n",
    "    y_train = (np.zeros(len(pos_train)) + 1).tolist() + np.zeros(len(neg_train)).tolist()\n",
    "\n",
    "    X_train = X_train.astype('float32')\n",
    "    X_train.drop(['is_duplicate'], axis = 1, inplace = True)\n",
    "    return X_train, y_train\n",
    "\n",
    "def oversample2(X_train):\n",
    "    print('Oversampling negative y according to SRK method')\n",
    "    y_train = np.array(X_train[\"is_duplicate\"])\n",
    "    X_train.drop(['is_duplicate'], axis = 1, inplace = True)\n",
    "    X_train_dup = X_train[y_train==1]\n",
    "    X_train_non_dup = X_train[y_train==0]\n",
    "\n",
    "    X_train = np.vstack([X_train_non_dup, X_train_dup, X_train_non_dup, X_train_non_dup])\n",
    "    y_train = np.array([0]*X_train_non_dup.shape[0] + [1]*X_train_dup.shape[0] + [0]*X_train_non_dup.shape[0] + [0]*X_train_non_dup.shape[0])\n",
    "    del X_train_dup\n",
    "    del X_train_non_dup\n",
    "    print(\"Mean target rate : \",y_train.mean())\n",
    "    X_train = X_train.astype('float32')\n",
    "    return X_train, y_train\n",
    "\n",
    "def kappa(preds, y):\n",
    "    score = []\n",
    "    a = 0.165 / 0.37\n",
    "    b = (1 - 0.165) / (1 - 0.37)\n",
    "    for pp,yy in zip(preds, y.get_label()):\n",
    "        score.append(a * yy * np.log (pp) + b * (1 - yy) * np.log(1-pp))\n",
    "    score = -np.sum(score) / len(score)\n",
    "    return 'kappa', score\n",
    "\n",
    "def get_temporal_pattern(df2):\n",
    "    df = df2.copy()\n",
    "    df[\"qmax\"] = df.apply( lambda row: max(row[\"qid1\"], row[\"qid2\"]), axis=1 )\n",
    "    df = df.sort_values(by=[\"qmax\"], ascending=True)\n",
    "    df[\"dupe_rate\"] = df.is_duplicate.rolling(window=500, min_periods=500).mean()\n",
    "    df[\"timeline\"] = np.arange(df.shape[0]) / float(df.shape[0])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def train_xgb(cv = False):\n",
    "    \n",
    "    t = time.time()\n",
    "    params = {\n",
    "    'seed': 1337,\n",
    "    'colsample_bytree': 0.7,\n",
    "    'silent': 1,\n",
    "    'subsample': 0.7,\n",
    "    'eta': 0.05,\n",
    "    'objective': 'binary:logistic',\n",
    "    'eval_metric': 'logloss',\n",
    "    'max_depth': 4,\n",
    "    'min_child_weight': 1,\n",
    "    'nthread': 6,\n",
    "    'tree_method': 'hist',\n",
    "    }\n",
    "\n",
    "    \n",
    "    X_train, y_train = get_train()\n",
    "    #X_train = X_train.astype('float32')\n",
    "    #X_train.drop(['is_duplicate'], axis = 1, inplace = True)\n",
    "    X_train, y_train = oversample2(X_train)\n",
    "    if cv:\n",
    "        dtrain = xgb.DMatrix(X_train, y_train)\n",
    "        hist = xgb.cv(params, dtrain, num_boost_round = 100000, nfold = 5,\n",
    "                      stratified = True, early_stopping_rounds = 350, verbose_eval = 250,\n",
    "                      seed = 1337)\n",
    "        del X_train, y_train\n",
    "        gc.collect()\n",
    "        print('Time it took to train in CV manner:', time.time() - t)\n",
    "        return hist\n",
    "    \n",
    "    else:\n",
    "        X_tr, X_val, y_tr, y_val = train_test_split(X_train, y_train, stratify = y_train,\n",
    "                                                    test_size = 0.2, random_state = 111)\n",
    "        del X_train, y_train\n",
    "        gc.collect()\n",
    "        dtrain = xgb.DMatrix(X_tr, label = y_tr)\n",
    "        dval = xgb.DMatrix(X_val, label = y_val)\n",
    "        watchlist = [(dtrain, 'train'), (dval, 'valid')]\n",
    "\n",
    "        print('Start training...')\n",
    "        gbm = xgb.train(params, dtrain, 100000, watchlist, \n",
    "                        early_stopping_rounds = 350, verbose_eval = 250)\n",
    "\n",
    "        print('Start predicting...')\n",
    "        val_pred = gbm.predict(xgb.DMatrix(X_val), ntree_limit=gbm.best_ntree_limit)\n",
    "        score = log_loss(y_val, val_pred)\n",
    "        print('Final score:', score, '\\n', 'Time it took to train and predict:', time.time() - t)\n",
    "        \n",
    "        del X_tr, X_val, y_tr, y_val\n",
    "        gc.collect()\n",
    "        return gbm\n",
    "    \n",
    "\n",
    "def run_xgb(model_name, train = True, test = False, cv = False):\n",
    "    if cv:\n",
    "        gbm_hist = train_xgb(True)\n",
    "        return gbm_hist\n",
    "    if train:\n",
    "        gbm = train_xgb()\n",
    "        gbm.save_model('saved_models/XGB/{}.txt'.format(model_name))\n",
    "        if test:\n",
    "            predict_test('{}'.format(model_name))\n",
    "        return gbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_train():\n",
    "    keras_q1 = np.load('../../data/transformed/keras_tokenizer/train_q1_transformed.npy')\n",
    "    keras_q2 = np.load('../../data/transformed/keras_tokenizer/train_q2_transformed.npy')\n",
    "    xgb_feats = pd.read_csv('../../data/features/the_1owl/owl_train.csv')\n",
    "    abhishek_feats = pd.read_csv('../../data/features/abhishek/train_features.csv',\n",
    "                              encoding = 'ISO-8859-1').iloc[:, 2:]\n",
    "    text_feats = pd.read_csv('../../data/features/other_features/text_features_train.csv',\n",
    "                            encoding = 'ISO-8859-1')\n",
    "    img_feats = pd.read_csv('../../data/features/other_features/img_features_train.csv')\n",
    "    srk_feats = pd.read_csv('../../data/features/srk/SRK_grams_features_train.csv')\n",
    "\n",
    "    xgb_feats.drop(['z_len1', 'z_len2', 'z_word_len1', 'z_word_len2'], axis = 1, inplace = True)\n",
    "    y_train = xgb_feats['is_duplicate']\n",
    "    xgb_feats = xgb_feats.iloc[:, 8:]\n",
    "    \n",
    "    #X_train2 = np.concatenate([keras_q1, keras_q2, xgb_feats, abhishek_feats, text_feats, img_feats], axis = 1)\n",
    "    #X_train2 = np.concatenate([xgb_feats, abhishek_feats, text_feats, img_feats], axis = 1)\n",
    "    X_train2 = np.concatenate([keras_q1, keras_q2, xgb_feats, abhishek_feats, img_feats], axis = 1)\n",
    "    for i in range(X_train2.shape[1]):\n",
    "        if np.sum(X_train2[:, i] == y_train.values) == X_train2.shape[0]:\n",
    "            print('LEAK FOUND')\n",
    "    \n",
    "    X_train2 = X_train2.astype('float32')\n",
    "    X_train2 = pd.DataFrame(X_train2)\n",
    "    X_train2['is_duplicate'] = y_train\n",
    "    print('Training data shape:', X_train2.shape)\n",
    "    return X_train2, y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape: (404290, 210)\n"
     ]
    }
   ],
   "source": [
    "input_folder = '/media/w/1c392724-ecf3-4615-8f3c-79368ec36380/DS Projects/Kaggle/Quora/data/'\n",
    "\n",
    "df_train = pd.read_csv(input_folder + 'train.csv')\n",
    "X_train, y_train = get_train()\n",
    "\n",
    "X_train['qid1'] = df_train['qid1']\n",
    "X_train['qid2'] = df_train['qid2']\n",
    "X_traintemp = get_temporal_pattern(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/w/anaconda3/envs/idp3/lib/python3.5/site-packages/ipykernel_launcher.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  after removing the cwd from sys.path.\n",
      "/home/w/anaconda3/envs/idp3/lib/python3.5/site-packages/ipykernel_launcher.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"\n",
      "/home/w/anaconda3/envs/idp3/lib/python3.5/site-packages/ipykernel_launcher.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n",
      "/home/w/anaconda3/envs/idp3/lib/python3.5/site-packages/ipykernel_launcher.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  # This is added back by InteractiveShellApp.init_path()\n"
     ]
    }
   ],
   "source": [
    "X_tr = X_traintemp.iloc[:360000, :]\n",
    "X_val = X_traintemp.iloc[360000:, :]\n",
    "\n",
    "X_tr.drop(['qid1', 'qid2', 'qmax', 'dupe_rate'], axis = 1, inplace = True)\n",
    "X_val.drop(['qid1', 'qid2', 'qmax', 'dupe_rate'], axis = 1, inplace = True)\n",
    "\n",
    "y_tr = X_tr['is_duplicate']\n",
    "X_tr.drop(['is_duplicate'], axis = 1, inplace = True)\n",
    "\n",
    "y_val = X_val['is_duplicate']\n",
    "X_val.drop(['is_duplicate'], axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training...\n",
      "[0]\ttrain-logloss:0.674218\tvalid-logloss:0.67017\n",
      "Multiple eval metrics have been passed: 'valid-logloss' will be used for early stopping.\n",
      "\n",
      "Will train until valid-logloss hasn't improved in 350 rounds.\n",
      "[250]\ttrain-logloss:0.392507\tvalid-logloss:0.324946\n",
      "[500]\ttrain-logloss:0.374015\tvalid-logloss:0.315331\n",
      "[750]\ttrain-logloss:0.36287\tvalid-logloss:0.310121\n",
      "[1000]\ttrain-logloss:0.353967\tvalid-logloss:0.307269\n",
      "[1250]\ttrain-logloss:0.346235\tvalid-logloss:0.304677\n",
      "[1500]\ttrain-logloss:0.339551\tvalid-logloss:0.303332\n",
      "[1750]\ttrain-logloss:0.333347\tvalid-logloss:0.301864\n",
      "[2000]\ttrain-logloss:0.327589\tvalid-logloss:0.3009\n",
      "[2250]\ttrain-logloss:0.322191\tvalid-logloss:0.300018\n",
      "[2500]\ttrain-logloss:0.317015\tvalid-logloss:0.299543\n",
      "[2750]\ttrain-logloss:0.311972\tvalid-logloss:0.298753\n",
      "[3000]\ttrain-logloss:0.307197\tvalid-logloss:0.298148\n",
      "[3250]\ttrain-logloss:0.30253\tvalid-logloss:0.297568\n",
      "[3500]\ttrain-logloss:0.29814\tvalid-logloss:0.2971\n",
      "[3750]\ttrain-logloss:0.293853\tvalid-logloss:0.296638\n",
      "[4000]\ttrain-logloss:0.289726\tvalid-logloss:0.296476\n",
      "[4250]\ttrain-logloss:0.285698\tvalid-logloss:0.296055\n",
      "[4500]\ttrain-logloss:0.281791\tvalid-logloss:0.295729\n",
      "[4750]\ttrain-logloss:0.277964\tvalid-logloss:0.295639\n",
      "[5000]\ttrain-logloss:0.274213\tvalid-logloss:0.295502\n",
      "Stopping. Best iteration:\n",
      "[4657]\ttrain-logloss:0.279363\tvalid-logloss:0.295499\n",
      "\n",
      "Start predicting...\n",
      "Final score: 0.295499414609 \n",
      " Time it took to train and predict: 732.6083900928497\n"
     ]
    }
   ],
   "source": [
    "params = {\n",
    "    'seed': 1337,\n",
    "    'colsample_bytree': 0.7,\n",
    "    'silent': 1,\n",
    "    'subsample': 0.7,\n",
    "    'eta': 0.05,\n",
    "    'objective': 'binary:logistic',\n",
    "    'eval_metric': 'logloss',\n",
    "    'max_depth': 4,\n",
    "    'min_child_weight': 1,\n",
    "    'nthread': 6,\n",
    "    'tree_method': 'hist',\n",
    "    }\n",
    "\n",
    "t = time.time()\n",
    "dtrain = xgb.DMatrix(X_tr, label = y_tr)\n",
    "dval = xgb.DMatrix(X_val, label = y_val)\n",
    "watchlist = [(dtrain, 'train'), (dval, 'valid')]\n",
    "\n",
    "print('Start training...')\n",
    "gbm = xgb.train(params, dtrain, 100000, watchlist, \n",
    "                early_stopping_rounds = 350, verbose_eval = 250)\n",
    "\n",
    "print('Start predicting...')\n",
    "val_pred = gbm.predict(xgb.DMatrix(X_val), ntree_limit=gbm.best_ntree_limit)\n",
    "score = log_loss(y_val, val_pred)\n",
    "print('Final score:', score, '\\n', 'Time it took to train and predict:', time.time() - t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "predict_test('XGB_nooversampling')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
