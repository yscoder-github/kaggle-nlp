{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import warnings\n",
    "\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.model_selection import KFold\n",
    "import xgboost as xgb\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "sns.set_style(\"dark\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_real_feature(df, fname):\n",
    "    \n",
    "    ix_train = np.where(df['id'] >= 0)[0]\n",
    "    ix_test = np.where(df['id'] == -1)[0]\n",
    "    ix_is_dup = np.where(df['is_duplicate'] == 1)[0]\n",
    "    ix_not_dup = np.where(df['is_duplicate'] == 0)[0]\n",
    "\n",
    "    fig = plt.figure(figsize=(16, 12))\n",
    "    ax1 = plt.subplot2grid((3, 2), (0, 0), colspan=2)\n",
    "    ax2 = plt.subplot2grid((3, 2), (1, 0), colspan=2)\n",
    "    ax3 = plt.subplot2grid((3, 2), (2, 0))\n",
    "    ax4 = plt.subplot2grid((3, 2), (2, 1))\n",
    "    ax1.set_title('Distribution of %s' % fname, fontsize=20)\n",
    "    sns.distplot(df.loc[ix_train][fname], \n",
    "                 bins=50, \n",
    "                 ax=ax1)    \n",
    "    sns.distplot(df.loc[ix_is_dup][fname], \n",
    "                 bins=50, \n",
    "                 ax=ax2,\n",
    "                 label='is dup')    \n",
    "    sns.distplot(df.loc[ix_not_dup][fname], \n",
    "                 bins=50, \n",
    "                 ax=ax2,\n",
    "                 label='not dup')\n",
    "    ax2.legend(loc='upper right', prop={'size': 18})\n",
    "    sns.boxplot(y=fname, \n",
    "                x='is_duplicate', \n",
    "                data=df.loc[ix_train], \n",
    "                ax=ax3)\n",
    "    sns.violinplot(y=fname, \n",
    "                   x='is_duplicate', \n",
    "                   data=df.loc[ix_train], \n",
    "                   ax=ax4)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src = '/media/w/1c392724-ecf3-4615-8f3c-79368ec36380/DS Projects/Kaggle/Quora/scripts/features/'\n",
    "\n",
    "# trdf =  pd.read_csv(src + 'df_train_spacylemmat_fullclean.csv').iloc[:, :-1]\n",
    "# tedf =  pd.read_csv(src + 'df_test_spacylemmat_fullclean.csv').iloc[:, 4:]\n",
    "\n",
    "trdf =  pd.read_csv(src + 'df_train_lemmatfullcleanSTEMMED.csv').iloc[:, :-1]\n",
    "tedf =  pd.read_csv(src + 'df_test_lemmatfullcleanSTEMMED.csv').iloc[:, 4:]\n",
    "\n",
    "#trdf = pd.read_csv('input/train.csv').iloc[:, :-1]\n",
    "#tedf = pd.read_csv('input/test.csv')\n",
    "\n",
    "tr = pd.concat([trdf, tedf], ignore_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = nx.Graph()\n",
    "g.add_nodes_from(tr.question1)\n",
    "g.add_nodes_from(tr.question2)\n",
    "edges = list(tr[['question1', 'question2']].to_records(index=False))\n",
    "g.add_edges_from(edges)\n",
    "\n",
    "print('Number of unique questions:', len(set(tr.question1) | set(tr.question2)), g.number_of_nodes())\n",
    "print('Number of rows in the data:', len(tr), g.number_of_edges())\n",
    "\n",
    "d = g.degree()\n",
    "print('Mean number of connections:', np.mean([d[k] for k in d]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_q_interactions(name):\n",
    "    comb['min_'+name] = comb[['q1_'+name, 'q2_'+name]].min(1)\n",
    "    comb['max_'+name] = comb[['q1_'+name, 'q2_'+name]].max(1)\n",
    "    comb['mean_'+name] = comb[['q1_'+name, 'q2_'+name]].mean(1)\n",
    "    comb['sum_'+name] = comb['q1_'+name] + comb['q2_'+name]\n",
    "    comb['diff_'+name] = abs(comb['q1_'+name] - comb['q2_'+name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "comb = pd.DataFrame()\n",
    "\n",
    "comb['q1_neighbor_count'] = tr['question1'].map(g.neighbors).map(len)\n",
    "comb['q2_neighbor_count'] = tr['question2'].map(g.neighbors).map(len)\n",
    "create_q_interactions('neighbor_count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "comb['shared_neighbor_count'] = tr[['question1', 'question2']].apply(\n",
    "    lambda x: nx.common_neighbors(g, x.question1, x.question2), 1).apply(lambda x: sum(1 for _ in x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "comb_tr = comb.iloc[:trdf.shape[0], :]\n",
    "comb_te = comb.iloc[trdf.shape[0]:, :]\n",
    "comb_te = comb_te.reset_index(drop=True)\n",
    "\n",
    "comb_tr.to_csv('train_network_neighbors.csv', index=False)\n",
    "comb_te.to_csv('test_network_neighbors.csv', index=False)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "train_af = pd.read_csv('input/train_comb_feats.csv')\n",
    "test_af = pd.read_csv('input/test_comb_feats.csv')\n",
    "train_labels = pd.read_csv('train1.csv', usecols=['is_duplicate'], squeeze=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load, drop duplicates and save full feature set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "comb_te = pd.read_csv('test_network_neighbors.csv')\n",
    "test_networkfeats = pd.read_pickle('test_networkfeats.pkl')\n",
    "test_networkfeats = test_networkfeats.reset_index(drop=True)\n",
    "\n",
    "df = pd.concat([test_networkfeats, comb_te], axis = 1)\n",
    "\n",
    "dfc = df.iloc[0:10000,:]\n",
    "dfc = dfc.T.drop_duplicates().T\n",
    "duplicate_cols = sorted(list(set(df.columns).difference(set(dfc.columns))))\n",
    "print('Dropping duplicate columns:', duplicate_cols)\n",
    "df.drop(duplicate_cols, axis = 1, inplace = True)\n",
    "print('Final shape:', df.shape)\n",
    "\n",
    "df.to_csv('test_fullnetworkfeatsTony.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comb_tr = pd.read_csv('train_network_neighbors.csv')\n",
    "train_networkfeats = pd.read_pickle('train_networkfeats.pkl')\n",
    "\n",
    "df = pd.concat([train_networkfeats, comb_tr], axis = 1)\n",
    "\n",
    "dfc = df.iloc[0:10000,:]\n",
    "dfc = dfc.T.drop_duplicates().T\n",
    "duplicate_cols = sorted(list(set(df.columns).difference(set(dfc.columns))))\n",
    "print('Dropping duplicate columns:', duplicate_cols)\n",
    "df.drop(duplicate_cols, axis = 1, inplace = True)\n",
    "print('Final shape:', df.shape)\n",
    "\n",
    "df.to_csv('train_fullnetworkfeatsTony.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_networkfeats.drop(['q1_counts', 'q2_counts', 'sum_counts', 'diff_counts'], 1, inplace=True)\n",
    "test_networkfeats.drop(['q1_counts', 'q2_counts', 'sum_counts', 'diff_counts'], 1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def quick_xgb(train_feats, test_feats, train=train_af, test=test_af, pred_trans=True,\n",
    "              train_labels=train_labels, weights=np.ones(len(train_af))):\n",
    "    train_id = np.arange(len(train_af))\n",
    "    test_id = np.arange(len(test_af))\n",
    "    \n",
    "    train = pd.concat([train, train_feats], 1)\n",
    "    test = pd.concat([test, test_feats], 1)\n",
    "    \n",
    "    params = {}\n",
    "    params[\"objective\"] = \"binary:logistic\"\n",
    "    params['eval_metric'] = ['logloss']\n",
    "    params[\"eta\"] = 0.2\n",
    "    params[\"subsample\"] = 0.7\n",
    "    params[\"min_child_weight\"] = 5\n",
    "    params[\"colsample_bytree\"] = 0.5\n",
    "    #params[\"max_delta_step\"] = 5.0\n",
    "    #params[\"gamma\"] = 10.0\n",
    "    params[\"max_depth\"] = 10\n",
    "    params[\"silent\"] = 1\n",
    "    params[\"seed\"] = 1001\n",
    "    \n",
    "    skf = KFold(n_splits=10, shuffle=True, random_state=1001).split(train_labels)\n",
    "    test_preds = np.zeros(len(test))\n",
    "    for i, (idx_train, idx_val) in enumerate(skf):\n",
    "        val_preds = np.zeros(len(train.iloc[idx_val, :]))\n",
    "        d_train = xgb.DMatrix(train.iloc[idx_train, :], label=train_labels[idx_train], weight=weights[idx_train])\n",
    "        d_valid = xgb.DMatrix(train.iloc[idx_val, :], label=train_labels[idx_val], weight=weights[idx_val])\n",
    "        watchlist = [(d_train, 'train'), (d_valid, 'valid')]\n",
    "        bst = xgb.train(params, d_train, 500000, watchlist, early_stopping_rounds=10, verbose_eval=25)\n",
    "        val_preds = bst.predict(d_valid, ntree_limit=bst.best_ntree_limit)\n",
    "        test_preds = bst.predict(xgb.DMatrix(test), ntree_limit=bst.best_ntree_limit)\n",
    "        break\n",
    "    \n",
    "    loss = log_loss(train_labels[idx_val], val_preds)\n",
    "    \n",
    "    def pred_transform(preds):\n",
    "        a = 0.165 / 0.369191399096\n",
    "        b = (1 - 0.165) / (1 - 0.369191399096)\n",
    "        return a * preds / (a * preds + b * (1 - preds))\n",
    "    if pred_trans:\n",
    "        print(test_id.shape)\n",
    "        print(test_preds.shape)\n",
    "        test_df = pd.DataFrame({\"test_id\": test_id, \"is_duplicate\": pred_transform(test_preds)})\n",
    "    else:\n",
    "        test_df = pd.DataFrame({\"test_id\": test_id, \"is_duplicate\": test_preds})\n",
    "    print('Log Loss:', loss)\n",
    "    print('Accuracy:', (train_labels[idx_val] == np.round(val_preds)).mean())\n",
    "    \n",
    "    now = datetime.datetime.now()\n",
    "    \n",
    "    test_pred_filename = \"model_out/quick_preds_xgb_{:.4f}_{:%Y%m%d_%H%M}.csv.gz\".format(loss, now)\n",
    "    test_df.to_csv(test_pred_filename, index=False, compression='gzip')\n",
    "    \n",
    "    importance = bst.get_fscore()\n",
    "    importance = sorted(importance.items(), key=lambda x:x[1], reverse=True)[:50]\n",
    "    \n",
    "    df = pd.DataFrame(importance, columns=['feature', 'fscore'])\n",
    "    df['fscore'] = df['fscore'] / df['fscore'].sum()\n",
    "    \n",
    "    plt.figure()\n",
    "    df.plot()\n",
    "    df.plot(kind='barh', x='feature', y='fscore', legend=False, figsize=(6, 10))\n",
    "    plt.title('XGBoost Feature Importance')\n",
    "    plt.xlabel('relative importance')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quick_xgb(pd.DataFrame({'i': np.ones(len(train_af))}), pd.DataFrame({'i': np.ones(len(test_af))}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quick_xgb(comb_tr, comb_te)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quick_xgb(train_networkfeats, test_networkfeats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_networkfeats = pd.concat([train_networkfeats, comb_tr], 1)\n",
    "test_networkfeats = pd.concat([test_networkfeats, comb_te], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quick_xgb(train_networkfeats, test_networkfeats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_networkfeats.to_pickle('train_networkfeats.pkl')\n",
    "test_networkfeats.to_pickle('test_networkfeats.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
