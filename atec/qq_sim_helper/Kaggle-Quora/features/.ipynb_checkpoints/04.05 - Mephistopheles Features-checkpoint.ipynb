{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.display import HTML\n",
    "HTML(\"\"\"\n",
    "<style>\n",
    "\n",
    "div.cell { /* Tunes the space between cells */\n",
    "margin-top:1em;\n",
    "margin-bottom:1em;\n",
    "}\n",
    "\n",
    "div.text_cell_render h1 { /* Main titles bigger, centered */\n",
    "font-size: 2.2em;\n",
    "line-height:1.4em;\n",
    "text-align:center;\n",
    "}\n",
    "\n",
    "div.text_cell_render h2 { /*  Parts names nearer from text */\n",
    "margin-bottom: -0.4em;\n",
    "}\n",
    "\n",
    "\n",
    "div.text_cell_render { /* Customize text cells */\n",
    "font-family: 'Times New Roman';\n",
    "font-size:1.5em;\n",
    "line-height:1.4em;\n",
    "padding-left:3em;\n",
    "padding-right:3em;\n",
    "}\n",
    "</style>\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The process of developing a solution of the top 15%\n",
    "\n",
    "<center>Pavel Nesterov</center>\n",
    "<center>http://pavelnesterov.info/</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', 500)\n",
    "import seaborn as sns\n",
    "sns.set_style(\"dark\")\n",
    "plt.rcParams['figure.figsize'] = 16, 12\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "import itertools as it\n",
    "import pickle\n",
    "import glob\n",
    "import os\n",
    "import string\n",
    "\n",
    "from scipy import sparse\n",
    "\n",
    "import nltk\n",
    "import spacy\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score, GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import log_loss, make_scorer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "import eli5\n",
    "from IPython.display import display\n",
    "\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src = '/media/w/1c392724-ecf3-4615-8f3c-79368ec36380/DS Projects/Kaggle/Quora/data/'\n",
    "\n",
    "df_train = pd.read_csv(src + 'train.csv', \n",
    "                       dtype={\n",
    "                           'question1': np.str,\n",
    "                           'question2': np.str\n",
    "                       })\n",
    "df_train['test_id'] = -1\n",
    "df_test = pd.read_csv(src + 'data/test.csv', \n",
    "                      dtype={\n",
    "                          'question1': np.str,\n",
    "                          'question2': np.str\n",
    "                      })\n",
    "df_test['id'] = -1\n",
    "df_test['qid1'] = -1\n",
    "df_test['qid2'] = -1\n",
    "df_test['is_duplicate'] = -1\n",
    "\n",
    "df = pd.concat([df_train, df_test])\n",
    "df['question1'] = df['question1'].fillna('')\n",
    "df['question2'] = df['question2'].fillna('')\n",
    "df['uid'] = np.arange(df.shape[0])\n",
    "df = df.set_index(['uid'])\n",
    "print df.dtypes\n",
    "del(df_train, df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ix_train = np.where(df['id'] >= 0)[0]\n",
    "ix_test = np.where(df['id'] == -1)[0]\n",
    "ix_is_dup = np.where(df['is_duplicate'] == 1)[0]\n",
    "ix_not_dup = np.where(df['is_duplicate'] == 0)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print df[df['is_duplicate'] >= 0]['is_duplicate'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let us study the problem\n",
    "\n",
    "<img src=\"./../images/buben1.jpg\" />\n",
    "\n",
    "## Testing the test data set\n",
    "<br />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if glob.glob('./../submits/0_*.csv') < 5:\n",
    "    df_submit = df.loc[ix_test][['test_id', 'is_duplicate']].copy()\n",
    "    df_submit['is_duplicate'] = 0.0\n",
    "    df_submit.to_csv('./../submits/0_zeros.csv', index=False)\n",
    "    df_submit['is_duplicate'] = 1.0\n",
    "    df_submit.to_csv('./../submits/0_ones.csv', index=False)\n",
    "    df_submit['is_duplicate'] = 0.5\n",
    "    df_submit.to_csv('./../submits/0_half.csv', index=False)\n",
    "    df_submit['is_duplicate'] = 0.25\n",
    "    df_submit.to_csv('./../submits/0_quarter.csv', index=False)\n",
    "    df_submit['is_duplicate'] = 0.75\n",
    "    df_submit.to_csv('./../submits/0_threequarters.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score0 = 6.01888\n",
    "score1 = 28.52056\n",
    "score05 = 0.69315\n",
    "score025 = 0.47913\n",
    "score075 = 1.19485"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us recall the hypothesis risk formula:\n",
    "$$\\large Q\\left(h\\right) = \\text{E}_{x, y \\sim p\\left(x, y\\right)}\\left[L\\left(h\\left(x\\right), y\\right)\\right]$$\n",
    "\n",
    "Then empirical risk has the form:\n",
    "$$\\large Q_{\\text{emp}}\\left(h\\right) = \\frac{1}{n}\\sum_{i = 1}^n L\\left(h\\left(x_i\\right), y_i\\right)$$\n",
    "\n",
    "Although if we know that the distribution of labels is not uniform, then this can be taken into account by adding the weight of the example:\n",
    "$$\\large Q_{\\text{emp}}\\left(h\\right) = \\sum_{i = 1}^n w_i L\\left(h\\left(x_i\\right), y_i\\right)$$\n",
    "- Where $\\sum_{i=1}^n w_i = 1$\n",
    "\n",
    "\n",
    "In the Quora Question Pairs task, $L$ is the metric logloss (remember logistic regression), and the hypotheses give the probability of belonging to the class of duplicates $h\\left(x_i\\right) = p_i$:\n",
    "$$\\large\\begin{array}{rcl}\n",
    "Q_{\\text{emp}} &=& \\frac{1}{n}\\sum_{i = 1}^n L\\left(h\\left(x_i\\right), y_i\\right) \\\\\n",
    "&=& -\\frac{1}{n}\\sum_{i = 1}^n y_i \\log p_i + \\left(1 - y_i\\right)\\log\\left(1 - p_i\\right) \\\\\n",
    "&=& -\\frac{1}{n}\\sum_{i \\in I_0} \\log\\left(1 - p_i\\right) - \\frac{1}{n}\\sum_{i \\in I_1} \\log p_i\n",
    "\\end{array}$$\n",
    "\n",
    "\n",
    "Let's pretend that $\\forall p_i = 0$, the equation <font color=\"blue\">(1)</font>:\n",
    "$$\\large\\begin{array}{rcl}\n",
    "q_0 &\\approx& -\\frac{1}{n}\\sum_{i \\in I_0} \\log\\left(1 - \\epsilon\\right) - \\frac{1}{n}\\sum_{i \\in I_1} \\log \\epsilon \\\\\n",
    "&=& -\\frac{n_0}{n}\\log\\left(1 - \\epsilon\\right) - \\frac{n_1}{n} \\log \\epsilon \\\\\n",
    "&=& -r_0\\log\\left(1 - \\epsilon\\right) - r_1 \\log \\epsilon\n",
    "\\end{array}$$\n",
    "\n",
    "$\\forall p_i = 1$, the equation <font color=\"blue\">(2)</font>:\n",
    "$$\\large\\begin{array}{rcl}\n",
    "q_1 &\\approx& -r_0\\log \\epsilon - r_1 \\log \\left(1 - \\epsilon\\right)\n",
    "\\end{array}$$\n",
    "\n",
    "$\\forall p_i = \\frac{1}{2}$, the equation <font color=\"blue\">(3)</font>:\n",
    "$$\\large\\begin{array}{rcl}\n",
    "q_{1/2} &=& -r_0\\log \\frac{1}{2} - r_1 \\log \\frac{1}{2} \\\\\n",
    "&=& \\left(r_0 + r_1\\right) \\log 2\n",
    "\\end{array}$$\n",
    "\n",
    "<font color=\"blue\">(1)</font> + <font color=\"blue\">(2)</font>:\n",
    "$$\\large\\begin{array}{rcl}\n",
    "-\\left(q_0+ q_1\\right) &=& r_0 \\log \\left(1 - \\epsilon\\right) + r_1 \\log \\epsilon + r_0 \\log \\epsilon + r_1 \\log \\left(1 - \\epsilon\\right) \\\\\n",
    "&=& \\left(r_0 + r_1\\right) \\log \\epsilon \\left(1 - \\epsilon\\right)\n",
    "\\end{array}$$\n",
    "\n",
    "\n",
    "Using equation <font color=\"blue\">(3)</font>, Calculate $\\epsilon$:\n",
    "$$\\large\\begin{array}{rcl}\n",
    "-\\left(q_0+ q_1\\right) &=& \\frac{q_{1/2}}{\\log 2} \\log \\epsilon \\left(1 - \\epsilon\\right) \\Leftrightarrow \\\\\n",
    "\\log \\epsilon \\left(1 - \\epsilon\\right) &=& \\frac{q_0 + q_1}{q_{1/2}} \\log \\frac{1}{2} \\Leftrightarrow \\\\\n",
    "\\log \\epsilon \\left(1 - \\epsilon\\right) &=& A \\Leftrightarrow \\\\\n",
    "\\epsilon^2 - \\epsilon + e^A &=& 0 \\Rightarrow \\\\\n",
    "\\epsilon &=& \\frac{1}{2}\\left(1 \\pm \\sqrt{1 - 4e^A}\\right)\n",
    "\\end{array}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = (score0 + score1)*np.log(0.5)/score05\n",
    "print 'A =', A\n",
    "print 'eps_0 =', (1 + np.sqrt(1 - 4*np.exp(A)))/2\n",
    "print 'eps_1 =', (1 - np.sqrt(1 - 4*np.exp(A)))/2\n",
    "eps = 10e-16\n",
    "print 'eps =', eps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "For simplicity, we introduce two more constants:\n",
    "$$\\large\\begin{array}{rcl}\n",
    "B &=& \\log \\left(1 - \\epsilon\\right) \\\\\n",
    "C &=& \\log \\epsilon\n",
    "\\end{array}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "B = np.log(1 - eps)\n",
    "print 'B =', B\n",
    "C = np.log(eps)\n",
    "print 'C =', C"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "\n",
    "Solving the system of two equations <font color=\"blue\">(1)</font> \n",
    "and <font color=\"blue\">(2)</font>, \n",
    "We get:\n",
    "$$\\large\\begin{array}{rcl}\n",
    "r_1 &=& \\frac{B\\left(q_1 - \\frac{C}{B}q_0\\right)}{C^2 - B^2} \\\\\n",
    "r_0 &=& \\frac{1}{C}\\left(-q_1 - r_1 B\\right)\n",
    "\\end{array}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r1 = (score1 - (C/B)*score0) / ((C*C/B) - B)\n",
    "print 'r1 =', r1\n",
    "r0 = (-score1 - r1*B)/C\n",
    "print 'r0 =', r0\n",
    "print 'r0 + r1 =', r0 + r1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.isfile('./../submits/0_174.csv'):\n",
    "    df_submit = df.loc[ix_test][['test_id', 'is_duplicate']].copy()\n",
    "    df_submit['is_duplicate'] = 0.17426442474\n",
    "    df_submit.to_csv('./../submits/0_174.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- score0174 = **0.46258**\n",
    "- At the moment of submission it is **1208/1880**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.bar(range(6), [score0, score0174, score025, score05, score075, score1])\n",
    "ax.set_title('Logloss')\n",
    "ax.set_xticks(np.arange(6) + 0.4)\n",
    "ax.set_xticklabels([0, 0.174, 0.25, 0.5, 0.75, 1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correct the model forecast\n",
    "\n",
    "It turns out that the distribution of tags on the trainee and the test is different, I would like to learn on the train, but somehow optimize the error function on the test. Let's make a reasonable assumption:\n",
    " - The trainee and test differ only in the distribution of classes, but not in classes:\n",
    "  - $\\forall i: p\\left(X \\mid y = i\\right) = p\\left(X' \\mid y' = i\\right)$, Where $\\left(X, y\\right) \\sim p_{\\text{train}}$, $\\left(X', y'\\right) \\sim p_{\\text{test}}$\n",
    "\n",
    "Denote by $p$ The best model on the training set of data, then:\n",
    "\n",
    "$$\\large\\begin{array}{rcl}\n",
    "p = P\\left(y = 1 \\mid x\\right) &=& \\frac{P\\left(x \\mid y = 1\\right)P\\left(y = 1\\right)}{P\\left(x \\mid y = 1\\right)P\\left(y = 1\\right) + P\\left(x \\mid y = 0\\right)P\\left(y = 0\\right)} \\\\\n",
    "&=& \\frac{a p_1}{a p_1 + b p_0} \\Rightarrow \\\\\n",
    "b p_0 &=& \\frac{a p_1}{p} - a p_1 = \\frac{a p_1 - a p_1 p}{p}\n",
    "\\end{array}$$\n",
    "\n",
    "\n",
    "By assumption, the distribution of labels on the test is simply oblique, i.e $P\\left(y' = i\\right) = \\gamma_i P\\left(y = 1\\right)$, then:\n",
    "\n",
    "$$\\large\\begin{array}{rcl}\n",
    "P\\left(y' = 1 \\mid x\\right) &=& \\frac{P\\left(x \\mid y' = 1\\right)P\\left(y' = 1\\right)}{P\\left(x \\mid y' = 1\\right)P\\left(y' = 1\\right) + P\\left(x \\mid y' = 0\\right)P\\left(y' = 0\\right)} \\\\\n",
    "&=& \\frac{a\\gamma_1p_1}{a\\gamma_1p_1 + b\\gamma_0p_0} \\\\\n",
    "&=& \\frac{a\\gamma_1p_1}{a\\gamma_1p_1 + \\gamma_0\\frac{a p_1 - a p_1 p}{p}} \\\\\n",
    "&=& \\frac{a\\gamma_1p_1p}{a\\gamma_1p_1p + \\gamma_0a p_1 - \\gamma_0a p_1 p} \\\\\n",
    "&=& \\frac{\\gamma_1 p}{\\gamma_1 p + \\gamma_0 - \\gamma_0 p} \\\\\n",
    "&=& \\frac{\\gamma_1 p}{\\gamma_1 p + \\gamma_0 \\left(1 - p\\right)} \\\\\n",
    "\\end{array}$$\n",
    "\n",
    "\n",
    "It turns out that the forecast of the model, trained on the training set, must be converted before sending by using the function:\n",
    "$$\\large f\\left(x\\right) = \\frac{\\gamma_1 x}{\\gamma_1 x + \\gamma_0 \\left(1 - x\\right)}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = df[df['is_duplicate'] >= 0]['is_duplicate'].value_counts(normalize=True).to_dict()\n",
    "print 'P(y = 0) =', d[0]\n",
    "print 'P(y = 1) =', d[1]\n",
    "print 'P(y\\' = 0) =', r0\n",
    "print 'P(y\\' = 1) =', r1\n",
    "gamma_0 = r0/d[0]\n",
    "gamma_1 = r1/d[1]\n",
    "print 'gamma_0 =', gamma_0\n",
    "print 'gamma_1 =', gamma_1\n",
    "\n",
    "def link_function(x):\n",
    "    return gamma_1*x/(gamma_1*x + gamma_0*(1 - x))\n",
    "\n",
    "support = np.linspace(0, 1, 1000)\n",
    "values = link_function(support)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(support, values)\n",
    "ax.set_title('Link transformation', fontsize=20)\n",
    "ax.set_xlabel('x')\n",
    "ax.set_ylabel('f(x)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature engineering\n",
    "\n",
    "<img src=\"./../images/buben2.jpg\" width=\"50%\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Length of question\n",
    "<br />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df['len1'] = df['question1'].str.len().astype(np.float32)\n",
    "df['len2'] = df['question2'].str.len().astype(np.float32)\n",
    "df['abs_diff_len1_len2'] = np.abs(df['len1'] - df['len2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_real_feature(fname):\n",
    "    fig = plt.figure()\n",
    "    ax1 = plt.subplot2grid((3, 2), (0, 0), colspan=2)\n",
    "    ax2 = plt.subplot2grid((3, 2), (1, 0), colspan=2)\n",
    "    ax3 = plt.subplot2grid((3, 2), (2, 0))\n",
    "    ax4 = plt.subplot2grid((3, 2), (2, 1))\n",
    "    ax1.set_title('Distribution of %s' % fname, fontsize=20)\n",
    "    sns.distplot(df.loc[ix_train][fname], \n",
    "                 bins=50, \n",
    "                 ax=ax1)    \n",
    "    sns.distplot(df.loc[ix_is_dup][fname], \n",
    "                 bins=50, \n",
    "                 ax=ax2,\n",
    "                 label='is dup')    \n",
    "    sns.distplot(df.loc[ix_not_dup][fname], \n",
    "                 bins=50, \n",
    "                 ax=ax2,\n",
    "                 label='not dup')\n",
    "    ax2.legend(loc='upper right', prop={'size': 18})\n",
    "    sns.boxplot(y=fname, \n",
    "                x='is_duplicate', \n",
    "                data=df.loc[ix_train], \n",
    "                ax=ax3)\n",
    "    sns.violinplot(y=fname, \n",
    "                   x='is_duplicate', \n",
    "                   data=df.loc[ix_train], \n",
    "                   ax=ax4)\n",
    "    plt.show()\n",
    "    \n",
    "plot_real_feature('abs_diff_len1_len2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print df.loc[ix_train]['abs_diff_len1_len2'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_in_dup = df.loc[ix_is_dup]['abs_diff_len1_len2'].max()\n",
    "print 'Maximum among duplicates:       ', max_in_dup\n",
    "max_in_not_dups = df.loc[ix_not_dup]['abs_diff_len1_len2'].max()\n",
    "print 'Maximum among non-duplicates:     ', max_in_not_dups\n",
    "print 'Maximum among non-duplicates: ', (df.loc[ix_train]['abs_diff_len1_len2'] > max_in_dup).sum()\n",
    "std_in_dups = df.loc[ix_is_dup]['abs_diff_len1_len2'].std()\n",
    "print 'Standard deviation in duplicates:', std_in_dups\n",
    "replace_value = max_in_dup + 2*std_in_dups\n",
    "print 'New value:              ', replace_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['abs_diff_len1_len2'] = df['abs_diff_len1_len2'].apply(lambda x: x if x < replace_value else replace_value)\n",
    "plot_real_feature('abs_diff_len1_len2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['log_abs_diff_len1_len2'] = np.log(df['abs_diff_len1_len2'] + 1)\n",
    "plot_real_feature('log_abs_diff_len1_len2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['ratio_len1_len2'] = df['len1'].apply(lambda x: x if x > 0.0 else 1.0)/\\\n",
    "                        df['len2'].apply(lambda x: x if x > 0.0 else 1.0)\n",
    "plot_real_feature('ratio_len1_len2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print df.loc[ix_train]['ratio_len1_len2'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_in_dup = df.loc[ix_is_dup]['ratio_len1_len2'].max()\n",
    "print 'Maximum among duplicates:        ', max_in_dup\n",
    "max_in_not_dups = df.loc[ix_not_dup]['ratio_len1_len2'].max()\n",
    "print 'Maximum among non-duplicates:      ', max_in_not_dups\n",
    "print 'Number of lines greater than threshold: ', (df.loc[ix_train]['ratio_len1_len2'] > max_in_dup).sum()\n",
    "std_in_dups = df.loc[ix_is_dup]['ratio_len1_len2'].std()\n",
    "print 'Number of lines greater than threshold: ', std_in_dups\n",
    "replace_value = max_in_dup + 2*std_in_dups\n",
    "print 'New value:               ', replace_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['ratio_len1_len2'] = df['ratio_len1_len2'].apply(lambda x: x if x < replace_value else replace_value)\n",
    "plot_real_feature('ratio_len1_len2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['log_ratio_len1_len2'] = np.log(df['ratio_len1_len2'] + 1)\n",
    "plot_real_feature('log_ratio_len1_len2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sns.pairplot(df.loc[ix_train][df.columns[7:].tolist() + ['is_duplicate']], hue=\"is_duplicate\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.jointplot('log_ratio_len1_len2', 'abs_diff_len1_len2', df.loc[ix_train], alpha=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_corr(predictors):\n",
    "    predictors = predictors[:]\n",
    "    predictors += ['is_duplicate']\n",
    "    mcorr = df.loc[ix_train][predictors].corr()\n",
    "    mask = np.zeros_like(mcorr, dtype=np.bool)\n",
    "    mask[np.triu_indices_from(mask)] = True\n",
    "    cmap = sns.diverging_palette(220, 10, as_cmap=True)\n",
    "    g = sns.heatmap(mcorr, mask=mask, cmap=cmap, square=True, annot=True, fmt='0.2f')\n",
    "    g.set_xticklabels(predictors, rotation=90)\n",
    "    g.set_yticklabels(reversed(predictors))\n",
    "    plt.show()\n",
    "    \n",
    "plot_corr(df.columns[7:].tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Check for new symptoms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictors = df.columns[7:].tolist()\n",
    "print predictors\n",
    "\n",
    "def check_model(predictors):\n",
    "    classifier = lambda: SGDClassifier(\n",
    "        loss='log', \n",
    "        penalty='elasticnet', \n",
    "        fit_intercept=True, \n",
    "        n_iter=100, \n",
    "        shuffle=True, \n",
    "        n_jobs=-1,\n",
    "        class_weight=None)\n",
    "\n",
    "    model = Pipeline(steps=[\n",
    "        ('ss', StandardScaler()),\n",
    "        ('en', classifier())\n",
    "    ])\n",
    "\n",
    "    parameters = {\n",
    "        'en__alpha': [0.00001, 0.0001, 0.001, 0.01, 0.02, 0.1, 0.5, 0.9, 1],\n",
    "        'en__l1_ratio': [0, 0.0001, 0.001, 0.01, 0.1, 0.3, 0.5, 0.75, 0.9, 1]\n",
    "    }\n",
    "\n",
    "    folder = StratifiedKFold(n_splits=5, shuffle=True)\n",
    "    \n",
    "    grid_search = GridSearchCV(\n",
    "        model, \n",
    "        parameters, \n",
    "        cv=folder, \n",
    "        n_jobs=-1, \n",
    "        verbose=1)\n",
    "    grid_search = grid_search.fit(df.loc[ix_train][predictors], \n",
    "                                  df.loc[ix_train]['is_duplicate'])\n",
    "    \n",
    "    return grid_search\n",
    "\n",
    "if not os.path.isfile('./../tmp/1_model.pkl'):\n",
    "    model = check_model(predictors)\n",
    "    print model.best_score_\n",
    "    print model.best_params_\n",
    "    with open('./../tmp/1_model.pkl', 'wb') as f:\n",
    "        pickle.dump(model, f)\n",
    "else:\n",
    "    with open('./../tmp/1_model.pkl', 'rb') as f:\n",
    "        model = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "Fitting 5 folds for each of 90 candidates, totalling 450 fits\n",
    "[Parallel(n_jobs=-1)]: Done  18 tasks      | elapsed:   21.6s\n",
    "[Parallel(n_jobs=-1)]: Done 168 tasks      | elapsed:  2.1min\n",
    "[Parallel(n_jobs=-1)]: Done 418 tasks      | elapsed:  5.2min\n",
    "[Parallel(n_jobs=-1)]: Done 450 out of 450 | elapsed:  5.5min finished\n",
    "0.633560068268\n",
    "{'en__l1_ratio': 0, 'en__alpha': 0.02}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./../images/htop.png\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred = model.predict_proba(df.loc[ix_test][predictors])[:, 1]\n",
    "y_test_pred_fixed = link_function(y_test_pred)\n",
    "\n",
    "fig = plt.figure()\n",
    "ax1 = plt.subplot2grid((1, 2), (0, 0))\n",
    "ax2 = plt.subplot2grid((1, 2), (0, 1))\n",
    "sns.distplot(y_test_pred, ax=ax1)\n",
    "ax1.set_title('Model prediction')\n",
    "sns.distplot(y_test_pred_fixed, ax=ax2)\n",
    "ax2.set_title('Model prediction fixed')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.isfile('./../submits/1_pred.csv'):\n",
    "    pd.DataFrame.from_records(\n",
    "        zip(df.loc[ix_test]['test_id'].values, \n",
    "            y_test_pred), \n",
    "        columns=['test_id', 'is_duplicate']).to_csv('./../submits/1_pred.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.isfile('./../submits/1_pred_fixed.csv'):\n",
    "    pd.DataFrame.from_records(\n",
    "        zip(df.loc[ix_test]['test_id'].values, \n",
    "            y_test_pred_fixed), \n",
    "        columns=['test_id', 'is_duplicate']).to_csv('./../submits/1_pred_fixed.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "- Without amendment: **0.52929**\n",
    "- Amended: **0.44127**\n",
    "- Previous value: **0.46258**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(\n",
    "    eli5.sklearn.explain_linear_classifier_weights(\n",
    "        model.best_estimator_.steps[1][1],\n",
    "        feature_names=predictors))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Letter n-gram\n",
    "<br />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "if os.path.isfile('./../tmp/cv_char.pkl') and os.path.isfile('./../tmp/ch_freq.pkl'):\n",
    "    with open('./../tmp/cv_char.pkl', 'rb') as f:\n",
    "        cv_char = pickle.load(f)\n",
    "    with open('./../tmp/ch_freq.pkl', 'rb') as f:\n",
    "        ch_freq = pickle.load(f)\n",
    "else:\n",
    "    cv_char = CountVectorizer(ngram_range=(1, 3), analyzer='char')\n",
    "    ch_freq = np.array(cv_char.fit_transform(df['question1'].tolist() + df['question2'].tolist()).sum(axis=0))[0, :]\n",
    "    with open('./../tmp/cv_char.pkl', 'wb') as f:\n",
    "        pickle.dump(cv_char, f)\n",
    "    with open('./../tmp/ch_freq.pkl', 'wb') as f:\n",
    "        pickle.dump(ch_freq, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unigrams = dict([(k, v) for (k, v) in cv_char.vocabulary_.items() if len(k) == 1])\n",
    "ix_unigrams = np.sort(unigrams.values())\n",
    "print 'Unigrams:', len(unigrams)\n",
    "bigrams = dict([(k, v) for (k, v) in cv_char.vocabulary_.items() if len(k) == 2])\n",
    "ix_bigrams = np.sort(bigrams.values())\n",
    "print 'Bigrams: ', len(bigrams)\n",
    "trigrams = dict([(k, v) for (k, v) in cv_char.vocabulary_.items() if len(k) == 3])\n",
    "ix_trigrams = np.sort(trigrams.values())\n",
    "print 'Trigrams:', len(trigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "def save_sparse_csr(fname, sm):\n",
    "    np.savez(fname, \n",
    "             data=sm.data, \n",
    "             indices=sm.indices,\n",
    "             indptr=sm.indptr, \n",
    "             shape=sm.shape)\n",
    "\n",
    "def load_sparse_csr(fname):\n",
    "    loader = np.load(fname)\n",
    "    return sparse.csr_matrix((\n",
    "        loader['data'], \n",
    "        loader['indices'], \n",
    "        loader['indptr']),\n",
    "        shape=loader['shape'])\n",
    "\n",
    "if os.path.isfile('./../tmp/m_q1.npz') and os.path.isfile('./../tmp/m_q2.npz'):\n",
    "    m_q1 = load_sparse_csr('./../tmp/m_q1.npz')\n",
    "    m_q2 = load_sparse_csr('./../tmp/m_q2.npz')\n",
    "else:\n",
    "    m_q1 = cv_char.transform(df['question1'].values)\n",
    "    m_q2 = cv_char.transform(df['question2'].values)\n",
    "    save_sparse_csr('./../tmp/m_q1.npz', m_q1)\n",
    "    save_sparse_csr('./../tmp/m_q2.npz', m_q2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Jaccard similarity and derivatives\n",
    "$$\\large\\begin{array}{rcl}\n",
    "J\\left(A, B\\right) &=& \\dfrac{\\left|A \\cap B\\right|}{\\left|A \\cup B\\right|}\n",
    "\\end{array}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Unigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v_num = (m_q1[:, ix_unigrams] > 0).minimum((m_q2[:, ix_unigrams] > 0)).sum(axis=1)\n",
    "v_den = (m_q1[:, ix_unigrams] > 0).maximum((m_q2[:, ix_unigrams] > 0)).sum(axis=1)\n",
    "v_score = np.array(v_num.flatten()).astype(np.float32)[0, :]/np.array(v_den.flatten())[0, :]\n",
    "\n",
    "df['unigram_jaccard'] = v_score\n",
    "plot_real_feature('unigram_jaccard')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We take into account each letter more than once\n",
    "v_num = m_q1[:, ix_unigrams].minimum(m_q2[:, ix_unigrams]).sum(axis=1)\n",
    "v_den = m_q1[:, ix_unigrams].sum(axis=1) + m_q2[:, ix_unigrams].sum(axis=1)\n",
    "v_score = np.array(v_num.flatten()).astype(np.float32)[0, :]/np.array(v_den.flatten())[0, :]\n",
    "\n",
    "df['unigram_all_jaccard'] = v_score\n",
    "plot_real_feature('unigram_all_jaccard')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We take into account each letter more than once\n",
    "# Normalize the maximum value, and not the sum\n",
    "v_num = m_q1[:, ix_unigrams].minimum(m_q2[:, ix_unigrams]).sum(axis=1)\n",
    "v_den = m_q1[:, ix_unigrams].maximum(m_q2[:, ix_unigrams]).sum(axis=1)\n",
    "v_score = np.array(v_num.flatten()).astype(np.float32)[0, :]/np.array(v_den.flatten())[0, :]\n",
    "\n",
    "df['unigram_all_jaccard_max'] = v_score\n",
    "plot_real_feature('unigram_all_jaccard_max')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v_num = (m_q1[:, ix_bigrams] > 0).minimum((m_q2[:, ix_bigrams] > 0)).sum(axis=1)\n",
    "v_den = (m_q1[:, ix_bigrams] > 0).maximum((m_q2[:, ix_bigrams] > 0)).sum(axis=1)\n",
    "v_score = np.array(v_num.flatten()).astype(np.float32)[0, :]/np.array(v_den.flatten())[0, :]\n",
    "\n",
    "df['bigram_jaccard'] = v_score\n",
    "plot_real_feature('bigram_jaccard')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[ix_train].groupby(['is_duplicate'])['bigram_jaccard'].agg(\n",
    "    {\n",
    "        'min': np.min,\n",
    "        'max': np.max,\n",
    "        'p1': lambda x: np.percentile(x, q=0.01),\n",
    "        'p99': lambda x: np.percentile(x, q=99.99)\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print 'Number of right speakers on the right:', (df['bigram_jaccard'] > 1).sum()\n",
    "print 'Number of outsiders on the left: ', (df['bigram_jaccard'] < -1.47).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[df['bigram_jaccard'] < -1.478751, 'bigram_jaccard'] = -1.478751\n",
    "df.loc[df['bigram_jaccard'] > 1.0, 'bigram_jaccard'] = 1.0\n",
    "plot_real_feature('bigram_jaccard')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We take into account each letter more than once\n",
    "v_num = m_q1[:, ix_bigrams].minimum(m_q2[:, ix_bigrams]).sum(axis=1)\n",
    "v_den = m_q1[:, ix_bigrams].sum(axis=1) + m_q2[:, ix_bigrams].sum(axis=1)\n",
    "v_score = np.array(v_num.flatten()).astype(np.float32)[0, :]/np.array(v_den.flatten())[0, :]\n",
    "\n",
    "df['bigram_all_jaccard'] = v_score\n",
    "plot_real_feature('bigram_all_jaccard')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We take into account each letter more than once\n",
    "# Normalize the maximum value, and not the sum\n",
    "v_num = m_q1[:, ix_bigrams].minimum(m_q2[:, ix_bigrams]).sum(axis=1)\n",
    "v_den = m_q1[:, ix_bigrams].maximum(m_q2[:, ix_bigrams]).sum(axis=1)\n",
    "v_score = np.array(v_num.flatten()).astype(np.float32)[0, :]/np.array(v_den.flatten())[0, :]\n",
    "\n",
    "df['bigram_all_jaccard_max'] = v_score\n",
    "plot_real_feature('bigram_all_jaccard_max')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Тrigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "m_q1 = m_q1[:, ix_trigrams]\n",
    "m_q2 = m_q2[:, ix_trigrams]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v_num = (m_q1 > 0).minimum((m_q2 > 0)).sum(axis=1)\n",
    "v_den = (m_q1 > 0).maximum((m_q2 > 0)).sum(axis=1)\n",
    "v_den[np.where(v_den == 0)] = 1\n",
    "v_score = np.array(v_num.flatten()).astype(np.float32)[0, :]/np.array(v_den.flatten())[0, :]\n",
    "\n",
    "df['trigram_jaccard'] = v_score\n",
    "plot_real_feature('trigram_jaccard')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# учитываем каждую букву больше одного раза\n",
    "v_num = m_q1.minimum(m_q2).sum(axis=1)\n",
    "v_den = m_q1.sum(axis=1) + m_q2.sum(axis=1)\n",
    "v_den[np.where(v_den == 0)] = 1\n",
    "v_score = np.array(v_num.flatten()).astype(np.float32)[0, :]/np.array(v_den.flatten())[0, :]\n",
    "\n",
    "df['trigram_all_jaccard'] = v_score\n",
    "plot_real_feature('trigram_all_jaccard')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We take into account each letter more than once\n",
    "# Normalize the maximum value, and not the sum\n",
    "v_num = m_q1.minimum(m_q2).sum(axis=1)\n",
    "v_den = m_q1.maximum(m_q2).sum(axis=1)\n",
    "v_den[np.where(v_den == 0)] = 1\n",
    "v_score = np.array(v_num.flatten()).astype(np.float32)[0, :]/np.array(v_den.flatten())[0, :]\n",
    "\n",
    "df['trigram_all_jaccard_max'] = v_score\n",
    "plot_real_feature('trigram_all_jaccard_max')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tf-idf and pair metrics on trigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tft = TfidfTransformer(\n",
    "    norm='l2', \n",
    "    use_idf=True, \n",
    "    smooth_idf=True, \n",
    "    sublinear_tf=False)\n",
    "\n",
    "tft = tft.fit(sparse.vstack((m_q1, m_q2)))\n",
    "m_q1_tf = tft.transform(m_q1)\n",
    "m_q2_tf = tft.transform(m_q2)\n",
    "\n",
    "v_num = np.array(m_q1_tf.multiply(m_q2_tf).sum(axis=1))[:, 0]\n",
    "v_den = np.array(np.sqrt(m_q1_tf.multiply(m_q1_tf).sum(axis=1)))[:, 0] * \\\n",
    "        np.array(np.sqrt(m_q2_tf.multiply(m_q2_tf).sum(axis=1)))[:, 0]\n",
    "v_num[np.where(v_den == 0)] = 1\n",
    "v_den[np.where(v_den == 0)] = 1\n",
    "\n",
    "v_score = 1 - v_num/v_den\n",
    "\n",
    "df['trigram_tfidf_cosine'] = v_score\n",
    "plot_real_feature('trigram_tfidf_cosine')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tft = TfidfTransformer(\n",
    "    norm='l2', \n",
    "    use_idf=True, \n",
    "    smooth_idf=True, \n",
    "    sublinear_tf=False)\n",
    "\n",
    "tft = tft.fit(sparse.vstack((m_q1, m_q2)))\n",
    "m_q1_tf = tft.transform(m_q1)\n",
    "m_q2_tf = tft.transform(m_q2)\n",
    "\n",
    "v_score = (m_q1_tf - m_q2_tf)\n",
    "v_score = np.sqrt(np.array(v_score.multiply(v_score).sum(axis=1))[:, 0])\n",
    "\n",
    "df['trigram_tfidf_l2_euclidean'] = v_score\n",
    "plot_real_feature('trigram_tfidf_l2_euclidean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tft = TfidfTransformer(\n",
    "    norm='l1', \n",
    "    use_idf=True, \n",
    "    smooth_idf=True, \n",
    "    sublinear_tf=False)\n",
    "\n",
    "tft = tft.fit(sparse.vstack((m_q1, m_q2)))\n",
    "m_q1_tf = tft.transform(m_q1)\n",
    "m_q2_tf = tft.transform(m_q2)\n",
    "\n",
    "v_score = (m_q1_tf - m_q2_tf)\n",
    "v_score = np.sqrt(np.array(v_score.multiply(v_score).sum(axis=1))[:, 0])\n",
    "\n",
    "df['trigram_tfidf_l1_euclidean'] = v_score\n",
    "plot_real_feature('trigram_tfidf_l1_euclidean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tft = TfidfTransformer(\n",
    "    norm='l2', \n",
    "    use_idf=False, \n",
    "    smooth_idf=True, \n",
    "    sublinear_tf=False)\n",
    "\n",
    "tft = tft.fit(sparse.vstack((m_q1, m_q2)))\n",
    "m_q1_tf = tft.transform(m_q1)\n",
    "m_q2_tf = tft.transform(m_q2)\n",
    "\n",
    "v_score = (m_q1_tf - m_q2_tf)\n",
    "v_score = np.sqrt(np.array(v_score.multiply(v_score).sum(axis=1))[:, 0])\n",
    "\n",
    "df['trigram_tf_l2_euclidean'] = v_score\n",
    "plot_real_feature('trigram_tf_l2_euclidean')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check with new features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictors = df.columns[7:].tolist()\n",
    "print predictors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_corr(predictors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(df.loc[ix_train][[\n",
    "    'abs_diff_len1_len2', \n",
    "    'trigram_jaccard',\n",
    "    'trigram_tfidf_cosine', \n",
    "    'trigram_tfidf_l2_euclidean', \n",
    "    'trigram_tfidf_l1_euclidean',\n",
    "    'is_duplicate']], hue=\"is_duplicate\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.isfile('./../tmp/2_model.pkl'):\n",
    "    model = check_model(predictors)\n",
    "    print model.best_score_\n",
    "    print model.best_params_\n",
    "    with open('./../tmp/2_model.pkl', 'wb') as f:\n",
    "        pickle.dump(model, f)\n",
    "else:\n",
    "    with open('./../tmp/2_model.pkl', 'rb') as f:\n",
    "        model = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "Fitting 5 folds for each of 90 candidates, totalling 450 fits\n",
    "[Parallel(n_jobs=-1)]: Done  18 tasks      | elapsed:   34.0s\n",
    "[Parallel(n_jobs=-1)]: Done 168 tasks      | elapsed:  3.2min\n",
    "[Parallel(n_jobs=-1)]: Done 418 tasks      | elapsed:  7.9min\n",
    "[Parallel(n_jobs=-1)]: Done 450 out of 450 | elapsed:  8.4min finished\n",
    "0.716752825942\n",
    "{'en__l1_ratio': 1, 'en__alpha': 1e-05}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred = model.predict_proba(df.loc[ix_test][predictors])[:, 1]\n",
    "y_test_pred_fixed = link_function(y_test_pred)\n",
    "\n",
    "if not os.path.isfile('./../submits/2_pred_fixed.csv'):\n",
    "    pd.DataFrame.from_records(\n",
    "        zip(df.loc[ix_test]['test_id'].values, \n",
    "            y_test_pred_fixed), \n",
    "        columns=['test_id', 'is_duplicate']).to_csv('./../submits/2_pred_fixed.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax1 = plt.subplot2grid((1, 2), (0, 0))\n",
    "ax2 = plt.subplot2grid((1, 2), (0, 1))\n",
    "sns.distplot(y_test_pred, ax=ax1)\n",
    "ax1.set_title('Model prediction')\n",
    "sns.distplot(y_test_pred_fixed, ax=ax2)\n",
    "ax2.set_title('Model prediction fixed')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(\n",
    "    eli5.sklearn.explain_linear_classifier_weights(\n",
    "        model.best_estimator_.steps[1][1],\n",
    "        feature_names=predictors))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "- Old value: **0.44127**\n",
    "- New result: **0.39816**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Out-of-fold prediction on trigram\n",
    "\n",
    "<img src=\"./../images/buben3.jpg\" />\n",
    "\n",
    "#### We use tf-idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def check_model(predictors, data=None, do_scaling=True):\n",
    "    classifier = lambda: SGDClassifier(\n",
    "        loss='log', \n",
    "        penalty='elasticnet', \n",
    "        fit_intercept=True, \n",
    "        n_iter=100, \n",
    "        shuffle=True, \n",
    "        n_jobs=-1,\n",
    "        class_weight=None)\n",
    "\n",
    "    steps = []\n",
    "    if do_scaling:\n",
    "        steps.append(('ss', StandardScaler()))\n",
    "    steps.append(('en', classifier()))\n",
    "    \n",
    "    model = Pipeline(steps=steps)\n",
    "\n",
    "    parameters = {\n",
    "        'en__alpha': [0.00001, 0.0001, 0.001, 0.01, 0.02, 0.1, 0.5, 0.9, 1],\n",
    "        'en__l1_ratio': [0, 0.0001, 0.001, 0.01, 0.1, 0.3, 0.5, 0.75, 0.9, 1]\n",
    "    }\n",
    "\n",
    "    folder = StratifiedKFold(n_splits=5, shuffle=True)\n",
    "    \n",
    "    grid_search = GridSearchCV(\n",
    "        model, \n",
    "        parameters, \n",
    "        cv=folder, \n",
    "        n_jobs=-1, \n",
    "        verbose=1)\n",
    "    if data is None:\n",
    "        grid_search = grid_search.fit(df.loc[ix_train][predictors], \n",
    "                                      df.loc[ix_train]['is_duplicate'])\n",
    "    else:\n",
    "        grid_search = grid_search.fit(data['X'], \n",
    "                                      data['y'])\n",
    "    \n",
    "    return grid_search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.isfile('./../tmp/3_model.pkl'):\n",
    "    model = check_model(None, data={\n",
    "        'X': sparse.csc_matrix(sparse.hstack((m_q1_tf, m_q2_tf)))[ix_train, :],\n",
    "        'y': df.loc[ix_train]['is_duplicate']\n",
    "    }, do_scaling=False)\n",
    "    print model.best_score_\n",
    "    print model.best_params_\n",
    "    with open('./../tmp/3_model.pkl', 'wb') as f:\n",
    "        pickle.dump(model, f)\n",
    "else:\n",
    "    with open('./../tmp/3_model.pkl', 'rb') as f:\n",
    "        model = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "Fitting 5 folds for each of 90 candidates, totalling 450 fits\n",
    "[Parallel(n_jobs=-1)]: Done  18 tasks      | elapsed:  3.3min\n",
    "[Parallel(n_jobs=-1)]: Done 168 tasks      | elapsed: 18.8min\n",
    "[Parallel(n_jobs=-1)]: Done 418 tasks      | elapsed: 43.8min\n",
    "[Parallel(n_jobs=-1)]: Done 450 out of 450 | elapsed: 46.3min finished\n",
    "0.741522174677\n",
    "{'en__l1_ratio': 0.0001, 'en__alpha': 1e-05}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred = model.predict_proba(\n",
    "    sparse.csc_matrix(sparse.hstack((m_q1_tf, m_q2_tf)))[ix_test, :])[:, 1]\n",
    "y_test_pred_fixed = link_function(y_test_pred)\n",
    "\n",
    "if not os.path.isfile('./../submits/3_pred_fixed.csv'):\n",
    "    pd.DataFrame.from_records(\n",
    "        zip(df.loc[ix_test]['test_id'].values, \n",
    "            y_test_pred_fixed), \n",
    "        columns=['test_id', 'is_duplicate']).to_csv('./../submits/3_pred_fixed.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax1 = plt.subplot2grid((1, 2), (0, 0))\n",
    "ax2 = plt.subplot2grid((1, 2), (0, 1))\n",
    "sns.distplot(y_test_pred, ax=ax1)\n",
    "ax1.set_title('Model prediction')\n",
    "sns.distplot(y_test_pred_fixed, ax=ax2)\n",
    "ax2.set_title('Model prediction fixed')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Old valueе: **0.39816**\n",
    "- New result: **0.36629**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "tmp = set(ix_trigrams.tolist())\n",
    "tmp = [k for (k, v) in sorted(cv_char.vocabulary_.items(), key=lambda t: t[1]) if v in tmp]\n",
    "tmp = tmp + tmp\n",
    "display(\n",
    "    eli5.sklearn.explain_linear_classifier_weights(\n",
    "        model.best_estimator_.steps[0][1],\n",
    "        feature_names=tmp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[:1000][df['question1'][:1000].apply(lambda s: 'o v' in s)]['question1'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[:1000][df['question1'][:1000].apply(lambda s: 'o v' in s)]['question2'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[:1000][df['question1'][:1000].apply(lambda s: '.co' in s)]['question1'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[:1000][df['question1'][:1000].apply(lambda s: '.co' in s)]['question2'].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<img src=\"./../images/oof.png\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\n",
    "    'X_train': sparse.csc_matrix(sparse.hstack((m_q1_tf, m_q2_tf)))[ix_train, :],\n",
    "    'y_train': df.loc[ix_train]['is_duplicate'],\n",
    "    'X_test': sparse.csc_matrix(sparse.hstack((m_q1_tf, m_q2_tf)))[ix_test, :],\n",
    "    'y_train_pred': np.zeros(ix_train.shape[0]),\n",
    "    'y_test_pred': []\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.isfile('./../tmp/4_model_pred.pkl'):\n",
    "    n_splits = 10\n",
    "    folder = StratifiedKFold(n_splits=n_splits, shuffle=True)\n",
    "    for ix_first, ix_second in tqdm_notebook(folder.split(np.zeros(data['y_train'].shape[0]), data['y_train']), \n",
    "                                             total=n_splits):\n",
    "        # {'en__l1_ratio': 0.0001, 'en__alpha': 1e-05}\n",
    "        model = SGDClassifier(\n",
    "            loss='log', \n",
    "            penalty='elasticnet', \n",
    "            fit_intercept=True, \n",
    "            n_iter=100, \n",
    "            shuffle=True, \n",
    "            n_jobs=-1,\n",
    "            l1_ratio=0.0001,\n",
    "            alpha=1e-05,\n",
    "            class_weight=None)\n",
    "        model = model.fit(data['X_train'][ix_first, :], data['y_train'][ix_first])\n",
    "        data['y_train_pred'][ix_second] = model.predict_proba(data['X_train'][ix_second, :])[:, 1]\n",
    "        data['y_test_pred'].append(model.predict_proba(data['X_test'])[:, 1])\n",
    "        \n",
    "    data['y_test_pred'] = np.array(data['y_test_pred']).T.mean(axis=1)\n",
    "    with open('./../tmp/4_model_pred.pkl', 'wb') as f:\n",
    "        pickle.dump({\n",
    "            'model': model,\n",
    "            'y_train_pred': data['y_train_pred'],\n",
    "            'y_test_pred': data['y_test_pred']\n",
    "        }, f)\n",
    "else:\n",
    "    with open('./../tmp/4_model_pred.pkl', 'rb') as f:\n",
    "        tmp = pickle.load(f)\n",
    "        model = tmp['model']\n",
    "        data['y_train_pred'] = tmp['y_train_pred']\n",
    "        data['y_test_pred'] = tmp['y_test_pred']\n",
    "        del(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax1 = plt.subplot2grid((1, 2), (0, 0))\n",
    "ax2 = plt.subplot2grid((1, 2), (0, 1))\n",
    "\n",
    "sns.distplot(data['y_train_pred'], label='train', ax=ax1)\n",
    "sns.distplot(data['y_test_pred'], label='test', ax=ax1)\n",
    "ax1.axvline(np.mean(data['y_train_pred']), color='b', alpha=1, linestyle='--', label='mean(train)')\n",
    "ax1.axvline(np.mean(data['y_test_pred']), color='g', alpha=1, linestyle='--', label='mean(test)')\n",
    "ax1.legend(loc='upper right', prop={'size': 18})\n",
    "ax1.set_title('Train/test OOF predictions')\n",
    "\n",
    "sns.distplot(data['y_train_pred'][:ix_train.shape[0]/2], label='train first part', ax=ax2)\n",
    "sns.distplot(data['y_train_pred'][ix_train.shape[0]/2:], label='train second part', ax=ax2)\n",
    "ax2.legend(loc='upper right', prop={'size': 18})\n",
    "ax2.set_title('Train/train OOF predictions')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is necessary to fix the distribution, at least the mean value, using the already familiar transformation:\n",
    "\n",
    "$$\\large g\\left(x\\right) = \\frac{w_1 x}{w_1 x + w_0 \\left(1 - x\\right)} = \\frac{w x}{w x + (1 - w) \\left(1 - x\\right)}$$\n",
    "\n",
    "- $w \\in \\left[0, 1\\right]$\n",
    "\n",
    "Condition:\n",
    "\n",
    "$$\\large\\begin{array}{rcl}\n",
    "\\mathbb{E}\\left[p\\right] = \\mathbb{E}\\left[g\\left(q\\right)\\right]\n",
    "\\end{array}$$\n",
    "- $p$ - Distribution on the training set\n",
    "- $q$ - Distribution on the test set\n",
    "\n",
    "Equation:\n",
    "\n",
    "$$\\large\\begin{array}{rcl}\n",
    "\\frac{1}{n} \\sum_{i=1}^n p_i &=& \\frac{1}{m} \\sum_{i=1}^m g\\left(q_i\\right) \\Rightarrow \\\\\n",
    "\\mu_p &=& \\frac{1}{m} \\sum_{i=1}^n \\frac{wq_i}{wq_i + \\left(1 - w\\right)\\left(1 - q_i\\right)} \n",
    "\\end{array}$$\n",
    "\n",
    "The following optimization problem is obtained:\n",
    "$$\\large \\hat{w} = \\arg\\min_w \\left(\\mu_p - \\frac{1}{m} \\sum_{i=1}^n \\frac{wq_i}{wq_i + \\left(1 - w\\right)\\left(1 - q_i\\right)}\\right)^2 $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mp = np.mean(data['y_train_pred'])\n",
    "print mp\n",
    "\n",
    "def func(w):\n",
    "    return (mp*data['y_test_pred'].shape[0] - \n",
    "            np.sum(w[0]*data['y_test_pred']/(w[0]*data['y_test_pred'] + \n",
    "                                             (1 - w[0]) * (1 - data['y_test_pred']))))**2\n",
    "\n",
    "print func(np.array([1]))\n",
    "\n",
    "res = minimize(func, np.array([1]), method='L-BFGS-B', bounds=[(0, 1)])\n",
    "\n",
    "print res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = res['x'][0]\n",
    "\n",
    "def fix_function(x):\n",
    "    return w*x/(w*x + (1 - w)*(1 - x))\n",
    "\n",
    "support = np.linspace(0, 1, 1000)\n",
    "values = fix_function(support)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(support, values)\n",
    "ax.set_title('Fix transformation', fontsize=20)\n",
    "ax.set_xlabel('x')\n",
    "ax.set_ylabel('f(x)')\n",
    "plt.show()\n",
    "\n",
    "data['y_test_pred_fixed'] = fix_function(data['y_test_pred'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax1 = plt.subplot2grid((1, 2), (0, 0))\n",
    "ax2 = plt.subplot2grid((1, 2), (0, 1))\n",
    "\n",
    "sns.distplot(data['y_train_pred'], label='train', ax=ax1)\n",
    "sns.distplot(data['y_test_pred_fixed'], label='test', ax=ax1)\n",
    "ax1.axvline(np.mean(data['y_train_pred']), color='b', alpha=1, linestyle='--', label='mean(train)')\n",
    "ax1.axvline(np.mean(data['y_test_pred_fixed']), color='g', alpha=1, linestyle='--', label='mean(test)')\n",
    "ax1.legend(loc='upper right', prop={'size': 18})\n",
    "ax1.set_title('Train/test OOF predictions')\n",
    "\n",
    "sns.distplot(data['y_train_pred'][:ix_train.shape[0]/2], label='train first part', ax=ax2)\n",
    "sns.distplot(data['y_train_pred'][ix_train.shape[0]/2:], label='train second part', ax=ax2)\n",
    "ax2.legend(loc='upper right', prop={'size': 18})\n",
    "ax2.set_title('Train/train OOF predictions')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['m_q1_q2_tf_oof'] = np.zeros(df.shape[0])\n",
    "df.loc[ix_train, 'm_q1_q2_tf_oof'] = data['y_train_pred']\n",
    "df.loc[ix_test, 'm_q1_q2_tf_oof'] = data['y_test_pred_fixed']\n",
    "del(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictors = df.columns[7:].tolist()\n",
    "print predictors\n",
    "\n",
    "if not os.path.isfile('./../tmp/5_model.pkl'):\n",
    "    model = check_model(predictors)\n",
    "    print model.best_score_\n",
    "    print model.best_params_\n",
    "    with open('./../tmp/5_model.pkl', 'wb') as f:\n",
    "        pickle.dump(model, f)\n",
    "else:\n",
    "    with open('./../tmp/5_model.pkl', 'rb') as f:\n",
    "        model = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "Fitting 5 folds for each of 90 candidates, totalling 450 fits\n",
    "[Parallel(n_jobs=-1)]: Done  18 tasks      | elapsed:   35.5s\n",
    "[Parallel(n_jobs=-1)]: Done 168 tasks      | elapsed:  3.5min\n",
    "[Parallel(n_jobs=-1)]: Done 418 tasks      | elapsed:  8.6min\n",
    "[Parallel(n_jobs=-1)]: Done 450 out of 450 | elapsed:  9.1min finished\n",
    "0.784444334512\n",
    "{'en__l1_ratio': 0.5, 'en__alpha': 1e-05}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_test_pred = model.predict_proba(df.loc[ix_test][predictors])[:, 1]\n",
    "y_test_pred_fixed = link_function(y_test_pred)\n",
    "\n",
    "if not os.path.isfile('./../submits/5_pred_fixed.csv'):\n",
    "    pd.DataFrame.from_records(\n",
    "        zip(df.loc[ix_test]['test_id'].values, \n",
    "            y_test_pred_fixed), \n",
    "        columns=['test_id', 'is_duplicate']).to_csv('./../submits/5_pred_fixed.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax1 = plt.subplot2grid((1, 2), (0, 0))\n",
    "ax2 = plt.subplot2grid((1, 2), (0, 1))\n",
    "sns.distplot(y_test_pred, ax=ax1)\n",
    "ax1.set_title('Model prediction')\n",
    "sns.distplot(y_test_pred_fixed, ax=ax2)\n",
    "ax2.set_title('Model prediction fixed')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(\n",
    "    eli5.sklearn.explain_linear_classifier_weights(\n",
    "        model.best_estimator_.steps[1][1],\n",
    "        feature_names=predictors))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Old value: **0.39816** and **0.36629**\n",
    "- New result: **0.3316**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_real_feature('m_q1_q2_tf_oof')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del(unigrams, bigrams, trigrams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Factoring the horizontally concatenated matrix and again OOF\n",
    "\n",
    "<img src=\"./../images/buben4.jpg\" />\n",
    "\n",
    "<br />\n",
    "\n",
    "<img src=\"./../images/mf1.png\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.isfile('./../tmp/1_svd.pkl'):\n",
    "    svd = TruncatedSVD(n_components=100)\n",
    "    m_svd = svd.fit_transform(sparse.csc_matrix(sparse.hstack((m_q1_tf, m_q2_tf))))\n",
    "    with open('./../tmp/1_svd.pkl', 'wb') as f:\n",
    "        pickle.dump(svd, f)\n",
    "    with open('./../tmp/1_m_svd.npz', 'wb') as f:\n",
    "        np.savez(f, m_svd)\n",
    "else:\n",
    "    with open('./../tmp/1_svd.pkl', 'rb') as f:\n",
    "        svd = pickle.load(f)\n",
    "    with open('./../tmp/1_m_svd.npz', 'rb') as f:\n",
    "        m_svd = np.load(f)['arr_0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.cumsum(svd.explained_variance_ratio_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax1 = plt.subplot2grid((2, 2), (0, 0))\n",
    "ax2 = plt.subplot2grid((2, 2), (0, 1))\n",
    "ax3 = plt.subplot2grid((2, 2), (1, 0))\n",
    "ax4 = plt.subplot2grid((2, 2), (1, 1))\n",
    "\n",
    "ax1.scatter(m_svd[np.where(df['is_duplicate'] == 0), 0], \n",
    "            m_svd[np.where(df['is_duplicate'] == 0), 1],\n",
    "            color='g', alpha=0.1, label='not dub')\n",
    "ax1.scatter(m_svd[np.where(df['is_duplicate'] == 1), 0], \n",
    "            m_svd[np.where(df['is_duplicate'] == 1), 1],\n",
    "            color='r', alpha=0.1, label='dub')\n",
    "ax1.set_title('rSVD: 0 vs 1')\n",
    "ax1.legend(loc='upper right', prop={'size': 18})\n",
    "\n",
    "ax2.scatter(m_svd[np.where(df['is_duplicate'] == 0), 0], \n",
    "            m_svd[np.where(df['is_duplicate'] == 0), 2],\n",
    "            color='g', alpha=0.1)\n",
    "ax2.scatter(m_svd[np.where(df['is_duplicate'] == 1), 0], \n",
    "            m_svd[np.where(df['is_duplicate'] == 1), 2],\n",
    "            color='r', alpha=0.1)\n",
    "ax2.set_title('rSVD: 0 vs 2')\n",
    "\n",
    "ax3.scatter(m_svd[np.where(df['is_duplicate'] == 0), 1], \n",
    "            m_svd[np.where(df['is_duplicate'] == 0), 2],\n",
    "            color='g', alpha=0.1)\n",
    "ax3.scatter(m_svd[np.where(df['is_duplicate'] == 1), 1], \n",
    "            m_svd[np.where(df['is_duplicate'] == 1), 2],\n",
    "            color='r', alpha=0.1)\n",
    "ax3.set_title('rSVD: 1 vs 2')\n",
    "\n",
    "ax4.scatter(m_svd[np.where(df['is_duplicate'] == 0), 0], \n",
    "            m_svd[np.where(df['is_duplicate'] == 0), 3],\n",
    "            color='g', alpha=0.1)\n",
    "ax4.scatter(m_svd[np.where(df['is_duplicate'] == 1), 0], \n",
    "            m_svd[np.where(df['is_duplicate'] == 1), 3],\n",
    "            color='r', alpha=0.1)\n",
    "ax4.set_title('rSVD: 0 vs 3')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['m_q1_q2_tf_svd0'] = m_svd[:, 0]\n",
    "plot_real_feature('m_q1_q2_tf_svd0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['m_q1_q2_tf_svd1'] = m_svd[:, 0]\n",
    "plot_real_feature('m_q1_q2_tf_svd1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['tmp'] = m_svd[:, 4]\n",
    "plot_real_feature('tmp')\n",
    "df = df.drop('tmp', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.isfile('./../tmp/6_model.pkl'):\n",
    "    model = check_model(None, data={\n",
    "        'X': m_svd[ix_train, :],\n",
    "        'y': df.loc[ix_train]['is_duplicate']\n",
    "    }, do_scaling=False)\n",
    "    print model.best_score_\n",
    "    print model.best_params_\n",
    "    with open('./../tmp/6_model.pkl', 'wb') as f:\n",
    "        pickle.dump(model, f)\n",
    "else:\n",
    "    with open('./../tmp/6_model.pkl', 'rb') as f:\n",
    "        model = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "Fitting 5 folds for each of 90 candidates, totalling 450 fits\n",
    "[Parallel(n_jobs=-1)]: Done  18 tasks      | elapsed:  1.8min\n",
    "[Parallel(n_jobs=-1)]: Done 168 tasks      | elapsed: 11.2min\n",
    "[Parallel(n_jobs=-1)]: Done 418 tasks      | elapsed: 29.6min\n",
    "[Parallel(n_jobs=-1)]: Done 450 out of 450 | elapsed: 31.9min finished\n",
    "0.68117193104\n",
    "{'en__l1_ratio': 0.5, 'en__alpha': 1e-05}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_test_pred = model.predict_proba(m_svd[ix_test, :])[:, 1]\n",
    "y_test_pred_fixed = link_function(y_test_pred)\n",
    "\n",
    "if not os.path.isfile('./../submits/6_pred_fixed.csv'):\n",
    "    pd.DataFrame.from_records(\n",
    "        zip(df.loc[ix_test]['test_id'].values, \n",
    "            y_test_pred_fixed), \n",
    "        columns=['test_id', 'is_duplicate']).to_csv('./../submits/6_pred_fixed.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax1 = plt.subplot2grid((1, 2), (0, 0))\n",
    "ax2 = plt.subplot2grid((1, 2), (0, 1))\n",
    "sns.distplot(y_test_pred, ax=ax1)\n",
    "ax1.set_title('Model prediction')\n",
    "sns.distplot(y_test_pred_fixed, ax=ax2)\n",
    "ax2.set_title('Model prediction fixed')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Result: **0.41092**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data={\n",
    "    'X_train': m_svd[ix_train, :],\n",
    "    'y_train': df.loc[ix_train]['is_duplicate'],\n",
    "    'X_test': m_svd[ix_test, :],\n",
    "    'y_train_pred': np.zeros(ix_train.shape[0]),\n",
    "    'y_test_pred': []\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.isfile('./../tmp/7_model_pred.pkl'):\n",
    "    n_splits = 10\n",
    "    folder = StratifiedKFold(n_splits=n_splits, shuffle=True)\n",
    "    for ix_first, ix_second in tqdm_notebook(folder.split(np.zeros(data['y_train'].shape[0]), data['y_train']), \n",
    "                                             total=n_splits):\n",
    "        # {'en__l1_ratio': 0.5, 'en__alpha': 1e-05}\n",
    "        model = SGDClassifier(\n",
    "            loss='log', \n",
    "            penalty='elasticnet', \n",
    "            fit_intercept=True, \n",
    "            n_iter=100, \n",
    "            shuffle=True, \n",
    "            n_jobs=-1,\n",
    "            l1_ratio=0.5,\n",
    "            alpha=1e-05,\n",
    "            class_weight=None)\n",
    "        model = model.fit(data['X_train'][ix_first, :], data['y_train'][ix_first])\n",
    "        data['y_train_pred'][ix_second] = model.predict_proba(data['X_train'][ix_second, :])[:, 1]\n",
    "        data['y_test_pred'].append(model.predict_proba(data['X_test'])[:, 1])\n",
    "        \n",
    "    data['y_test_pred'] = np.array(data['y_test_pred']).T.mean(axis=1)\n",
    "    with open('./../tmp/7_model_pred.pkl', 'wb') as f:\n",
    "        pickle.dump({\n",
    "            'model': model,\n",
    "            'y_train_pred': data['y_train_pred'],\n",
    "            'y_test_pred': data['y_test_pred']\n",
    "        }, f)\n",
    "else:\n",
    "    with open('./../tmp/7_model_pred.pkl', 'rb') as f:\n",
    "        tmp = pickle.load(f)\n",
    "        model = tmp['model']\n",
    "        data['y_train_pred'] = tmp['y_train_pred']\n",
    "        data['y_test_pred'] = tmp['y_test_pred']\n",
    "        del(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax1 = plt.subplot2grid((1, 2), (0, 0))\n",
    "ax2 = plt.subplot2grid((1, 2), (0, 1))\n",
    "\n",
    "sns.distplot(data['y_train_pred'], label='train', ax=ax1)\n",
    "sns.distplot(data['y_test_pred'], label='test', ax=ax1)\n",
    "ax1.axvline(np.mean(data['y_train_pred']), color='b', alpha=1, linestyle='--', label='mean(train)')\n",
    "ax1.axvline(np.mean(data['y_test_pred']), color='g', alpha=1, linestyle='--', label='mean(test)')\n",
    "ax1.legend(loc='upper right', prop={'size': 18})\n",
    "ax1.set_title('Train/test OOF predictions')\n",
    "\n",
    "sns.distplot(data['y_train_pred'][:ix_train.shape[0]/2], label='train first part', ax=ax2)\n",
    "sns.distplot(data['y_train_pred'][ix_train.shape[0]/2:], label='train second part', ax=ax2)\n",
    "ax2.legend(loc='upper right', prop={'size': 18})\n",
    "ax2.set_title('Train/train OOF predictions')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mp = np.mean(data['y_train_pred'])\n",
    "print mp\n",
    "\n",
    "def func(w):\n",
    "    return (mp*data['y_test_pred'].shape[0] - \n",
    "            np.sum(w[0]*data['y_test_pred']/(w[0]*data['y_test_pred'] + \n",
    "                                             (1 - w[0]) * (1 - data['y_test_pred']))))**2\n",
    "\n",
    "print func(np.array([1]))\n",
    "\n",
    "res = minimize(func, np.array([1]), method='L-BFGS-B', bounds=[(0, 1)])\n",
    "\n",
    "print res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = res['x'][0]\n",
    "\n",
    "def fix_function(x):\n",
    "    return w*x/(w*x + (1 - w)*(1 - x))\n",
    "\n",
    "support = np.linspace(0, 1, 1000)\n",
    "values = fix_function(support)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(support, values)\n",
    "ax.set_title('Fix transformation', fontsize=20)\n",
    "ax.set_xlabel('x')\n",
    "ax.set_ylabel('f(x)')\n",
    "plt.show()\n",
    "\n",
    "data['y_test_pred_fixed'] = fix_function(data['y_test_pred'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax1 = plt.subplot2grid((1, 2), (0, 0))\n",
    "ax2 = plt.subplot2grid((1, 2), (0, 1))\n",
    "\n",
    "sns.distplot(data['y_train_pred'], label='train', ax=ax1)\n",
    "sns.distplot(data['y_test_pred_fixed'], label='test', ax=ax1)\n",
    "ax1.axvline(np.mean(data['y_train_pred']), color='b', alpha=1, linestyle='--', label='mean(train)')\n",
    "ax1.axvline(np.mean(data['y_test_pred_fixed']), color='g', alpha=1, linestyle='--', label='mean(test)')\n",
    "ax1.legend(loc='upper right', prop={'size': 18})\n",
    "ax1.set_title('Train/test OOF predictions')\n",
    "\n",
    "sns.distplot(data['y_train_pred'][:ix_train.shape[0]/2], label='train first part', ax=ax2)\n",
    "sns.distplot(data['y_train_pred'][ix_train.shape[0]/2:], label='train second part', ax=ax2)\n",
    "ax2.legend(loc='upper right', prop={'size': 18})\n",
    "ax2.set_title('Train/train OOF predictions')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df['m_q1_q2_tf_svd100_oof'] = np.zeros(df.shape[0])\n",
    "df.loc[ix_train, 'm_q1_q2_tf_svd100_oof'] = data['y_train_pred']\n",
    "df.loc[ix_test, 'm_q1_q2_tf_svd100_oof'] = data['y_test_pred_fixed']\n",
    "del(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictors = df.columns[7:].tolist()\n",
    "print predictors\n",
    "\n",
    "if not os.path.isfile('./../tmp/8_model.pkl'):\n",
    "    model = check_model(predictors)\n",
    "    print model.best_score_\n",
    "    print model.best_params_\n",
    "    with open('./../tmp/8_model.pkl', 'wb') as f:\n",
    "        pickle.dump(model, f)\n",
    "else:\n",
    "    with open('./../tmp/8_model.pkl', 'rb') as f:\n",
    "        model = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "```python\n",
    "Fitting 5 folds for each of 90 candidates, totalling 450 fits\n",
    "[Parallel(n_jobs=-1)]: Done  18 tasks      | elapsed:   38.8s\n",
    "[Parallel(n_jobs=-1)]: Done 168 tasks      | elapsed:  3.8min\n",
    "[Parallel(n_jobs=-1)]: Done 418 tasks      | elapsed:  9.4min\n",
    "[Parallel(n_jobs=-1)]: Done 450 out of 450 | elapsed: 10.1min finished\n",
    "0.785762695095\n",
    "{'en__l1_ratio': 0.001, 'en__alpha': 1e-05}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_test_pred = model.predict_proba(df.loc[ix_test][predictors])[:, 1]\n",
    "y_test_pred_fixed = link_function(y_test_pred)\n",
    "\n",
    "if not os.path.isfile('./../submits/8_pred_fixed.csv'):\n",
    "    pd.DataFrame.from_records(\n",
    "        zip(df.loc[ix_test]['test_id'].values, \n",
    "            y_test_pred_fixed), \n",
    "        columns=['test_id', 'is_duplicate']).to_csv('./../submits/8_pred_fixed.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax1 = plt.subplot2grid((1, 2), (0, 0))\n",
    "ax2 = plt.subplot2grid((1, 2), (0, 1))\n",
    "sns.distplot(y_test_pred, ax=ax1)\n",
    "ax1.set_title('Model prediction')\n",
    "sns.distplot(y_test_pred_fixed, ax=ax2)\n",
    "ax2.set_title('Model prediction fixed')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "- Old value: **0.36629**\n",
    "- New result: **0.3316**\n",
    "- New value, but not improvement: **0.33694**\n",
    "- I want to believe that this is an error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_real_feature('m_q1_q2_tf_svd100_oof')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(\n",
    "    eli5.sklearn.explain_linear_classifier_weights(\n",
    "        model.best_estimator_.steps[1][1],\n",
    "        feature_names=predictors,\n",
    "        top=30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.jointplot('m_q1_q2_tf_oof', 'm_q1_q2_tf_svd100_oof', df.loc[ix_train], kind='reg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trying the difference instead of concatenation\n",
    "\n",
    "<img src=\"./../images/mf2.png\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del(m_q1, m_q2, m_svd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "m_diff_q1_q2 = m_q1_tf - m_q2_tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.isfile('./../tmp/9_model.pkl'):\n",
    "    model = check_model(None, data={\n",
    "        'X': m_diff_q1_q2[ix_train, :],\n",
    "        'y': df.loc[ix_train]['is_duplicate']\n",
    "    }, do_scaling=False)\n",
    "    print model.best_score_\n",
    "    print model.best_params_\n",
    "    with open('./../tmp/9_model.pkl', 'wb') as f:\n",
    "        pickle.dump(model, f)\n",
    "else:\n",
    "    with open('./../tmp/9_model.pkl', 'rb') as f:\n",
    "        model = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "Fitting 5 folds for each of 90 candidates, totalling 450 fits\n",
    "[Parallel(n_jobs=-1)]: Done  18 tasks      | elapsed:  2.5min\n",
    "[Parallel(n_jobs=-1)]: Done 168 tasks      | elapsed: 13.9min\n",
    "[Parallel(n_jobs=-1)]: Done 418 tasks      | elapsed: 34.0min\n",
    "[Parallel(n_jobs=-1)]: Done 450 out of 450 | elapsed: 36.1min finished\n",
    "0.630802146974\n",
    "{'en__l1_ratio': 0.01, 'en__alpha': 0.001}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_test_pred = model.predict_proba(m_diff_q1_q2[ix_test, :])[:, 1]\n",
    "y_test_pred_fixed = link_function(y_test_pred)\n",
    "\n",
    "if not os.path.isfile('./../submits/9_pred_fixed.csv'):\n",
    "    pd.DataFrame.from_records(\n",
    "        zip(df.loc[ix_test]['test_id'].values, \n",
    "            y_test_pred_fixed), \n",
    "        columns=['test_id', 'is_duplicate']).to_csv('./../submits/9_pred_fixed.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax1 = plt.subplot2grid((1, 2), (0, 0))\n",
    "ax2 = plt.subplot2grid((1, 2), (0, 1))\n",
    "sns.distplot(y_test_pred, ax=ax1)\n",
    "ax1.set_title('Model prediction')\n",
    "sns.distplot(y_test_pred_fixed, ax=ax2)\n",
    "ax2.set_title('Model prediction fixed')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Result: **0.46146**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data={\n",
    "    'X_train': m_diff_q1_q2[ix_train, :],\n",
    "    'y_train': df.loc[ix_train]['is_duplicate'],\n",
    "    'X_test': m_diff_q1_q2[ix_test, :],\n",
    "    'y_train_pred': np.zeros(ix_train.shape[0]),\n",
    "    'y_test_pred': []\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.isfile('./../tmp/10_model_pred.pkl'):\n",
    "    n_splits = 10\n",
    "    folder = StratifiedKFold(n_splits=n_splits, shuffle=True)\n",
    "    for ix_first, ix_second in tqdm_notebook(folder.split(np.zeros(data['y_train'].shape[0]), data['y_train']), \n",
    "                                             total=n_splits):\n",
    "        # {'en__l1_ratio': 0.01, 'en__alpha': 0.001}\n",
    "        model = SGDClassifier(\n",
    "            loss='log', \n",
    "            penalty='elasticnet', \n",
    "            fit_intercept=True, \n",
    "            n_iter=100, \n",
    "            shuffle=True, \n",
    "            n_jobs=-1,\n",
    "            l1_ratio=0.01,\n",
    "            alpha=0.001,\n",
    "            class_weight=None)\n",
    "        model = model.fit(data['X_train'][ix_first, :], data['y_train'][ix_first])\n",
    "        data['y_train_pred'][ix_second] = model.predict_proba(data['X_train'][ix_second, :])[:, 1]\n",
    "        data['y_test_pred'].append(model.predict_proba(data['X_test'])[:, 1])\n",
    "        \n",
    "    data['y_test_pred'] = np.array(data['y_test_pred']).T.mean(axis=1)\n",
    "    with open('./../tmp/10_model_pred.pkl', 'wb') as f:\n",
    "        pickle.dump({\n",
    "            'model': model,\n",
    "            'y_train_pred': data['y_train_pred'],\n",
    "            'y_test_pred': data['y_test_pred']\n",
    "        }, f)\n",
    "else:\n",
    "    with open('./../tmp/10_model_pred.pkl', 'rb') as f:\n",
    "        tmp = pickle.load(f)\n",
    "        model = tmp['model']\n",
    "        data['y_train_pred'] = tmp['y_train_pred']\n",
    "        data['y_test_pred'] = tmp['y_test_pred']\n",
    "        del(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax1 = plt.subplot2grid((1, 2), (0, 0))\n",
    "ax2 = plt.subplot2grid((1, 2), (0, 1))\n",
    "\n",
    "sns.distplot(data['y_train_pred'], label='train', ax=ax1)\n",
    "sns.distplot(data['y_test_pred'], label='test', ax=ax1)\n",
    "ax1.axvline(np.mean(data['y_train_pred']), color='b', alpha=1, linestyle='--', label='mean(train)')\n",
    "ax1.axvline(np.mean(data['y_test_pred']), color='g', alpha=1, linestyle='--', label='mean(test)')\n",
    "ax1.legend(loc='upper right', prop={'size': 18})\n",
    "ax1.set_title('Train/test OOF predictions')\n",
    "\n",
    "sns.distplot(data['y_train_pred'][:ix_train.shape[0]/2], label='train first part', ax=ax2)\n",
    "sns.distplot(data['y_train_pred'][ix_train.shape[0]/2:], label='train second part', ax=ax2)\n",
    "ax2.legend(loc='upper right', prop={'size': 18})\n",
    "ax2.set_title('Train/train OOF predictions')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['m_diff_q1_q2_tf_oof'] = np.zeros(df.shape[0])\n",
    "df.loc[ix_train, 'm_diff_q1_q2_tf_oof'] = data['y_train_pred']\n",
    "df.loc[ix_test, 'm_diff_q1_q2_tf_oof'] = data['y_test_pred']\n",
    "del(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_real_feature('m_diff_q1_q2_tf_oof')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del(m_diff_q1_q2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Normalized Validation\n",
    "\n",
    "<img src=\"./../images/buben5.jpg\" width=\"70%\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictors = df.columns[7:].tolist()\n",
    "print predictors\n",
    "\n",
    "if not os.path.isfile('./../tmp/11_model.pkl'):\n",
    "    model = check_model(predictors)\n",
    "    print model.best_score_\n",
    "    print model.best_params_\n",
    "    with open('./../tmp/11_model.pkl', 'wb') as f:\n",
    "        pickle.dump(model, f)\n",
    "else:\n",
    "    with open('./../tmp/11_model.pkl', 'rb') as f:\n",
    "        model = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "```python\n",
    "Fitting 5 folds for each of 90 candidates, totalling 450 fits\n",
    "[Parallel(n_jobs=-1)]: Done  18 tasks      | elapsed:   40.9s\n",
    "[Parallel(n_jobs=-1)]: Done 168 tasks      | elapsed:  3.9min\n",
    "[Parallel(n_jobs=-1)]: Done 418 tasks      | elapsed:  9.8min\n",
    "[Parallel(n_jobs=-1)]: Done 450 out of 450 | elapsed: 10.4min finished\n",
    "0.786160924089\n",
    "{'en__l1_ratio': 0.75, 'en__alpha': 1e-05}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred = model.predict_proba(df.loc[ix_test][predictors])[:, 1]\n",
    "y_test_pred_fixed = link_function(y_test_pred)\n",
    "\n",
    "if not os.path.isfile('./../submits/11_pred_fixed.csv'):\n",
    "    pd.DataFrame.from_records(\n",
    "        zip(df.loc[ix_test]['test_id'].values, \n",
    "            y_test_pred_fixed), \n",
    "        columns=['test_id', 'is_duplicate']).to_csv('./../submits/11_pred_fixed.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax1 = plt.subplot2grid((1, 2), (0, 0))\n",
    "ax2 = plt.subplot2grid((1, 2), (0, 1))\n",
    "sns.distplot(y_test_pred, ax=ax1)\n",
    "ax1.set_title('Model prediction')\n",
    "sns.distplot(y_test_pred_fixed, ax=ax2)\n",
    "ax2.set_title('Model prediction fixed')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "- Result: **0.33349**\n",
    "- The best result was earlier: **0.3316**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(\n",
    "    eli5.sklearn.explain_linear_classifier_weights(\n",
    "        model.best_estimator_.steps[1][1],\n",
    "        feature_names=predictors, top=30))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "$$\\large\\begin{array}{rcl}\n",
    "t_0 &=& \\frac{n_0}{n_0 + n_1} \\Rightarrow \\\\\n",
    "n_1 &=& \\frac{n_0 t_1}{t_0}\n",
    "\\end{array}$$\n",
    "\n",
    "- Train and the test split its proportions $t_i$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuoraKFold():\n",
    "    \n",
    "    def __init__(self, n_splits=10):\n",
    "        self.n_splits = n_splits\n",
    "        \n",
    "    def get_n_splits(self, X=None, y=None, groups=None):\n",
    "        return self.n_splits\n",
    "        \n",
    "    def split(self, X, y, groups=None):\n",
    "        r_split = 1 - 1.0/self.n_splits\n",
    "        k = r1*d[0]/(r0*d[1])\n",
    "        ix_ones = np.where(y == 1)[0]\n",
    "        ix_zeros = np.where(y == 0)[0]\n",
    "        for i in range(self.n_splits):\n",
    "            ix_first_zeros = np.random.choice(\n",
    "                ix_zeros, \n",
    "                size=int(r_split * ix_zeros.shape[0]), \n",
    "                replace=False)\n",
    "            ix_first_ones = np.random.choice(\n",
    "                ix_ones,\n",
    "                size=int(ix_first_zeros.shape[0] * d[1]/d[0]),\n",
    "                replace=False)\n",
    "            \n",
    "            ix_second_zeros = np.setdiff1d(ix_zeros, ix_first_zeros)\n",
    "            ix_second_ones = np.random.choice(\n",
    "                np.setdiff1d(ix_ones, ix_first_ones),\n",
    "                size=int(ix_second_zeros.shape[0] * r1/r0),\n",
    "                replace=False)\n",
    "            \n",
    "            ix_first = np.hstack((ix_first_zeros, ix_first_ones))\n",
    "            ix_first = ix_first[np.random.choice(\n",
    "                range(ix_first.shape[0]), size=ix_first.shape[0], replace=False)]\n",
    "            \n",
    "            ix_second = np.hstack((ix_second_zeros, ix_second_ones))\n",
    "            ix_second = ix_second[np.random.choice(\n",
    "                range(ix_second.shape[0]), size=ix_second.shape[0], replace=False)]\n",
    "            \n",
    "            yield ix_first, ix_second"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def log_loss_lf(y_true, y_pred):\n",
    "    return log_loss(y_true, link_function(y_pred[:, 1]), eps=eps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_model(predictors, data=None, do_scaling=True, folder=None):\n",
    "    classifier = lambda: SGDClassifier(\n",
    "        loss='log', \n",
    "        penalty='elasticnet', \n",
    "        fit_intercept=True, \n",
    "        n_iter=100, \n",
    "        shuffle=True, \n",
    "        n_jobs=-1,\n",
    "        class_weight=None)\n",
    "\n",
    "    steps = []\n",
    "    if do_scaling:\n",
    "        steps.append(('ss', StandardScaler()))\n",
    "    steps.append(('en', classifier()))\n",
    "    \n",
    "    model = Pipeline(steps=steps)\n",
    "\n",
    "    parameters = {\n",
    "        'en__alpha': [0.00001, 0.0001, 0.001, 0.01, 0.02, 0.1, 0.5, 0.9, 1],\n",
    "        'en__l1_ratio': [0, 0.0001, 0.001, 0.01, 0.1, 0.3, 0.5, 0.75, 0.9, 1]\n",
    "    }\n",
    "\n",
    "    if folder is None:\n",
    "        folder = QuoraKFold(n_splits=10)\n",
    "    \n",
    "    grid_search = GridSearchCV(\n",
    "        model, \n",
    "        parameters, \n",
    "        cv=folder, \n",
    "        n_jobs=-1,\n",
    "        scoring=make_scorer(log_loss_lf, greater_is_better=False, needs_proba=True),\n",
    "        verbose=1)\n",
    "    if data is None:\n",
    "        grid_search = grid_search.fit(df.loc[ix_train][predictors], \n",
    "                                      df.loc[ix_train]['is_duplicate'])\n",
    "    else:\n",
    "        grid_search = grid_search.fit(data['X'], \n",
    "                                      data['y'])\n",
    "    \n",
    "    return grid_search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.isfile('./../tmp/12_model.pkl'):\n",
    "    model = check_model(predictors, folder=StratifiedKFold(n_splits=5, shuffle=True))\n",
    "    print -model.best_score_\n",
    "    print model.best_params_\n",
    "    with open('./../tmp/12_model.pkl', 'wb') as f:\n",
    "        pickle.dump(model, f)\n",
    "else:\n",
    "    with open('./../tmp/12_model.pkl', 'rb') as f:\n",
    "        model = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "Fitting 5 folds for each of 90 candidates, totalling 450 fits\n",
    "[Parallel(n_jobs=-1)]: Done  18 tasks      | elapsed:   40.5s\n",
    "[Parallel(n_jobs=-1)]: Done 168 tasks      | elapsed:  4.0min\n",
    "[Parallel(n_jobs=-1)]: Done 418 tasks      | elapsed:  9.8min\n",
    "[Parallel(n_jobs=-1)]: Done 450 out of 450 | elapsed: 10.5min finished\n",
    "0.50136321542\n",
    "{'en__l1_ratio': 0.01, 'en__alpha': 1e-05}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.isfile('./../tmp/13_model.pkl'):\n",
    "    model = check_model(predictors)\n",
    "    print -model.best_score_\n",
    "    print model.best_params_\n",
    "    with open('./../tmp/13_model.pkl', 'wb') as f:\n",
    "        pickle.dump(model, f)\n",
    "else:\n",
    "    with open('./../tmp/13_model.pkl', 'rb') as f:\n",
    "        model = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "```python\n",
    "Fitting 10 folds for each of 90 candidates, totalling 900 fits\n",
    "[Parallel(n_jobs=-1)]: Done  18 tasks      | elapsed:   46.7s\n",
    "[Parallel(n_jobs=-1)]: Done 168 tasks      | elapsed:  4.5min\n",
    "[Parallel(n_jobs=-1)]: Done 418 tasks      | elapsed: 10.9min\n",
    "[Parallel(n_jobs=-1)]: Done 768 tasks      | elapsed: 19.9min\n",
    "[Parallel(n_jobs=-1)]: Done 900 out of 900 | elapsed: 23.4min finished\n",
    "0.323176002744\n",
    "{'en__l1_ratio': 1, 'en__alpha': 0.0001}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_test_pred = model.predict_proba(df.loc[ix_test][predictors])[:, 1]\n",
    "y_test_pred_fixed = link_function(y_test_pred)\n",
    "\n",
    "if not os.path.isfile('./../submits/13_pred_fixed.csv'):\n",
    "    pd.DataFrame.from_records(\n",
    "        zip(df.loc[ix_test]['test_id'].values, \n",
    "            y_test_pred_fixed), \n",
    "        columns=['test_id', 'is_duplicate']).to_csv('./../submits/13_pred_fixed.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax1 = plt.subplot2grid((1, 2), (0, 0))\n",
    "ax2 = plt.subplot2grid((1, 2), (0, 1))\n",
    "sns.distplot(y_test_pred, ax=ax1)\n",
    "ax1.set_title('Model prediction')\n",
    "sns.distplot(y_test_pred_fixed, ax=ax2)\n",
    "ax2.set_title('Model prediction fixed')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "- Result: **0.33524** Almost like in validation\n",
    "- The best result was earlier: **0.3316**\n",
    "- Let's hope that this is also an error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Factorization of a vertically concatenated matrix\n",
    "\n",
    "<img src=\"./../images/mf3.png\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if not os.path.isfile('./../tmp/2_svd.pkl'):\n",
    "    svd = TruncatedSVD(n_components=100)\n",
    "    m_svd = svd.fit_transform(sparse.csc_matrix(sparse.vstack((m_q1_tf, m_q2_tf))))\n",
    "    with open('./../tmp/2_svd.pkl', 'wb') as f:\n",
    "        pickle.dump(svd, f)\n",
    "    with open('./../tmp/2_m_svd.npz', 'wb') as f:\n",
    "        np.savez(f, m_svd)\n",
    "else:\n",
    "    with open('./../tmp/2_svd.pkl', 'rb') as f:\n",
    "        svd = pickle.load(f)\n",
    "    with open('./../tmp/2_m_svd.npz', 'rb') as f:\n",
    "        m_svd = np.load(f)['arr_0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del(m_q1_tf, m_q2_tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_svd_q1 = m_svd[:m_svd.shape[0]/2, :]\n",
    "m_svd_q2 = m_svd[m_svd.shape[0]/2:, :]\n",
    "del(m_svd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['m_vstack_svd_q1_q1_euclidean'] = ((m_svd_q1 - m_svd_q2)**2).mean(axis=1)\n",
    "plot_real_feature('m_vstack_svd_q1_q1_euclidean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num = (m_svd_q1*m_svd_q2).sum(axis=1)\n",
    "den = np.sqrt((m_svd_q1**2).sum(axis=1))*np.sqrt((m_svd_q2**2).sum(axis=1))\n",
    "num[np.where(den == 0)] = 0\n",
    "den[np.where(den == 0)] = 1\n",
    "df['m_vstack_svd_q1_q1_cosine'] = 1 - num/den\n",
    "plot_real_feature('m_vstack_svd_q1_q1_cosine')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### OOF from the product svd of vertical concatenation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "m_svd = m_svd_q1*m_svd_q2\n",
    "\n",
    "if not os.path.isfile('./../tmp/14_model.pkl'):\n",
    "    model = check_model(None, data={\n",
    "        'X': m_svd[ix_train, :],\n",
    "        'y': df.loc[ix_train]['is_duplicate']\n",
    "    }, do_scaling=False)\n",
    "    print model.best_score_\n",
    "    print model.best_params_\n",
    "    with open('./../tmp/14_model.pkl', 'wb') as f:\n",
    "        pickle.dump(model, f)\n",
    "else:\n",
    "    with open('./../tmp/14_model.pkl', 'rb') as f:\n",
    "        model = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "Fitting 10 folds for each of 90 candidates, totalling 900 fits\n",
    "[Parallel(n_jobs=-1)]: Done  18 tasks      | elapsed:  2.1min\n",
    "[Parallel(n_jobs=-1)]: Done 168 tasks      | elapsed: 12.3min\n",
    "[Parallel(n_jobs=-1)]: Done 418 tasks      | elapsed: 31.6min\n",
    "[Parallel(n_jobs=-1)]: Done 768 tasks      | elapsed: 60.1min\n",
    "[Parallel(n_jobs=-1)]: Done 900 out of 900 | elapsed: 70.9min finished\n",
    "-0.409563471096\n",
    "{'en__l1_ratio': 1, 'en__alpha': 1e-05}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data={\n",
    "    'X_train': m_svd[ix_train, :],\n",
    "    'y_train': df.loc[ix_train]['is_duplicate'],\n",
    "    'X_test': m_svd[ix_test, :],\n",
    "    'y_train_pred': np.zeros(ix_train.shape[0]),\n",
    "    'y_test_pred': []\n",
    "}\n",
    "del(m_svd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.isfile('./../tmp/15_model_pred.pkl'):\n",
    "    n_splits = 10\n",
    "    folder = StratifiedKFold(n_splits=n_splits, shuffle=True)\n",
    "    for ix_first, ix_second in tqdm_notebook(folder.split(np.zeros(data['y_train'].shape[0]), data['y_train']), \n",
    "                                             total=n_splits):\n",
    "        # {'en__l1_ratio': 1, 'en__alpha': 1e-05}\n",
    "        model = SGDClassifier(\n",
    "            loss='log', \n",
    "            penalty='elasticnet', \n",
    "            fit_intercept=True, \n",
    "            n_iter=100, \n",
    "            shuffle=True, \n",
    "            n_jobs=-1,\n",
    "            l1_ratio=1.0,\n",
    "            alpha=1e-05,\n",
    "            class_weight=None)\n",
    "        model = model.fit(data['X_train'][ix_first, :], data['y_train'][ix_first])\n",
    "        data['y_train_pred'][ix_second] = model.predict_proba(data['X_train'][ix_second, :])[:, 1]\n",
    "        data['y_test_pred'].append(model.predict_proba(data['X_test'])[:, 1])\n",
    "        \n",
    "    data['y_test_pred'] = np.array(data['y_test_pred']).T.mean(axis=1)\n",
    "    with open('./../tmp/15_model_pred.pkl', 'wb') as f:\n",
    "        pickle.dump({\n",
    "            'model': model,\n",
    "            'y_train_pred': data['y_train_pred'],\n",
    "            'y_test_pred': data['y_test_pred']\n",
    "        }, f)\n",
    "else:\n",
    "    with open('./../tmp/15_model_pred.pkl', 'rb') as f:\n",
    "        tmp = pickle.load(f)\n",
    "        model = tmp['model']\n",
    "        data['y_train_pred'] = tmp['y_train_pred']\n",
    "        data['y_test_pred'] = tmp['y_test_pred']\n",
    "        del(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax1 = plt.subplot2grid((1, 2), (0, 0))\n",
    "ax2 = plt.subplot2grid((1, 2), (0, 1))\n",
    "\n",
    "sns.distplot(data['y_train_pred'], label='train', ax=ax1)\n",
    "sns.distplot(data['y_test_pred'], label='test', ax=ax1)\n",
    "ax1.axvline(np.mean(data['y_train_pred']), color='b', alpha=1, linestyle='--', label='mean(train)')\n",
    "ax1.axvline(np.mean(data['y_test_pred']), color='g', alpha=1, linestyle='--', label='mean(test)')\n",
    "ax1.legend(loc='upper right', prop={'size': 18})\n",
    "ax1.set_title('Train/test OOF predictions')\n",
    "\n",
    "sns.distplot(data['y_train_pred'][:ix_train.shape[0]/2], label='train first part', ax=ax2)\n",
    "sns.distplot(data['y_train_pred'][ix_train.shape[0]/2:], label='train second part', ax=ax2)\n",
    "ax2.legend(loc='upper right', prop={'size': 18})\n",
    "ax2.set_title('Train/train OOF predictions')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mp = np.mean(data['y_train_pred'])\n",
    "print mp\n",
    "\n",
    "def func(w):\n",
    "    return (mp*data['y_test_pred'].shape[0] - \n",
    "            np.sum(w[0]*data['y_test_pred']/(w[0]*data['y_test_pred'] + \n",
    "                                             (1 - w[0]) * (1 - data['y_test_pred']))))**2\n",
    "\n",
    "print func(np.array([1]))\n",
    "\n",
    "res = minimize(func, np.array([1]), method='L-BFGS-B', bounds=[(0, 1)])\n",
    "\n",
    "print res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = res['x'][0]\n",
    "\n",
    "def fix_function(x):\n",
    "    return w*x/(w*x + (1 - w)*(1 - x))\n",
    "\n",
    "support = np.linspace(0, 1, 1000)\n",
    "values = fix_function(support)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(support, values)\n",
    "ax.set_title('Fix transformation', fontsize=20)\n",
    "ax.set_xlabel('x')\n",
    "ax.set_ylabel('f(x)')\n",
    "plt.show()\n",
    "\n",
    "data['y_test_pred_fixed'] = fix_function(data['y_test_pred'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax1 = plt.subplot2grid((1, 2), (0, 0))\n",
    "ax2 = plt.subplot2grid((1, 2), (0, 1))\n",
    "\n",
    "sns.distplot(data['y_train_pred'], label='train', ax=ax1)\n",
    "sns.distplot(data['y_test_pred_fixed'], label='test', ax=ax1)\n",
    "ax1.axvline(np.mean(data['y_train_pred']), color='b', alpha=1, linestyle='--', label='mean(train)')\n",
    "ax1.axvline(np.mean(data['y_test_pred_fixed']), color='g', alpha=1, linestyle='--', label='mean(test)')\n",
    "ax1.legend(loc='upper right', prop={'size': 18})\n",
    "ax1.set_title('Train/test OOF predictions')\n",
    "\n",
    "sns.distplot(data['y_train_pred'][:ix_train.shape[0]/2], label='train first part', ax=ax2)\n",
    "sns.distplot(data['y_train_pred'][ix_train.shape[0]/2:], label='train second part', ax=ax2)\n",
    "ax2.legend(loc='upper right', prop={'size': 18})\n",
    "ax2.set_title('Train/train OOF predictions')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['m_vstack_svd_mult_q1_q2_oof'] = np.zeros(df.shape[0])\n",
    "df.loc[ix_train, 'm_vstack_svd_mult_q1_q2_oof'] = data['y_train_pred']\n",
    "df.loc[ix_test, 'm_vstack_svd_mult_q1_q2_oof'] = data['y_test_pred_fixed']\n",
    "del(data)\n",
    "plot_real_feature('m_vstack_svd_mult_q1_q2_oof')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### OOF from the vertical concatenation svd difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_svd = np.abs(m_svd_q1 - m_svd_q2)\n",
    "\n",
    "if not os.path.isfile('./../tmp/16_model.pkl'):\n",
    "    model = check_model(None, data={\n",
    "        'X': m_svd[ix_train, :],\n",
    "        'y': df.loc[ix_train]['is_duplicate']\n",
    "    }, do_scaling=False)\n",
    "    print model.best_score_\n",
    "    print model.best_params_\n",
    "    with open('./../tmp/16_model.pkl', 'wb') as f:\n",
    "        pickle.dump(model, f)\n",
    "else:\n",
    "    with open('./../tmp/16_model.pkl', 'rb') as f:\n",
    "        model = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "```python\n",
    "\n",
    "[Parallel(n_jobs=-1)]: Done  18 tasks      | elapsed:  2.1min\n",
    "[Parallel(n_jobs=-1)]: Done 168 tasks      | elapsed: 12.5min\n",
    "[Parallel(n_jobs=-1)]: Done 418 tasks      | elapsed: 30.0min\n",
    "[Parallel(n_jobs=-1)]: Done 768 tasks      | elapsed: 54.6min\n",
    "[Parallel(n_jobs=-1)]: Done 900 out of 900 | elapsed: 63.4min finished\n",
    "-0.439305223632\n",
    "{'en__l1_ratio': 0.01, 'en__alpha': 1e-05}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data={\n",
    "    'X_train': m_svd[ix_train, :],\n",
    "    'y_train': df.loc[ix_train]['is_duplicate'],\n",
    "    'X_test': m_svd[ix_test, :],\n",
    "    'y_train_pred': np.zeros(ix_train.shape[0]),\n",
    "    'y_test_pred': []\n",
    "}\n",
    "del(m_svd, m_svd_q1, m_svd_q2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.isfile('./../tmp/17_model_pred.pkl'):\n",
    "    n_splits = 10\n",
    "    folder = StratifiedKFold(n_splits=n_splits, shuffle=True)\n",
    "    for ix_first, ix_second in tqdm_notebook(folder.split(np.zeros(data['y_train'].shape[0]), data['y_train']), \n",
    "                                             total=n_splits):\n",
    "        # {'en__l1_ratio': 0.01, 'en__alpha': 1e-05}\n",
    "        model = SGDClassifier(\n",
    "            loss='log', \n",
    "            penalty='elasticnet', \n",
    "            fit_intercept=True, \n",
    "            n_iter=100, \n",
    "            shuffle=True, \n",
    "            n_jobs=-1,\n",
    "            l1_ratio=0.01,\n",
    "            alpha=1e-05,\n",
    "            class_weight=None)\n",
    "        model = model.fit(data['X_train'][ix_first, :], data['y_train'][ix_first])\n",
    "        data['y_train_pred'][ix_second] = model.predict_proba(data['X_train'][ix_second, :])[:, 1]\n",
    "        data['y_test_pred'].append(model.predict_proba(data['X_test'])[:, 1])\n",
    "        \n",
    "    data['y_test_pred'] = np.array(data['y_test_pred']).T.mean(axis=1)\n",
    "    with open('./../tmp/17_model_pred.pkl', 'wb') as f:\n",
    "        pickle.dump({\n",
    "            'model': model,\n",
    "            'y_train_pred': data['y_train_pred'],\n",
    "            'y_test_pred': data['y_test_pred']\n",
    "        }, f)\n",
    "else:\n",
    "    with open('./../tmp/17_model_pred.pkl', 'rb') as f:\n",
    "        tmp = pickle.load(f)\n",
    "        model = tmp['model']\n",
    "        data['y_train_pred'] = tmp['y_train_pred']\n",
    "        data['y_test_pred'] = tmp['y_test_pred']\n",
    "        del(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax1 = plt.subplot2grid((1, 2), (0, 0))\n",
    "ax2 = plt.subplot2grid((1, 2), (0, 1))\n",
    "\n",
    "sns.distplot(data['y_train_pred'], label='train', ax=ax1)\n",
    "sns.distplot(data['y_test_pred'], label='test', ax=ax1)\n",
    "ax1.axvline(np.mean(data['y_train_pred']), color='b', alpha=1, linestyle='--', label='mean(train)')\n",
    "ax1.axvline(np.mean(data['y_test_pred']), color='g', alpha=1, linestyle='--', label='mean(test)')\n",
    "ax1.legend(loc='upper right', prop={'size': 18})\n",
    "ax1.set_title('Train/test OOF predictions')\n",
    "\n",
    "sns.distplot(data['y_train_pred'][:ix_train.shape[0]/2], label='train first part', ax=ax2)\n",
    "sns.distplot(data['y_train_pred'][ix_train.shape[0]/2:], label='train second part', ax=ax2)\n",
    "ax2.legend(loc='upper right', prop={'size': 18})\n",
    "ax2.set_title('Train/train OOF predictions')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mp = np.mean(data['y_train_pred'])\n",
    "print mp\n",
    "\n",
    "def func(w):\n",
    "    return (mp*data['y_test_pred'].shape[0] - \n",
    "            np.sum(w[0]*data['y_test_pred']/(w[0]*data['y_test_pred'] + \n",
    "                                             (1 - w[0]) * (1 - data['y_test_pred']))))**2\n",
    "\n",
    "print func(np.array([1]))\n",
    "\n",
    "res = minimize(func, np.array([1]), method='L-BFGS-B', bounds=[(0, 1)])\n",
    "\n",
    "print res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = res['x'][0]\n",
    "\n",
    "def fix_function(x):\n",
    "    return w*x/(w*x + (1 - w)*(1 - x))\n",
    "\n",
    "support = np.linspace(0, 1, 1000)\n",
    "values = fix_function(support)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(support, values)\n",
    "ax.set_title('Fix transformation', fontsize=20)\n",
    "ax.set_xlabel('x')\n",
    "ax.set_ylabel('f(x)')\n",
    "plt.show()\n",
    "\n",
    "data['y_test_pred_fixed'] = fix_function(data['y_test_pred'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax1 = plt.subplot2grid((1, 2), (0, 0))\n",
    "ax2 = plt.subplot2grid((1, 2), (0, 1))\n",
    "\n",
    "sns.distplot(data['y_train_pred'], label='train', ax=ax1)\n",
    "sns.distplot(data['y_test_pred_fixed'], label='test', ax=ax1)\n",
    "ax1.axvline(np.mean(data['y_train_pred']), color='b', alpha=1, linestyle='--', label='mean(train)')\n",
    "ax1.axvline(np.mean(data['y_test_pred_fixed']), color='g', alpha=1, linestyle='--', label='mean(test)')\n",
    "ax1.legend(loc='upper right', prop={'size': 18})\n",
    "ax1.set_title('Train/test OOF predictions')\n",
    "\n",
    "sns.distplot(data['y_train_pred'][:ix_train.shape[0]/2], label='train first part', ax=ax2)\n",
    "sns.distplot(data['y_train_pred'][ix_train.shape[0]/2:], label='train second part', ax=ax2)\n",
    "ax2.legend(loc='upper right', prop={'size': 18})\n",
    "ax2.set_title('Train/train OOF predictions')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['m_vstack_svd_absdiff_q1_q2_oof'] = np.zeros(df.shape[0])\n",
    "df.loc[ix_train, 'm_vstack_svd_absdiff_q1_q2_oof'] = data['y_train_pred']\n",
    "df.loc[ix_test, 'm_vstack_svd_absdiff_q1_q2_oof'] = data['y_test_pred']\n",
    "del(data)\n",
    "plot_real_feature('m_vstack_svd_absdiff_q1_q2_oof')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Process words as a whole\n",
    "<img src=\"./../images/buben6.jpg\" width=\"70%\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatized words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en')\n",
    "df.head()['question1'].apply(lambda s: ' '.join([c.lemma_ for c in nlp(unicode(s)) if c.lemma_  != '?']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SYMBOLS = set(' '.join(string.punctuation).split(' ') + ['...', '“', '”', '\\'ve'])\n",
    "\n",
    "if not os.path.isfile('./../tmp/bow_lemma.pkl'):\n",
    "    q1 = []\n",
    "\n",
    "    for doc in nlp.pipe(df['question1'].str.decode('utf-8'), n_threads=16, batch_size=10000):\n",
    "        q1.append([c.lemma_ for c in doc if c.lemma_ not in SYMBOLS])\n",
    "\n",
    "    q2 = []\n",
    "\n",
    "    for doc in nlp.pipe(df['question2'].str.decode('utf-8'), n_threads=16, batch_size=10000):\n",
    "        q2.append([c.lemma_ for c in doc if c.lemma_ not in SYMBOLS])\n",
    "        \n",
    "    with open('./../tmp/bow_lemma.pkl', 'wb') as f:\n",
    "        pickle.dump({\n",
    "            'q1': q1,\n",
    "            'q2': q2\n",
    "        }, f)\n",
    "else:\n",
    "    with open('./../tmp/bow_lemma.pkl', 'rb') as f:\n",
    "        tmp = pickle.load(f)\n",
    "        q1 = tmp['q1']\n",
    "        q2 = tmp['q2']\n",
    "        del(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "if os.path.isfile('./../tmp/cv_word_lemma.pkl') and os.path.isfile('./../tmp/wl_freq.pkl'):\n",
    "    with open('./../tmp/cv_word_lemma.pkl', 'rb') as f:\n",
    "        cv_words = pickle.load(f)\n",
    "    with open('./../tmp/wl_freq.pkl', 'rb') as f:\n",
    "        w_freq = pickle.load(f)\n",
    "else:\n",
    "    cv_words = CountVectorizer(ngram_range=(1, 1), analyzer='word')\n",
    "    w_freq = np.array(cv_words.fit_transform(\n",
    "        [' '.join(s) for s in q1] + [' '.join(s) for s in q2]).sum(axis=0))[0, :]\n",
    "    with open('./../tmp/cv_word_lemma.pkl', 'wb') as f:\n",
    "        pickle.dump(cv_words, f)\n",
    "    with open('./../tmp/wl_freq.pkl', 'wb') as f:\n",
    "        pickle.dump(w_freq, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.isfile('./../tmp/m_q1_wl.npz') and os.path.isfile('./../tmp/m_q2_wl.npz'):\n",
    "    m_q1 = load_sparse_csr('./../tmp/m_q1_wl.npz')\n",
    "    m_q2 = load_sparse_csr('./../tmp/m_q2_wl.npz')\n",
    "else:\n",
    "    m_q1 = cv_words.transform([' '.join(s) for s in q1])\n",
    "    m_q2 = cv_words.transform([' '.join(s) for s in q2])\n",
    "    save_sparse_csr('./../tmp/m_q1_wl.npz', m_q1)\n",
    "    save_sparse_csr('./../tmp/m_q2_wl.npz', m_q2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tft = TfidfTransformer(\n",
    "    norm='l2', \n",
    "    use_idf=True, \n",
    "    smooth_idf=True, \n",
    "    sublinear_tf=False)\n",
    "\n",
    "tft = tft.fit(sparse.vstack((m_q1, m_q2)))\n",
    "m_q1_tf = tft.transform(m_q1)\n",
    "m_q2_tf = tft.transform(m_q2)\n",
    "\n",
    "v_num = np.array(m_q1_tf.multiply(m_q2_tf).sum(axis=1))[:, 0]\n",
    "v_den = np.array(np.sqrt(m_q1_tf.multiply(m_q1_tf).sum(axis=1)))[:, 0] * \\\n",
    "        np.array(np.sqrt(m_q2_tf.multiply(m_q2_tf).sum(axis=1)))[:, 0]\n",
    "v_num[np.where(v_den == 0)] = 1\n",
    "v_den[np.where(v_den == 0)] = 1\n",
    "\n",
    "v_score = 1 - v_num/v_den\n",
    "\n",
    "df['1wl_tfidf_cosine'] = v_score\n",
    "plot_real_feature('1wl_tfidf_cosine')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tft = TfidfTransformer(\n",
    "    norm='l2', \n",
    "    use_idf=True, \n",
    "    smooth_idf=True, \n",
    "    sublinear_tf=False)\n",
    "\n",
    "tft = tft.fit(sparse.vstack((m_q1, m_q2)))\n",
    "m_q1_tf = tft.transform(m_q1)\n",
    "m_q2_tf = tft.transform(m_q2)\n",
    "\n",
    "v_score = (m_q1_tf - m_q2_tf)\n",
    "v_score = np.sqrt(np.array(v_score.multiply(v_score).sum(axis=1))[:, 0])\n",
    "\n",
    "df['1wl_tfidf_l2_euclidean'] = v_score\n",
    "plot_real_feature('1wl_tfidf_l2_euclidean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tft = TfidfTransformer(\n",
    "    norm='l2', \n",
    "    use_idf=False, \n",
    "    smooth_idf=True, \n",
    "    sublinear_tf=False)\n",
    "\n",
    "tft = tft.fit(sparse.vstack((m_q1, m_q2)))\n",
    "m_q1_tf = tft.transform(m_q1)\n",
    "m_q2_tf = tft.transform(m_q2)\n",
    "\n",
    "v_score = (m_q1_tf - m_q2_tf)\n",
    "v_score = np.sqrt(np.array(v_score.multiply(v_score).sum(axis=1))[:, 0])\n",
    "\n",
    "df['1wl_tf_l2_euclidean'] = v_score\n",
    "plot_real_feature('1wl_tf_l2_euclidean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tft = TfidfTransformer(\n",
    "    norm='l2', \n",
    "    use_idf=True, \n",
    "    smooth_idf=True, \n",
    "    sublinear_tf=False)\n",
    "\n",
    "tft = tft.fit(sparse.vstack((m_q1, m_q2)))\n",
    "m_q1_tf = tft.transform(m_q1)\n",
    "m_q2_tf = tft.transform(m_q2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.isfile('./../tmp/20_model.pkl'):\n",
    "    model = check_model(None, data={\n",
    "        'X': sparse.csc_matrix(sparse.hstack((m_q1_tf, m_q2_tf)))[ix_train, :],\n",
    "        'y': df.loc[ix_train]['is_duplicate']\n",
    "    }, do_scaling=False)\n",
    "    print model.best_score_\n",
    "    print model.best_params_\n",
    "    with open('./../tmp/20_model.pkl', 'wb') as f:\n",
    "        pickle.dump(model, f)\n",
    "else:\n",
    "    with open('./../tmp/20_model.pkl', 'rb') as f:\n",
    "        model = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "```python\n",
    "Fitting 10 folds for each of 90 candidates, totalling 900 fits\n",
    "[Parallel(n_jobs=-1)]: Done  18 tasks      | elapsed:  1.4min\n",
    "[Parallel(n_jobs=-1)]: Done 168 tasks      | elapsed:  8.2min\n",
    "[Parallel(n_jobs=-1)]: Done 418 tasks      | elapsed: 19.2min\n",
    "[Parallel(n_jobs=-1)]: Done 768 tasks      | elapsed: 34.4min\n",
    "[Parallel(n_jobs=-1)]: Done 900 out of 900 | elapsed: 39.9min finished\n",
    "-0.365063639733\n",
    "{'en__l1_ratio': 0.0001, 'en__alpha': 1e-05}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred = model.predict_proba(sparse.csc_matrix(sparse.hstack((m_q1_tf, m_q2_tf)))[ix_test, :])[:, 1]\n",
    "y_test_pred_fixed = link_function(y_test_pred)\n",
    "\n",
    "if not os.path.isfile('./../submits/20_pred_fixed.csv'):\n",
    "    pd.DataFrame.from_records(\n",
    "        zip(df.loc[ix_test]['test_id'].values, \n",
    "            y_test_pred_fixed), \n",
    "        columns=['test_id', 'is_duplicate']).to_csv('./../submits/20_pred_fixed.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Old result: **0.3316**\n",
    "- Suddenly a new result: **0.30964**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data={\n",
    "    'X_train': sparse.csc_matrix(sparse.hstack((m_q1_tf, m_q2_tf)))[ix_train, :],\n",
    "    'y_train': df.loc[ix_train]['is_duplicate'],\n",
    "    'X_test': sparse.csc_matrix(sparse.hstack((m_q1_tf, m_q2_tf)))[ix_test, :],\n",
    "    'y_train_pred': np.zeros(ix_train.shape[0]),\n",
    "    'y_test_pred': []\n",
    "}\n",
    "del(m_q1_tf, m_q2_tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.isfile('./../tmp/21_model_pred.pkl'):\n",
    "    n_splits = 10\n",
    "    folder = StratifiedKFold(n_splits=n_splits, shuffle=True)\n",
    "    for ix_first, ix_second in tqdm_notebook(folder.split(np.zeros(data['y_train'].shape[0]), data['y_train']), \n",
    "                                             total=n_splits):\n",
    "        # {'en__l1_ratio': 0.0001, 'en__alpha': 1e-05}\n",
    "        model = SGDClassifier(\n",
    "            loss='log', \n",
    "            penalty='elasticnet', \n",
    "            fit_intercept=True, \n",
    "            n_iter=100, \n",
    "            shuffle=True, \n",
    "            n_jobs=-1,\n",
    "            l1_ratio=0.0001,\n",
    "            alpha=1e-05,\n",
    "            class_weight=None)\n",
    "        model = model.fit(data['X_train'][ix_first, :], data['y_train'][ix_first])\n",
    "        data['y_train_pred'][ix_second] = model.predict_proba(data['X_train'][ix_second, :])[:, 1]\n",
    "        data['y_test_pred'].append(model.predict_proba(data['X_test'])[:, 1])\n",
    "        \n",
    "    data['y_test_pred'] = np.array(data['y_test_pred']).T.mean(axis=1)\n",
    "    with open('./../tmp/21_model_pred.pkl', 'wb') as f:\n",
    "        pickle.dump({\n",
    "            'model': model,\n",
    "            'y_train_pred': data['y_train_pred'],\n",
    "            'y_test_pred': data['y_test_pred']\n",
    "        }, f)\n",
    "else:\n",
    "    with open('./../tmp/21_model_pred.pkl', 'rb') as f:\n",
    "        tmp = pickle.load(f)\n",
    "        model = tmp['model']\n",
    "        data['y_train_pred'] = tmp['y_train_pred']\n",
    "        data['y_test_pred'] = tmp['y_test_pred']\n",
    "        del(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax1 = plt.subplot2grid((1, 2), (0, 0))\n",
    "ax2 = plt.subplot2grid((1, 2), (0, 1))\n",
    "\n",
    "sns.distplot(data['y_train_pred'], label='train', ax=ax1)\n",
    "sns.distplot(data['y_test_pred'], label='test', ax=ax1)\n",
    "ax1.axvline(np.mean(data['y_train_pred']), color='b', alpha=1, linestyle='--', label='mean(train)')\n",
    "ax1.axvline(np.mean(data['y_test_pred']), color='g', alpha=1, linestyle='--', label='mean(test)')\n",
    "ax1.legend(loc='upper right', prop={'size': 18})\n",
    "ax1.set_title('Train/test OOF predictions')\n",
    "\n",
    "sns.distplot(data['y_train_pred'][:ix_train.shape[0]/2], label='train first part', ax=ax2)\n",
    "sns.distplot(data['y_train_pred'][ix_train.shape[0]/2:], label='train second part', ax=ax2)\n",
    "ax2.legend(loc='upper right', prop={'size': 18})\n",
    "ax2.set_title('Train/train OOF predictions')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mp = np.mean(data['y_train_pred'])\n",
    "print mp\n",
    "\n",
    "def func(w):\n",
    "    return (mp*data['y_test_pred'].shape[0] - \n",
    "            np.sum(w[0]*data['y_test_pred']/(w[0]*data['y_test_pred'] + \n",
    "                                             (1 - w[0]) * (1 - data['y_test_pred']))))**2\n",
    "\n",
    "print func(np.array([1]))\n",
    "\n",
    "res = minimize(func, np.array([1]), method='L-BFGS-B', bounds=[(0, 1)])\n",
    "\n",
    "print res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = res['x'][0]\n",
    "\n",
    "def fix_function(x):\n",
    "    return w*x/(w*x + (1 - w)*(1 - x))\n",
    "\n",
    "support = np.linspace(0, 1, 1000)\n",
    "values = fix_function(support)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(support, values)\n",
    "ax.set_title('Fix transformation', fontsize=20)\n",
    "ax.set_xlabel('x')\n",
    "ax.set_ylabel('f(x)')\n",
    "plt.show()\n",
    "\n",
    "data['y_test_pred_fixed'] = fix_function(data['y_test_pred'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax1 = plt.subplot2grid((1, 2), (0, 0))\n",
    "ax2 = plt.subplot2grid((1, 2), (0, 1))\n",
    "\n",
    "sns.distplot(data['y_train_pred'], label='train', ax=ax1)\n",
    "sns.distplot(data['y_test_pred_fixed'], label='test', ax=ax1)\n",
    "ax1.axvline(np.mean(data['y_train_pred']), color='b', alpha=1, linestyle='--', label='mean(train)')\n",
    "ax1.axvline(np.mean(data['y_test_pred_fixed']), color='g', alpha=1, linestyle='--', label='mean(test)')\n",
    "ax1.legend(loc='upper right', prop={'size': 18})\n",
    "ax1.set_title('Train/test OOF predictions')\n",
    "\n",
    "sns.distplot(data['y_train_pred'][:ix_train.shape[0]/2], label='train first part', ax=ax2)\n",
    "sns.distplot(data['y_train_pred'][ix_train.shape[0]/2:], label='train second part', ax=ax2)\n",
    "ax2.legend(loc='upper right', prop={'size': 18})\n",
    "ax2.set_title('Train/train OOF predictions')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['m_w1l_tfidf_oof'] = np.zeros(df.shape[0])\n",
    "df.loc[ix_train, 'm_w1l_tfidf_oof'] = data['y_train_pred']\n",
    "df.loc[ix_test, 'm_w1l_tfidf_oof'] = data['y_test_pred_fixed']\n",
    "del(data)\n",
    "plot_real_feature('m_w1l_tfidf_oof')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictors = df.columns[7:]\n",
    "print predictors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class QuoraKFold():\n",
    "    \n",
    "    def __init__(self, n_splits=10, r_split=0.7):\n",
    "        self.n_splits = n_splits\n",
    "        self.r_split = r_split\n",
    "        \n",
    "    def get_n_splits(self, X=None, y=None, groups=None):\n",
    "        return self.n_splits\n",
    "        \n",
    "    def split(self, X, y, groups=None):\n",
    "        if self.r_split is None:\n",
    "            self.r_split = 1 - 1.0/self.n_splits\n",
    "        k = r1*d[0]/(r0*d[1])\n",
    "        ix_ones = np.where(y == 1)[0]\n",
    "        ix_zeros = np.where(y == 0)[0]\n",
    "        for i in range(self.n_splits):\n",
    "            ix_first_zeros = np.random.choice(\n",
    "                ix_zeros, \n",
    "                size=int(self.r_split * ix_zeros.shape[0]), \n",
    "                replace=False)\n",
    "            ix_first_ones = np.random.choice(\n",
    "                ix_ones,\n",
    "                size=int(ix_first_zeros.shape[0] * d[1]/d[0]),\n",
    "                replace=False)\n",
    "            \n",
    "            ix_second_zeros = np.setdiff1d(ix_zeros, ix_first_zeros)\n",
    "            ix_second_ones = np.random.choice(\n",
    "                np.setdiff1d(ix_ones, ix_first_ones),\n",
    "                size=int(ix_second_zeros.shape[0] * r1/r0),\n",
    "                replace=False)\n",
    "            \n",
    "            ix_first = np.hstack((ix_first_zeros, ix_first_ones))\n",
    "            ix_first = ix_first[np.random.choice(\n",
    "                range(ix_first.shape[0]), size=ix_first.shape[0], replace=False)]\n",
    "            \n",
    "            ix_second = np.hstack((ix_second_zeros, ix_second_ones))\n",
    "            ix_second = ix_second[np.random.choice(\n",
    "                range(ix_second.shape[0]), size=ix_second.shape[0], replace=False)]\n",
    "            \n",
    "            yield ix_first, ix_second\n",
    "\n",
    "\n",
    "def check_model(predictors, data=None, do_scaling=True, folder=None, r_split=0.7):\n",
    "    classifier = lambda: SGDClassifier(\n",
    "        loss='log', \n",
    "        penalty='elasticnet', \n",
    "        fit_intercept=True, \n",
    "        n_iter=100, \n",
    "        shuffle=True, \n",
    "        n_jobs=-1,\n",
    "        class_weight=None)\n",
    "\n",
    "    steps = []\n",
    "    if do_scaling:\n",
    "        steps.append(('ss', StandardScaler()))\n",
    "    steps.append(('en', classifier()))\n",
    "    \n",
    "    model = Pipeline(steps=steps)\n",
    "\n",
    "    parameters = {\n",
    "        'en__alpha': [0.00001, 0.0001, 0.001, 0.01, 0.02, 0.1, 0.5, 0.9, 1],\n",
    "        'en__l1_ratio': [0, 0.0001, 0.001, 0.01, 0.1, 0.3, 0.5, 0.75, 0.9, 1]\n",
    "    }\n",
    "\n",
    "    if folder is None:\n",
    "        folder = QuoraKFold(n_splits=10, r_split=r_split)\n",
    "    \n",
    "    grid_search = GridSearchCV(\n",
    "        model, \n",
    "        parameters, \n",
    "        cv=folder, \n",
    "        n_jobs=-1,\n",
    "        scoring=make_scorer(log_loss_lf, greater_is_better=False, needs_proba=True),\n",
    "        verbose=1)\n",
    "    if data is None:\n",
    "        grid_search = grid_search.fit(df.loc[ix_train][predictors], \n",
    "                                      df.loc[ix_train]['is_duplicate'])\n",
    "    else:\n",
    "        grid_search = grid_search.fit(data['X'], \n",
    "                                      data['y'])\n",
    "    \n",
    "    return grid_search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.isfile('./../tmp/22_model.pkl'):\n",
    "    model = check_model(None, data={\n",
    "        'X': df.loc[ix_train][predictors],\n",
    "        'y': df.loc[ix_train]['is_duplicate']\n",
    "    }, do_scaling=True, r_split=0.7)\n",
    "    print model.best_score_\n",
    "    print model.best_params_\n",
    "    with open('./../tmp/22_model.pkl', 'wb') as f:\n",
    "        pickle.dump(model, f)\n",
    "else:\n",
    "    with open('./../tmp/22_model.pkl', 'rb') as f:\n",
    "        model = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "Fitting 10 folds for each of 90 candidates, totalling 900 fits\n",
    "[Parallel(n_jobs=-1)]: Done  18 tasks      | elapsed:   43.1s\n",
    "[Parallel(n_jobs=-1)]: Done 168 tasks      | elapsed:  4.6min\n",
    "[Parallel(n_jobs=-1)]: Done 418 tasks      | elapsed: 11.0min\n",
    "[Parallel(n_jobs=-1)]: Done 768 tasks      | elapsed: 20.4min\n",
    "[Parallel(n_jobs=-1)]: Done 900 out of 900 | elapsed: 23.9min finished\n",
    "-0.307676722634\n",
    "{'en__l1_ratio': 1, 'en__alpha': 0.0001}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(\n",
    "    eli5.sklearn.explain_linear_classifier_weights(\n",
    "        model.best_estimator_.steps[1][1],\n",
    "        feature_names=predictors.tolist(),\n",
    "        top=len(predictors)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred = model.predict_proba(df.loc[ix_test, predictors])[:, 1]\n",
    "y_test_pred_fixed = link_function(y_test_pred)\n",
    "\n",
    "if not os.path.isfile('./../submits/22_pred_fixed.csv'):\n",
    "    pd.DataFrame.from_records(\n",
    "        zip(df.loc[ix_test]['test_id'].values, \n",
    "            y_test_pred_fixed), \n",
    "        columns=['test_id', 'is_duplicate']).to_csv('./../submits/22_pred_fixed.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax1 = plt.subplot2grid((1, 2), (0, 0))\n",
    "ax2 = plt.subplot2grid((1, 2), (0, 1))\n",
    "sns.distplot(y_test_pred, ax=ax1)\n",
    "ax1.set_title('Model prediction')\n",
    "sns.distplot(y_test_pred_fixed, ax=ax2)\n",
    "ax2.set_title('Model prediction fixed')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Suddenly worse: **0.32406**\n",
    "- If you do not correct the average: **0.40287**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del(m_q1, m_q2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Black magic\n",
    "\n",
    "<img src=\"./../images/buben7.jpg\" width=\"70%\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictors = df.columns[7:]\n",
    "print predictors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "xgb_models = []\n",
    "if not os.path.isfile('./../submits/25_pred_fixed.csv'):\n",
    "    def log_loss_lf_xgb(y_pred, y_true):\n",
    "        return 'llf', log_loss(y_true.get_label(), link_function(y_pred), eps=eps)\n",
    "\n",
    "    xgb_params = {\n",
    "        'max_depth': 9, \n",
    "        'learning_rate': 0.1,\n",
    "        'n_estimators': 2500, \n",
    "        'objective': 'binary:logistic',\n",
    "        'nthread': 16, \n",
    "        'gamma': 0, \n",
    "        'subsample': 0.75, \n",
    "        'colsample_bytree': 0.75, \n",
    "        'colsample_bylevel': 1,\n",
    "        'reg_alpha': 0, \n",
    "        'reg_lambda': 1, \n",
    "        'scale_pos_weight': 1\n",
    "    }\n",
    "\n",
    "    y_test = []\n",
    "    folder = QuoraKFold(n_splits=10, r_split=0.7)\n",
    "    splits = folder.split(np.zeros(ix_train.shape[0]), df.loc[ix_train, 'is_duplicate'])\n",
    "    for ix_first, ix_second in tqdm_notebook(splits, total=10):\n",
    "\n",
    "        model = xgb.XGBClassifier(silent=True).set_params(**xgb_params)\n",
    "        model = model.fit(df.loc[ix_first, predictors], df.loc[ix_first, 'is_duplicate'], \n",
    "                          eval_set=[(df.loc[ix_second, predictors], df.loc[ix_second, 'is_duplicate'])], \n",
    "                          eval_metric=log_loss_lf_xgb,\n",
    "                          early_stopping_rounds=100, \n",
    "                          verbose=False)\n",
    "        y_test.append(model.predict_proba(df.loc[ix_test, predictors])[:, 1])\n",
    "        xgb_models.append(model)\n",
    "\n",
    "    y_test_pred = np.array(y_test).T.mean(axis=1)\n",
    "    y_test_pred_fixed = link_function(y_test_pred)\n",
    "    \n",
    "    pd.DataFrame.from_records(\n",
    "        zip(df.loc[ix_test]['test_id'].values, \n",
    "            y_test_pred_fixed), \n",
    "        columns=['test_id', 'is_duplicate']).to_csv('./../submits/25_pred_fixed.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if not os.path.isfile('./../tmp/bm1.pkl'):\n",
    "    with open('./../tmp/bm1.pkl', 'wb') as f:\n",
    "        pickle.dump(xgb_models, f)\n",
    "else:\n",
    "    with open('./../tmp/bm1.pkl', 'rb') as f:\n",
    "        xgb_models = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = xgb_models[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(\n",
    "    eli5.explain_weights_xgboost(\n",
    "        model, \n",
    "        feature_names=predictors.tolist(), \n",
    "        top=len(predictors)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb.plot_importance(model)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Some black magic gives: **0.29936**\n",
    "- At the moment of submission it is **244/1880**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
