{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gensim\n",
    "import re\n",
    "import nltk\n",
    "import json\n",
    "import sys\n",
    "import datetime\n",
    "import operator\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import csv\n",
    "import timeit\n",
    "\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize, ngrams\n",
    "from gensim import corpora, models, similarities\n",
    "from gensim.models.doc2vec import Doc2Vec\n",
    "from gensim.models import doc2vec\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.neighbors import NearestNeighbors, NearestCentroid, LSHForest\n",
    "from pylab import plot, show, subplot, specgram, imshow, savefig\n",
    "from tqdm import tqdm\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cosine(v1, v2):\n",
    "    v1 = np.array(v1)\n",
    "    v2 = np.array(v2)\n",
    "    return np.dot(v1, v2) / (np.sqrt(np.sum(v1**2)) * np.sqrt(np.sum(v2**2)))\n",
    "\n",
    "def concatenate(data):\n",
    "    X_set1 = data['question1']\n",
    "    X_set2 = data['question2']\n",
    "    X = X_set1.append(X_set2, ignore_index=True)\n",
    "    return X\n",
    "\n",
    "class LabeledLineSentence(object):\n",
    "    def __init__(self, doc_list, labels_list):\n",
    "        self.labels_list = labels_list\n",
    "        self.doc_list = doc_list\n",
    "    def __iter__(self):\n",
    "        for idx, doc in enumerate(self.doc_list):\n",
    "            yield doc2vec.TaggedDocument(words=word_tokenize(doc),\n",
    "                                         tags=[self.labels_list[idx]])\n",
    "            \n",
    "def get_dists_doc2vec(data):\n",
    "    docvec1s = []\n",
    "    docvec2s = []\n",
    "    for i in tqdm(range(data.shape[0])):\n",
    "        doc1 = word_tokenize(data.iloc[i, -2])\n",
    "        doc2 = word_tokenize(data.iloc[i, -1])\n",
    "        docvec1 = model.infer_vector(doc1, alpha=start_alpha, steps=infer_epoch)\n",
    "        docvec2 = model.infer_vector(doc2, alpha=start_alpha, steps=infer_epoch)\n",
    "        docvec1s.append(docvec1)\n",
    "        docvec2s.append(docvec2)\n",
    "    return docvec1s, docvec2s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "src_train = 'df_train_spacylemmat_fullclean.csv'\n",
    "src_test = 'df_test_spacylemmat_fullclean.csv'\n",
    "\n",
    "df_train = pd.read_csv(src_train)\n",
    "df_test = pd.read_csv(src_test)\n",
    "df_train = df_train[['id', 'question1', 'question2']]\n",
    "df_test = df_test[['test_id', 'question1', 'question2']]\n",
    "\n",
    "df_train.fillna('NULL', inplace =  True)\n",
    "df_test.fillna('NULL', inplace = True)\n",
    "\n",
    "\n",
    "df_test.rename(columns = {'test_id': 'id'}, inplace = True)\n",
    "data = pd.concat([df_train, df_test], ignore_index = True)\n",
    "X_train = data[['id', 'question1', 'question2']]\n",
    "X = concatenate(X_train)\n",
    "\n",
    "labels = []\n",
    "for label in X_train['id'].tolist():\n",
    "    labels.append('SENT_%s_1' % label)\n",
    "for label in X_train['id'].tolist():\n",
    "    labels.append('SENT_%s_2' % label)\n",
    "\n",
    "docs = LabeledLineSentence(X.tolist(), labels)\n",
    "it = docs.__iter__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Doc2Vec(size=100, window=10, min_count=2, sample=1e-5, workers=8, iter = 20)\n",
    "model.build_vocab(docs)\n",
    "print('Model built.')\n",
    "model.train(docs, total_examples=model.corpus_count, epochs=model.iter)\n",
    "print('Model trained.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_alpha = 0.01\n",
    "infer_epoch = 10\n",
    "\n",
    "results = get_dists_doc2vec(data)\n",
    "docvec1s, docvec2s = results[0], results[1]\n",
    "\n",
    "docvec1s = np.array(docvec1s)\n",
    "docvec1s_tr = docvec1s[:df_train.shape[0]]\n",
    "docvec1s_te = docvec1s[df_train.shape[0]:]\n",
    "\n",
    "docvec2s = np.array(docvec2s)\n",
    "docvec2s_tr = docvec2s[:df_train.shape[0]]\n",
    "docvec2s_te = docvec2s[df_train.shape[0]:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.save('train_q1_doc2vec_vectors_trainquora', docvec1s_tr)\n",
    "np.save('test_q1_doc2vec_vectors_trainquora', docvec1s_te)\n",
    "\n",
    "np.save('train_q2_doc2vec_vectors_trainquora', docvec2s_tr)\n",
    "np.save('test_q2_doc2vec_vectors_trainquora', docvec2s_te)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
