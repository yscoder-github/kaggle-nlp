{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import spacy\n",
    "import ujson as json\n",
    "import numpy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import en_core_web_md\n",
    "from tqdm import tqdm\n",
    "\n",
    "import numpy\n",
    "import numpy.random\n",
    "import json\n",
    "from spacy.tokens.span import Span"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_word_ids(docs, rnn_encode=False, tree_truncate=False, max_length=100, nr_unk=100):\n",
    "    Xs = numpy.zeros((len(docs), max_length), dtype='int32')\n",
    "    for i, doc in enumerate(docs):\n",
    "        if tree_truncate:\n",
    "            if isinstance(doc, Span):\n",
    "                queue = [doc.root]\n",
    "            else:\n",
    "                queue = [sent.root for sent in doc.sents]\n",
    "        else:\n",
    "            queue = list(doc)\n",
    "        words = []\n",
    "        while len(words) <= max_length and queue:\n",
    "            word = queue.pop(0)\n",
    "            if rnn_encode or (not word.is_punct and not word.is_space):\n",
    "                words.append(word)\n",
    "            if tree_truncate:\n",
    "                queue.extend(list(word.lefts))\n",
    "                queue.extend(list(word.rights))\n",
    "        words.sort()\n",
    "        for j, token in enumerate(words):\n",
    "            if token.has_vector:\n",
    "                Xs[i, j] = token.rank+1\n",
    "            else:\n",
    "                Xs[i, j] = (token.shape % (nr_unk-1))+2\n",
    "            j += 1\n",
    "            if j >= max_length:\n",
    "                break\n",
    "        else:\n",
    "            Xs[i, len(words)] = 1\n",
    "    return Xs\n",
    "\n",
    "def spacy_encode(df, df_lens, settings, savename, save = True):\n",
    "    print('Encoding data according to following settings:', settings, '\\n')\n",
    "    train_texts1, train_texts2 = df['question1'], df['question2']\n",
    "    print(\"Loading spaCy\")\n",
    "    nlp = en_core_web_md.load()\n",
    "    assert nlp.path is not None\n",
    "    print(\"Processing texts...\")\n",
    "    encoded_data = []\n",
    "    for texts in tqdm((train_texts1, train_texts2)):\n",
    "        encoded_data.append(get_word_ids(list(nlp.pipe(texts, n_threads=6, batch_size=5000)),\n",
    "                         max_length=settings['sentence_length'],\n",
    "                         rnn_encode=settings['gru_encode'],\n",
    "                         tree_truncate=settings['tree_truncate']))\n",
    "    q1, q2 = encoded_data\n",
    "    if save:\n",
    "        q1_tr = q1[:df_lens[0]]\n",
    "        q1_te = q1[df_lens[0]:]\n",
    "        q2_tr = q2[:df_lens[0]]\n",
    "        q2_te = q2[df_lens[0]:]\n",
    "        np.save('q1train_{}'.format(savename), q1_tr)\n",
    "        np.save('q2train_{}'.format(savename), q2_tr)\n",
    "        np.save('q1test_{}'.format(savename), q1_te)\n",
    "        np.save('q2test_{}'.format(savename), q2_te)\n",
    "        return\n",
    "    else:\n",
    "        return q1, q2\n",
    "    \n",
    "def load_quora_data(src_train, src_test):\n",
    "    df_train = pd.read_csv(src_train)\n",
    "    df_test = pd.read_csv(src_test)\n",
    "    df_train.fillna('NULL', inplace = True)\n",
    "    df_test.fillna('NULL', inplace = True)\n",
    "    df = pd.concat([df_train, df_test])\n",
    "    df_lens = (df_train.shape[0], df_test.shape[0])\n",
    "    return df, df_lens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "src_train = 'df_train_lemmatfullcleanSTEMMED.csv'\n",
    "src_test = 'df_test_lemmatfullcleanSTEMMED.csv'\n",
    "\n",
    "settings = {\n",
    "    'tree_truncate': False,\n",
    "    'gru_encode': False,\n",
    "    'sentence_length': 48,\n",
    "    }\n",
    "\n",
    "df, lengths = load_quora_data(src_train, src_test)\n",
    "df.question1 = df.question1.apply(lambda x: ' '.join(x))\n",
    "spacy_encode(df, lengths, settings, 'lemmatfullcleanSTEMMED_48len')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
