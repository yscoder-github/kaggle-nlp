{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import spacy\n",
    "import re\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "nlp = spacy.load('en_core_web_md')\n",
    "stopwords = set(stopwords.words('english'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clean_string(text):\n",
    "    \n",
    "    def pad_str(s):\n",
    "        return ' '+s+' '\n",
    "    \n",
    "    # Empty question\n",
    "    \n",
    "    if type(text) != str or text=='':\n",
    "        return ''\n",
    "    \n",
    "    # preventing first and last word being ignored by regex\n",
    "    # and convert first word in question to lower case\n",
    "    \n",
    "    text = ' ' + text[0].lower() + text[1:] + ' '\n",
    "    \n",
    "    # replace all first char after either [.!?)\"'] with lowercase\n",
    "    # don't mind if we lowered a proper noun, it won't be a big problem\n",
    "    \n",
    "    def lower_first_char(pattern):\n",
    "        matched_string = pattern.group(0)\n",
    "        return matched_string[:-1] + matched_string[-1].lower()\n",
    "    \n",
    "    text = re.sub(\"(?<=[\\.\\?\\)\\!\\'\\\"])[\\s]*.\",lower_first_char , text)\n",
    "    \n",
    "    # Replace weird chars in text\n",
    "    \n",
    "    text = re.sub(\"’\", \"'\", text) # special single quote\n",
    "    text = re.sub(\"`\", \"'\", text) # special single quote\n",
    "    text = re.sub(\"“\", '\"', text) # special double quote\n",
    "    text = re.sub(\"？\", \"?\", text) \n",
    "    text = re.sub(\"…\", \" \", text) \n",
    "    text = re.sub(\"é\", \"e\", text) \n",
    "    \n",
    "    # Clean shorthands\n",
    "    \n",
    "    text = re.sub(\"\\'s\", \" \", text) # we have cases like \"Sam is\" or \"Sam's\" (i.e. his) these two cases aren't separable, I choose to compromise are kill \"'s\" directly\n",
    "    text = re.sub(\" whats \", \" what is \", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(\"\\'ve\", \" have \", text)\n",
    "    text = re.sub(\"can't\", \"can not\", text)\n",
    "    text = re.sub(\"n't\", \" not \", text)\n",
    "    text = re.sub(\"i'm\", \"i am\", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(\"\\'re\", \" are \", text)\n",
    "    text = re.sub(\"\\'d\", \" would \", text)\n",
    "    text = re.sub(\"\\'ll\", \" will \", text)\n",
    "    text = re.sub(\"e\\.g\\.\", \" eg \", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(\"b\\.g\\.\", \" bg \", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(\"(\\d+)(kK)\", \" \\g<1>000 \", text)\n",
    "    text = re.sub(\"e-mail\", \" email \", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(\"(the[\\s]+|The[\\s]+)?U\\.S\\.A\\.\", \" America \", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(\"(the[\\s]+|The[\\s]+)?United State(s)?\", \" America \", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(\"\\(s\\)\", \" \", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(\"[c-fC-F]\\:\\/\", \" disk \", text)\n",
    "    \n",
    "    # replace the float numbers with a random number, it will be parsed as number afterward, and also been replaced with word \"number\"\n",
    "    \n",
    "    text = re.sub('[0-9]+\\.[0-9]+', \" 87 \", text)\n",
    "    \n",
    "    # remove comma between numbers, i.e. 15,000 -> 15000\n",
    "    \n",
    "    text = re.sub('(?<=[0-9])\\,(?=[0-9])', \"\", text)\n",
    "    \n",
    "#     # all numbers should separate from words, this is too aggressive\n",
    "    \n",
    "#     def pad_number(pattern):\n",
    "#         matched_string = pattern.group(0)\n",
    "#         return pad_str(matched_string)\n",
    "#     text = re.sub('[0-9]+', pad_number, text)\n",
    "    \n",
    "    # add padding to punctuations and special chars, we still need them later\n",
    "    \n",
    "    text = re.sub('\\$', \" dollar \", text)\n",
    "    text = re.sub('\\%', \" percent \", text)\n",
    "    text = re.sub('\\&', \" and \", text)\n",
    "    \n",
    "    def pad_pattern(pattern):\n",
    "        matched_string = pattern.group(0)\n",
    "        return pad_str(matched_string)\n",
    "    text = re.sub('[\\!\\?\\@\\^\\+\\*\\/\\,\\~\\|\\`\\=\\:\\;\\.\\#\\\\\\]', pad_pattern, text) \n",
    "        \n",
    "    text = re.sub('[^\\x00-\\x7F]+', pad_str(SPECIAL_TOKENS['non-ascii']), text) # replace non-ascii word with special word\n",
    "    \n",
    "    # indian dollar\n",
    "    \n",
    "    text = re.sub(\"(?<=[0-9])rs \", \" rs \", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(\" rs(?=[0-9])\", \" rs \", text, flags=re.IGNORECASE)\n",
    "    \n",
    "    # clean text rules get from : https://www.kaggle.com/currie32/the-importance-of-cleaning-text\n",
    "    \n",
    "    text = re.sub(r\" (the[\\s]+|The[\\s]+)?US(A)? \", \" America \", text)\n",
    "    text = re.sub(r\" UK \", \" England \", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r\" india \", \" India \", text)\n",
    "    text = re.sub(r\" switzerland \", \" Switzerland \", text)\n",
    "    text = re.sub(r\" china \", \" China \", text)\n",
    "    text = re.sub(r\" chinese \", \" Chinese \", text) \n",
    "    text = re.sub(r\" imrovement \", \" improvement \", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r\" intially \", \" initially \", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r\" quora \", \" Quora \", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r\" dms \", \" direct messages \", text, flags=re.IGNORECASE)  \n",
    "    text = re.sub(r\" demonitization \", \" demonetization \", text, flags=re.IGNORECASE) \n",
    "    text = re.sub(r\" actived \", \" active \", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r\" kms \", \" kilometers \", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r\" cs \", \" computer science \", text, flags=re.IGNORECASE) \n",
    "    text = re.sub(r\" upvote\", \" up vote\", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r\" iPhone \", \" phone \", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r\" \\0rs \", \" rs \", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r\" calender \", \" calendar \", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r\" ios \", \" operating system \", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r\" gps \", \" GPS \", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r\" gst \", \" GST \", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r\" programing \", \" programming \", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r\" bestfriend \", \" best friend \", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r\" dna \", \" DNA \", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r\" III \", \" 3 \", text)\n",
    "    text = re.sub(r\" banglore \", \" Banglore \", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r\" J K \", \" JK \", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r\" J\\.K\\. \", \" JK \", text, flags=re.IGNORECASE)\n",
    "    \n",
    "    # typos identified with my eyes\n",
    "    \n",
    "    text = re.sub(r\" quikly \", \" quickly \", text)\n",
    "    text = re.sub(r\" unseccessful \", \" unsuccessful \", text)\n",
    "    text = re.sub(r\" demoniti[\\S]+ \", \" demonetization \", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r\" demoneti[\\S]+ \", \" demonetization \", text, flags=re.IGNORECASE)  \n",
    "    text = re.sub(r\" addmision \", \" admission \", text)\n",
    "    text = re.sub(r\" insititute \", \" institute \", text)\n",
    "    text = re.sub(r\" connectionn \", \" connection \", text)\n",
    "    text = re.sub(r\" permantley \", \" permanently \", text)\n",
    "    text = re.sub(r\" sylabus \", \" syllabus \", text)\n",
    "    text = re.sub(r\" sequrity \", \" security \", text)\n",
    "    text = re.sub(r\" undergraduation \", \" undergraduate \", text) # not typo, but GloVe can't find it\n",
    "    text = re.sub(r\"(?=[a-zA-Z])ig \", \"ing \", text)\n",
    "    text = re.sub(r\" latop\", \" laptop\", text)\n",
    "    text = re.sub(r\" programmning \", \" programming \", text)  \n",
    "    text = re.sub(r\" begineer \", \" beginner \", text)  \n",
    "    text = re.sub(r\" qoura \", \" Quora \", text)\n",
    "    text = re.sub(r\" wtiter \", \" writer \", text)  \n",
    "    text = re.sub(r\" litrate \", \" literate \", text)  \n",
    "\n",
    "      \n",
    "    # for words like A-B-C-D or \"A B C D\", \n",
    "    # if A,B,C,D individuaally has vector in glove:\n",
    "    #     it can be treat as separate words\n",
    "    # else:\n",
    "    #     replace it as a special word, A_B_C_D is enough, we'll deal with that word later\n",
    "    #\n",
    "    # Testcase: 'a 3-year-old 4 -tier car'\n",
    "    \n",
    "    def dash_dealer(pattern):\n",
    "        matched_string = pattern.group(0)\n",
    "        splited = matched_string.split('-')\n",
    "        splited = [sp.strip() for sp in splited if sp!=' ' and sp!='']\n",
    "        joined = ' '.join(splited)\n",
    "        parsed = nlp(joined)\n",
    "        for token in parsed:\n",
    "            # if one of the token is not common word, then join the word into one single word\n",
    "            if not token.has_vector or token.text in SPECIAL_TOKENS.values():\n",
    "                return '_'.join(splited)\n",
    "        # if all tokens are common words, then split them\n",
    "        return joined\n",
    "\n",
    "    text = re.sub(\"[a-zA-Z0-9\\-]*-[a-zA-Z0-9\\-]*\", dash_dealer, text)\n",
    "    \n",
    "    # try to see if sentence between quotes is meaningful\n",
    "    # rule:\n",
    "    #     if exist at least one word is \"not number\" and \"length longer than 2\" and \"it can be identified by SpaCy\":\n",
    "    #         then consider the string is meaningful\n",
    "    #     else:\n",
    "    #         replace the string with a special word, i.e. quoted_item\n",
    "    # Testcase:\n",
    "    # i am a good (programmer)      -> i am a good programmer\n",
    "    # i am a good (programmererer)  -> i am a good quoted_item\n",
    "    # i am \"i am a\"                 -> i am quoted_item\n",
    "    # i am \"i am a programmer\"      -> i am i am a programmer\n",
    "    # i am \"i am a programmererer\"  -> i am quoted_item\n",
    "    \n",
    "    def quoted_string_parser(pattern):\n",
    "        string = pattern.group(0)\n",
    "        parsed = nlp(string[1:-1])\n",
    "        is_meaningful = False\n",
    "        for token in parsed:\n",
    "            # if one of the token is meaningful, we'll consider the full string is meaningful\n",
    "            if len(token.text)>2 and not token.text.isdigit() and token.has_vector:\n",
    "                is_meaningful = True\n",
    "            elif token.text in SPECIAL_TOKENS.values():\n",
    "                is_meaningful = True\n",
    "            \n",
    "        if is_meaningful:\n",
    "            return string\n",
    "        else:\n",
    "            return pad_str(string[0]) + SPECIAL_TOKENS['quoted'] + pad_str(string[-1])\n",
    "\n",
    "    text = re.sub('\\\".*\\\"', quoted_string_parser, text)\n",
    "    text = re.sub(\"\\'.*\\'\", quoted_string_parser, text)\n",
    "    text = re.sub(\"\\(.*\\)\", quoted_string_parser, text)\n",
    "    text = re.sub(\"\\[.*\\]\", quoted_string_parser, text)\n",
    "    text = re.sub(\"\\{.*\\}\", quoted_string_parser, text)\n",
    "    text = re.sub(\"\\<.*\\>\", quoted_string_parser, text)\n",
    "\n",
    "    text = re.sub('[\\(\\)\\[\\]\\{\\}\\<\\>\\'\\\"]', pad_pattern, text) \n",
    "    \n",
    "    # the single 's' in this stage is 99% of not clean text, just kill it\n",
    "    text = re.sub(' s ', \" \", text)\n",
    "    \n",
    "    # reduce extra spaces into single spaces\n",
    "    text = re.sub('[\\s]+', \" \", text)\n",
    "    text = text.strip()\n",
    "    \n",
    "    return text\n",
    "\n",
    "# APIs to access entity type\n",
    "def token_type_lookup(token, report_detail=False):\n",
    "    if type(token)==str:\n",
    "        token = nlp(token)[0]\n",
    "    key = token.lower_\n",
    "    try:\n",
    "        if report_detail:\n",
    "            print(ENTITY_ENUM[word_ent_type_dict[key]], ' <= ', {ENTITY_ENUM[ent_t] : vote_dict[key][ent_t] for ent_t in vote_dict[key]} )\n",
    "        return word_ent_type_dict[key]\n",
    "    except KeyError:\n",
    "        return ''\n",
    "\n",
    "def is_token_has_second_type(token):\n",
    "    if type(token)==str:\n",
    "        token = nlp(token)[0]\n",
    "    key = token.lower_\n",
    "    try:\n",
    "        return key in word_ent_type_second_dict\n",
    "    except KeyError:\n",
    "        return False\n",
    "\n",
    "def token_second_type_lookup(token, report_detail=False):\n",
    "    if type(token)==str:\n",
    "        token = nlp(token)[0]\n",
    "    key = token.lower_\n",
    "    try:\n",
    "        if report_detail:\n",
    "            print(ENTITY_ENUM[word_ent_type_second_dict[key]], ' <= ', {ENTITY_ENUM[ent_t] : vote_dict[key][ent_t] for ent_t in vote_dict[key]} )\n",
    "        return word_ent_type_second_dict[key]\n",
    "    except KeyError:\n",
    "        return ''\n",
    "    \n",
    "def process_question_with_spacy(spacy_obj, debug=False, show_fail=False, idx=None):\n",
    "    def not_alpha_or_digit(token):\n",
    "        ch = token.text[0]\n",
    "        return not (ch.isalpha() or ch.isdigit())\n",
    "    result_word_list = []\n",
    "    res = ''\n",
    "    # for continuous entity type string, we need only single term. \n",
    "    # EX: \"2017-01-01\"\n",
    "    # => \"time time time\" (X)\n",
    "    # => \"time\" (O)\n",
    "    previous_ent_type = None\n",
    "    is_a_word_parsed_fail = False\n",
    "    fail_words = []\n",
    "    \n",
    "    for token in spacy_obj:\n",
    "        global_ent_type = token_type_lookup(token)\n",
    "        # problematic token, use its base form\n",
    "        if token.text in exception_list:\n",
    "            previous_ent_type = None\n",
    "            result_word_list.append(token.text)\n",
    "        # special kind of tokens\n",
    "        elif token.text in SPECIAL_TOKENS.values():\n",
    "            # we have no choice but use the entity type detected by spaCy directly, but it still might fail\n",
    "            if token.ent_type_!='':\n",
    "                previous_ent_type = token.ent_type_\n",
    "                result_word_list.append(token.ent_type_)\n",
    "            else:\n",
    "                previous_ent_type = None\n",
    "                result_word_list.append(token.text)\n",
    "        # skip none words tokens\n",
    "        elif not_alpha_or_digit(token) or token.text==' ' or token.text=='s':\n",
    "            previous_ent_type = None\n",
    "            if debug: print(token.text, ' : remove punc or special chars')\n",
    "        # if the \"remove stop word\" flag is set to True\n",
    "        elif is_remove_stopwords and token.lemma_ in is_remove_stopwords:\n",
    "            previous_ent_type = None\n",
    "            if debug: print(token.text, ' : remove stop word')\n",
    "        # contiguous same type, skip it\n",
    "        elif global_ent_type==previous_ent_type or token.ent_type_==previous_ent_type:\n",
    "            if debug: print('contiguous same type')\n",
    "        elif global_ent_type in NUMERIC_TYPES and previous_ent_type in NUMERIC_TYPES:\n",
    "            if debug: print('contiguous numeric')\n",
    "        elif token.ent_type_ in NUMERIC_TYPES and previous_ent_type in NUMERIC_TYPES:\n",
    "            if debug: print('contiguous numeric')\n",
    "        # number without an ent_type_\n",
    "        elif token.text.isdigit():\n",
    "            \n",
    "            if debug: print(token.text, 'force to be number')\n",
    "                \n",
    "            if previous_ent_type in NUMERIC_TYPES:\n",
    "                pass\n",
    "            else:\n",
    "                previous_ent_type = 'CARDINAL' # any number type would be okay\n",
    "                result_word_list.append('number')\n",
    "        # replace proper nouns into name entities. \n",
    "        # EX:\n",
    "        # Original : Taiwan is next to China\n",
    "        # Result   : country is next to country \n",
    "        elif global_ent_type!='':\n",
    "            result_word_list.append(ENTITY_ENUM[global_ent_type])\n",
    "            previous_ent_type = global_ent_type\n",
    "            if debug: print(token.text, ' : sub ent_type:', ENTITY_ENUM[global_ent_type])\n",
    "        # Identify if a word is proper noun or not, if it is a proper noun, we'll try to use second highest rated ent_type_\n",
    "        #\n",
    "        # A proper noun has following special patterns:\n",
    "        #     1. its lemma_ (base form) returned by spaCy is just its lowercase form\n",
    "        #     2. if one of its character except the first character is uppercase, it is a propernoun (in most cases)\n",
    "        # except the special cases like \"I LOVE YOU\", we cal say that if (1.) and (2.), then the token is proper noun\n",
    "        #\n",
    "        # for cases like \"Tensorflow\", we have no good rule to identify it is a proper noun or not ... let's just move on\n",
    "        elif token.lower_==token.lemma_ and token.text[1:]!=token.lemma_[1:] and is_token_has_second_type(token):\n",
    "            second_type = token_second_type_lookup(token)\n",
    "            result_word_list.append(ENTITY_ENUM[second_type])\n",
    "            if debug: print(token.text, ' : use second ent_type:', ENTITY_ENUM[second_type])\n",
    "            previous_ent_type = second_type\n",
    "        # words arrive here are either \"extremely common\" or \"extremely rare and has no method to deal with\"\n",
    "        else:\n",
    "            # A weird behavior of SpaCy, it substitutes [I, my, they] into '-PRON-', which mean pronoun (代名詞)\n",
    "            # More detail in : https://github.com/explosion/spaCy/issues/962\n",
    "            if token.lemma_=='-PRON-':\n",
    "                result_word_list.append(token.lower_)\n",
    "                res = token.lower\n",
    "                previous_ent_type = None\n",
    "            # the lemma can be identified by GloVe\n",
    "            elif nlp(token.lemma_)[0].has_vector:\n",
    "                result_word_list.append(token.lemma_)\n",
    "                res = token.lemma_\n",
    "                previous_ent_type = None\n",
    "            # the lemma cannot be identified, very probably a proper noun\n",
    "            elif is_token_has_second_type(token):\n",
    "                second_type = token_second_type_lookup(token)\n",
    "                result_word_list.append(ENTITY_ENUM[second_type])\n",
    "                res = ENTITY_ENUM[second_type]\n",
    "                previous_ent_type = second_type\n",
    "                if debug: print(token.text, ' : use second ent_type in else :', ENTITY_ENUM[second_type])\n",
    "            # the lemma is not in glove and Spacy can't identify if it is a proper noun, last try, \n",
    "            #      if the word itself can be identified by GloVe or not\n",
    "            elif nlp(token.lower_)[0].has_vector:\n",
    "                result_word_list.append(token.lower_)\n",
    "                res = token.lower_\n",
    "                previous_ent_type = None\n",
    "                if debug: print(token.text, ' : the token itself can be identified :', token.lower_)\n",
    "            elif token.has_vector:\n",
    "                result_word_list.append(token.text)\n",
    "                res = token.text\n",
    "                previous_ent_type = None\n",
    "                if debug: print(token.text, ' : the token itself can be identified :', token.text)\n",
    "            # Damn, I have totally no idea what's going on\n",
    "            # You got to deal with it by yourself\n",
    "            # In my case, I use fasttext to deal with it\n",
    "            else:\n",
    "                is_a_word_parsed_fail = True\n",
    "                fail_words.append(token.text)\n",
    "                previous_ent_type = None\n",
    "                #  Question:\n",
    "                #  can we replace all this kind of word into \"something\" ?\n",
    "                result_word_list.append(SPECIAL_TOKENS['undefined'])\n",
    "                if debug: print(token.text, ' : can\\'t identify, replace with \"something\"')\n",
    "    if show_fail and is_a_word_parsed_fail:\n",
    "        if idx!=None:\n",
    "            print('At qid=', idx)\n",
    "        print('Fail words: ', fail_words)\n",
    "        print('Before:', spacy_obj.text)\n",
    "        print('After: ', ' '.join(result_word_list))\n",
    "        print('====================================================================')\n",
    "    return np.array(result_word_list)\n",
    "\n",
    "def process_new_string(s):\n",
    "    s = clean_string(s)\n",
    "    s = nlp(s)\n",
    "    s = process_question_with_spacy(s)\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "is_substitute_proper_noun = True # [not implemented] a little complicated than it seems\n",
    "is_remove_stopwords = False # not yet tested\n",
    "\n",
    "ENTITY_ENUM = {\n",
    "    '': '',\n",
    "    'PERSON': 'person',\n",
    "    'NORP': 'nationality',\n",
    "    'FAC': 'facility',\n",
    "    'ORG': 'organization',\n",
    "    'GPE': 'country',\n",
    "    'LOC': 'location',\n",
    "    'PRODUCT': 'product',\n",
    "    'EVENT': 'event',\n",
    "    'WORK_OF_ART': 'artwork',\n",
    "    'LANGUAGE': 'language',\n",
    "    'DATE': 'date',\n",
    "    'TIME': 'time',\n",
    "    'PERCENT': 'percent',\n",
    "    'MONEY': 'money',\n",
    "    'QUANTITY': 'quantity',  \n",
    "    'ORDINAL': 'ordinal',\n",
    "    'CARDINAL': 'cardinal',\n",
    "    'PERCENT': 'number',\n",
    "    'MONEY': 'number',\n",
    "    'QUANTITY': 'number',\n",
    "    'ORDINAL': 'number',\n",
    "    'CARDINAL': 'number',\n",
    "    'LAW': 'law'\n",
    "}\n",
    "\n",
    "NUMERIC_TYPES = set([\n",
    "    'DATE',\n",
    "    'TIME',\n",
    "    'PERCENT',\n",
    "    'MONEY',\n",
    "    'QUANTITY',\n",
    "    'ORDINAL',\n",
    "    'CARDINAL',\n",
    "])\n",
    "\n",
    "SPECIAL_TOKENS = {\n",
    "    'quoted': 'quoted_item',\n",
    "    'non-ascii': 'non_ascii_word',\n",
    "    'undefined': 'NULL'\n",
    "    }\n",
    "\n",
    "exception_list =  set(['need']) # spaCy identifies need's lamma as 'ne', which is not we want\n",
    "numeric_types = set(['DATE', 'TIME', 'PERCENT', 'MONEY', 'QUANTITY', 'ORDINAL', 'CARDINAL'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "src = '/media/w/1c392724-ecf3-4615-8f3c-79368ec36380/DS Projects/Kaggle/Quora/data/'\n",
    "\n",
    "df_train = pd.read_csv(src + 'train.csv')\n",
    "df_test = pd.read_csv(src + 'test.csv')\n",
    "\n",
    "df_train = df_train[['question1', 'question2']]\n",
    "df_test = df_test[['question1', 'question2']]\n",
    "\n",
    "#train_set = df_train.copy()\n",
    "train_set = pd.concat([df_train, df_test], axis = 0)\n",
    "\n",
    "train_set['qid1'] = range(0, train_set.shape[0])\n",
    "train_set['qid2'] = range(train_set.shape[0], train_set.shape[0]*2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2750086it [04:34, 10006.16it/s]\n",
      "5500172it [53:18, 1719.56it/s]\n"
     ]
    }
   ],
   "source": [
    "qid_dict = {}\n",
    "for i,series in tqdm(train_set.iterrows()):\n",
    "    if series['qid1'] not in qid_dict:\n",
    "        qid_dict[series['qid1']] = series['question1']\n",
    "    if series['qid2'] not in qid_dict:\n",
    "        qid_dict[series['qid2']] = series['question2']\n",
    "for k in qid_dict:\n",
    "    qid_dict[k] = clean_string(qid_dict[k])\n",
    "    \n",
    "spacy_obj_dict = {}\n",
    "total_len = len(qid_dict)\n",
    "for i,k in tqdm(enumerate(qid_dict)):\n",
    "    # some questions are null\n",
    "    if type(qid_dict[k])!= str:\n",
    "        qid_dict[k] = ''\n",
    "    spacy_obj_dict[k] = nlp(qid_dict[k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5500172/5500172 [09:01<00:00, 10154.73it/s]          \n",
      "100%|██████████| 118944/118944 [00:01<00:00, 74020.14it/s]\n"
     ]
    }
   ],
   "source": [
    "threshold_of_second_type = 3\n",
    "vote_dict = {}\n",
    "word_ent_type_dict = {}\n",
    "word_ent_type_second_dict = {}\n",
    "\n",
    "# construct vote dictionary\n",
    "for qid in tqdm(spacy_obj_dict):\n",
    "    for token in spacy_obj_dict[qid]:\n",
    "        if token.lower_ not in vote_dict:\n",
    "            vote_dict[token.lower_] = {}\n",
    "        if token.ent_type_ not in vote_dict[token.lower_]:\n",
    "            vote_dict[token.lower_][token.ent_type_] = 0\n",
    "        # if the token has_vector is True, maybe we shouldn't record its \n",
    "        vote_dict[token.lower_][token.ent_type_] += 1 # TODO: not sure if storing in lowercase form is safe ?\n",
    "\n",
    "# vote for what should the type be\n",
    "for key in tqdm(vote_dict):\n",
    "    # non-type has lower priority\n",
    "    if '' in vote_dict[key]:\n",
    "        vote_dict[key][''] = vote_dict[key][''] - 0.1\n",
    "    ents = list(vote_dict[key].keys())\n",
    "    bi_list = [\n",
    "        ents,\n",
    "        [vote_dict[key][ent] for ent in ents]\n",
    "    ]\n",
    "    # if several ent_type_ have same count, just let it go, making them share same ent_type_ is enough\n",
    "    # TODO: if have time, can design a better metric to deal with second graded type\n",
    "    sorted_idx = np.argsort(bi_list[1])\n",
    "    if sorted_idx.shape[0]>1:\n",
    "        best_idx = sorted_idx[-1]\n",
    "        second_idx = sorted_idx[-2]\n",
    "        word_ent_type_dict[key] = bi_list[0][best_idx]\n",
    "        if bi_list[1][second_idx]>threshold_of_second_type:\n",
    "            word_ent_type_second_dict[key] = bi_list[0][second_idx]\n",
    "    else:\n",
    "        best_idx = sorted_idx[-1]\n",
    "        word_ent_type_dict[key] = bi_list[0][best_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5500172it [39:18, 2331.68it/s]\n"
     ]
    }
   ],
   "source": [
    "qid_spacy_cleaned_question_dict = {}\n",
    "qid_spacy_cleaned_word_lists_dict = {}\n",
    "for i,k in tqdm(enumerate(spacy_obj_dict)):\n",
    "    word_list = process_question_with_spacy(spacy_obj_dict[k])\n",
    "    qid_spacy_cleaned_word_lists_dict[k] = word_list\n",
    "    qid_spacy_cleaned_question_dict[k] = ' '.join(word_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time it took: 6673.542929410934\n"
     ]
    }
   ],
   "source": [
    "t = time.time()\n",
    "train_set.question1 = train_set.question1.apply(lambda x: ' '.join(process_new_string(x)))\n",
    "train_set.question2 = train_set.question2.apply(lambda x: ' '.join(process_new_string(x)))\n",
    "print('Time it took:', time.time() - t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_set.to_csv('dfNER_full_questionsonly.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "full_df = pd.read_csv('dfNER_full_questionsonly.csv')\n",
    "\n",
    "src = '/media/w/1c392724-ecf3-4615-8f3c-79368ec36380/DS Projects/Kaggle/Quora/data/'\n",
    "train_orig =  pd.read_csv(src + 'train.csv')\n",
    "test_orig =  pd.read_csv(src + 'test.csv')\n",
    "\n",
    "df_tr = full_df.iloc[:train_orig.shape[0], :]\n",
    "df_te = full_df.iloc[train_orig.shape[0]:, :]\n",
    "\n",
    "train_orig['question1'] = df_tr['question1']\n",
    "train_orig['question2'] = df_tr['question2']\n",
    "test_orig['question1'] = df_te['question1'].tolist()\n",
    "test_orig['question2'] = df_te['question2'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_orig.to_csv('df_train_NER.csv', index = False)\n",
    "test_orig.to_csv('df_test_NER.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
