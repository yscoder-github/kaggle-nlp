{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import sklearn\n",
    "import random\n",
    "import itertools\n",
    "import nltk\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.feature_extraction import FeatureHasher\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.manifold import LocallyLinearEmbedding, SpectralEmbedding, TSNE\n",
    "from sklearn.ensemble import RandomTreesEmbedding\n",
    "\n",
    "from textstat.textstat import textstat\n",
    "from gensim import corpora\n",
    "from gensim.models import KeyedVectors\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import *\n",
    "from nltk import word_tokenize, ngrams\n",
    "from nltk.tag import AffixTagger\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from scipy.spatial import distance\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import PCA, TruncatedSVD, LatentDirichletAllocation\n",
    "from sklearn.preprocessing import Normalizer\n",
    "\n",
    "seed = 1337"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def lowercase(df2):\n",
    "    df = df2.copy()\n",
    "    text_feats = df.select_dtypes(include=['object']).columns.values\n",
    "    for i in text_feats:\n",
    "        df[i] = df[i].str.lower()\n",
    "    return df\n",
    "\n",
    "def unidecode(df2):\n",
    "    df = df2.copy()\n",
    "    text_feats = df.select_dtypes(include=['object']).columns.values\n",
    "    for i in text_feats:\n",
    "        df[i] = df[i].str.encode('ascii', 'ignore')\n",
    "    return df\n",
    "\n",
    "def remove_nonalpha(df2):\n",
    "    df = df2.copy()\n",
    "    text_feats = df.select_dtypes(include=['object']).columns.values\n",
    "    for i in text_feats:\n",
    "        df[i] = df[i].str.replace('\\W+', ' ')\n",
    "    return df\n",
    "\n",
    "def repair_words(df2):\n",
    "    df = df2.copy()\n",
    "    text_feats = df.select_dtypes(include=['object']).columns.values\n",
    "    for i in text_feats:\n",
    "        df[i] = df[i].apply(lambda x: (''.join(''.join(s)[:2] for _, s in itertools.groupby(x))))\n",
    "    return df\n",
    "\n",
    "def concat_words(df2):\n",
    "    df = df2.copy()\n",
    "    text_feats = df.select_dtypes(include=['object']).columns.values\n",
    "    for i in text_feats:\n",
    "        df[i] = df[i].apply(lambda x: (' '.join(i for i in x)))\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "def tokenize(df2):\n",
    "    df = df2.copy()\n",
    "    text_feats = df.select_dtypes(include=['object']).columns.values\n",
    "    for i in text_feats:\n",
    "        df[i] = df[i].apply(lambda x: word_tokenize(x))\n",
    "    return df\n",
    "\n",
    "def ngram(df2, n):\n",
    "    df = df2.copy()\n",
    "    text_feats = df.select_dtypes(include=['object']).columns.values\n",
    "    for i in text_feats:\n",
    "        df[i] = df[i].apply(lambda x: [i for i in ngrams(word_tokenize(x), n)])\n",
    "    return df\n",
    "\n",
    "def skipgram(df2, ngram_n, skip_n):\n",
    "    def random_sample(words_list, skip_n):\n",
    "        return [words_list[i] for i in sorted(random.sample(range(len(words_list)), skip_n))]\n",
    "    \n",
    "    df = df2.copy()\n",
    "    text_feats = df.select_dtypes(include=['object']).columns.values\n",
    "    for i in text_feats:\n",
    "        df[i] = df[i].apply(lambda x: [i for i in ngrams(word_tokenize(x), ngram_n)])\n",
    "        df[i] = df[i].apply(lambda x: random_sample(x, skip_n))\n",
    "    return df\n",
    "\n",
    "def chargram(df2, n):\n",
    "    def chargram_generate(string, n):\n",
    "        return [string[i:i+n] for i in range(len(string)-n+1)]\n",
    "    df = df2.copy()\n",
    "    text_feats = df.select_dtypes(include=['object']).columns.values\n",
    "    for i in text_feats:\n",
    "        df[i] = df[i].apply(lambda x: [i for i in chargram_generate(x, 3)])\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "def remove_stops(df2, stopwords):\n",
    "    df = df2.copy()\n",
    "    text_feats = df.select_dtypes(include=['object']).columns.values\n",
    "    for i in text_feats:\n",
    "        df[i] = df[i].apply(lambda x: [i for i in word_tokenize(x) if i not in stopwords])\n",
    "    return df\n",
    "\n",
    "def remove_extremes(df2, stopwords, min_count = 3, max_frequency = 0.75):\n",
    "    df = df2.copy()\n",
    "    text_feats = df.select_dtypes(include=['object']).columns.values\n",
    "    for i in text_feats:\n",
    "        df[i] = df[i].apply(lambda x: [i for i in word_tokenize(x) if i not in stopwords])\n",
    "    tokenized = []\n",
    "    for i in text_feats:\n",
    "        tokenized += df[i].tolist()\n",
    "    dictionary = corpora.Dictionary(tokenized)\n",
    "    dictionary.filter_extremes(no_below = min_count, no_above = max_frequency)\n",
    "    dictionary.compactify()\n",
    "    df = df2.copy()\n",
    "    for i in text_feats:\n",
    "        df[i] = df[i].apply(lambda x: [i for i in word_tokenize(x) if i not in stopwords and i not in \n",
    "                                      list(dictionary.token2id.keys())])\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "def chop(df2, n):\n",
    "    df = df2.copy()\n",
    "    text_feats = df.select_dtypes(include=['object']).columns.values\n",
    "    for i in text_feats:\n",
    "        df[i] = df[i].apply(lambda x: [i[:n] for i in word_tokenize(x)])\n",
    "    return df\n",
    "\n",
    "def stem(df2):\n",
    "    df = df2.copy()\n",
    "    text_feats = df.select_dtypes(include=['object']).columns.values\n",
    "    for i in text_feats:\n",
    "        df[i] = df[i].apply(lambda x: ' '.join([stemmer.stem(i) for i in word_tokenize(x)]))\n",
    "    return df\n",
    "\n",
    "def lemmat(df2):\n",
    "    df = df2.copy()\n",
    "    text_feats = df.select_dtypes(include=['object']).columns.values\n",
    "    for i in text_feats:\n",
    "        df[i] = df[i].apply(lambda x: ' '.join([lemmatizer.lemmatize(i) for i in word_tokenize(x)]))\n",
    "    return df\n",
    "\n",
    "def extract_entity(df2):\n",
    "    df = df2.copy()\n",
    "    text_feats = df.select_dtypes(include=['object']).columns.values\n",
    "    for i in text_feats:\n",
    "        df[i] = df[i].apply(lambda x: word_tokenize(x))\n",
    "        df[i] = df[i].apply(lambda x: nltk.pos_tag(x))\n",
    "        df[i] = df[i].apply(lambda x: [i[1:] for i in x])\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "def doc_features(df2):\n",
    "    df = df2.copy()\n",
    "    text_feats = df.select_dtypes(include=['object']).columns.values\n",
    "    for i, col in enumerate(text_feats):\n",
    "        df['num_characters_{}'.format(i)] = df[col].map(lambda x: len(str(x))) # length of sentence\n",
    "        df['num_words_{}'.format(i)] = df[col].map(lambda x: len(str(x).split())) # number of words\n",
    "        df['num_spaces_{}'.format(i)] = df[col].map(lambda x: x.count(' '))\n",
    "        df['num_alpha_{}'.format(i)] = df[col].apply(lambda x: sum(i.isalpha()for i in x))\n",
    "        df['num_nonalpha_{}'.format(i)] = df[col].apply(lambda x: sum(1-i.isalpha()for i in x))\n",
    "    return df\n",
    "\n",
    "def get_readability(df2):\n",
    "    df = df2.copy()\n",
    "    text_feats = df.select_dtypes(include=['object']).columns.values\n",
    "    for i, col in enumerate(text_feats):\n",
    "        df['flesch_reading_ease{}'.format(i)] = df[col].apply(lambda x: textstat.flesch_reading_ease(x))\n",
    "        df['smog_index{}'.format(i)] = df[col].apply(lambda x: textstat.smog_index(x))\n",
    "        df['flesch_kincaid_grade{}'.format(i)] = df[col].apply(lambda x: textstat.flesch_kincaid_grade(x))\n",
    "        df['coleman_liau_index{}'.format(i)] = df[col].apply(lambda x: textstat.coleman_liau_index(x))\n",
    "        df['automated_readability_index{}'.format(i)] = df[col].apply(lambda x: textstat.automated_readability_index(x))\n",
    "        df['dale_chall_readability_score{}'.format(i)] = df[col].apply(lambda x: textstat.dale_chall_readability_score(x))\n",
    "        df['difficult_words{}'.format(i)] = df[col].apply(lambda x: textstat.difficult_words(x))\n",
    "        df['linsear_write_formula{}'.format(i)] = df[col].apply(lambda x: textstat.linsear_write_formula(x))\n",
    "        df['gunning_fog{}'.format(i)] = df[col].apply(lambda x: textstat.gunning_fog(x))\n",
    "        df['text_standard{}'.format(i)] = df[col].apply(lambda x: textstat.text_standard(x))\n",
    "    return df\n",
    "\n",
    "def bag_of_words(df2):\n",
    "    df = df2.copy()\n",
    "    cv = CountVectorizer(stop_words = 'english', min_df = 1, max_df = 0.999)\n",
    "    bow = cv.fit_transform(df.question1 + df.question2)\n",
    "    return bow\n",
    "\n",
    "def tf_idf(df2):\n",
    "    df = df2.copy()\n",
    "    tf = TfidfVectorizer(stop_words = 'english', min_df = 1, max_df = 0.999)\n",
    "    tfidf = tf.fit_transform(df.question1 + df.question2)\n",
    "    return tfidf\n",
    "\n",
    "def LDA_text2(df2, ntopics):\n",
    "    cv = CountVectorizer(stop_words = 'english', min_df = 2, max_df = 0.99)\n",
    "    lda = LatentDirichletAllocation(ntopics, random_state = seed, n_jobs = 1)\n",
    "    df = df2.copy()\n",
    "    text_feats = df.select_dtypes(include=['object']).columns.values\n",
    "    cv.fit(df.question1 + df.question2)\n",
    "    bow = cv.transform(df.question1 + df.question2)\n",
    "    lda.fit(bow)\n",
    "    ldas = []\n",
    "    for i in text_feats:\n",
    "        bow_i = cv.transform(df[i])\n",
    "        ldas.append(lda.transform(bow_i))\n",
    "    return ldas\n",
    "\n",
    "def SVD_text(df2, ndims):\n",
    "    df = df2.copy()\n",
    "    cv = CountVectorizer(stop_words = 'english', min_df = 2, max_df = 0.99)\n",
    "    svd = TruncatedSVD(ndims, random_state = seed)\n",
    "    text_feats = df.select_dtypes(include=['object']).columns.values\n",
    "    cv.fit(df.question1 + df.question2)\n",
    "    bow = cv.transform(df.question1 + df.question2)\n",
    "    svd.fit(bow)\n",
    "    svds = []\n",
    "    for i in text_feats:\n",
    "        bow_i = cv.transform(df[i])\n",
    "        svd_i = svd.transform(bow_i)\n",
    "        svds.append(svd_i)\n",
    "    return svds\n",
    "\n",
    "def LSA_text(df2, ndims):\n",
    "    cv = CountVectorizer(stop_words = 'english', min_df = 2, max_df = 0.99)\n",
    "    svd = TruncatedSVD(ndims, random_state = 1337)\n",
    "    normalizer = Normalizer(copy = False)\n",
    "    df = df2.copy()\n",
    "    text_feats = df.select_dtypes(include=['object']).columns.values\n",
    "    cv.fit(df.question1 + df.question2)\n",
    "    bow = cv.transform(df.question1 + df.question2)\n",
    "    svd.fit(bow)\n",
    "    transformed_bow = svd.transform(bow)\n",
    "    normed_bow = normalizer.fit(transformed_bow)\n",
    "    svds = []\n",
    "    for i in text_feats:\n",
    "        bow_i = cv.transform(df[i])\n",
    "        svd_i = svd.transform(bow_i)\n",
    "        normed_i = normalizer.transform(svd_i)\n",
    "        svds.append(normed_i)\n",
    "    return svds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stemmer = snowball.SnowballStemmer('english')\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stopwords_eng = stopwords.words('english')\n",
    "words = re.compile(r\"\\w+\",re.I)\n",
    "\n",
    "model = KeyedVectors.load_word2vec_format('/media/w/1c392724-ecf3-4615-8f3c-79368ec36380/DS Projects/Kaggle/Quora/data/embeddings/GoogleNews-vectors-negative300.bin',                            \n",
    "                                             binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "src = '/media/w/1c392724-ecf3-4615-8f3c-79368ec36380/DS Projects/Kaggle/Quora/scripts/features/'\n",
    "\n",
    "dftr2 = pd.read_csv('df_train_lemmatfullcleanSTEMMED.csv')\n",
    "dfte2 = pd.read_csv('df_test_lemmatfullcleanSTEMMED.csv')\n",
    "\n",
    "df_full = pd.concat((dftr2, dfte2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lsa_dff = LSA_text(df_full, 50)\n",
    "svd_dff = SVD_text(df_full, 50)\n",
    "#lda_dff = LDA_text2(df_full, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def split_traintest(l):\n",
    "    train = []\n",
    "    test = []\n",
    "    for i in l:\n",
    "        train.append(i[:dftr2.shape[0]])\n",
    "        test.append(i[dftr2.shape[0]:])\n",
    "    return train, test\n",
    "\n",
    "lsa50tr, lsa50te = split_traintest(lsa_dff)\n",
    "np.save('train_lsa50_3grams_tfidf', lsa50tr)\n",
    "np.save('test_lsa50_3grams_tfidf', lsa50te)\n",
    "\n",
    "svd50tr, svd50te = split_traintest(svd_dff)\n",
    "np.save('train_svd50_3grams_tfidf', svd50tr)\n",
    "np.save('test_svd50_3grams_tfidf', svd50te)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
