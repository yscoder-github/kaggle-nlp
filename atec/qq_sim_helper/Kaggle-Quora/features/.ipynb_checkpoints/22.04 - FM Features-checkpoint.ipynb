{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/qqgeogor/kaggle_quora_benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction import text\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "seed = 1024\n",
    "np.random.seed(seed)\n",
    "\n",
    "import re\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import OneHotEncoder,LabelEncoder,StandardScaler\n",
    "from sklearn.decomposition import TruncatedSVD,PCA\n",
    "from sklearn.metrics.pairwise import cosine_similarity, pairwise_distances\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction import text\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import OneHotEncoder,LabelEncoder,StandardScaler\n",
    "from sklearn.decomposition import TruncatedSVD,PCA\n",
    "from sklearn.metrics.pairwise import cosine_similarity, pairwise_distances\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "from sklearn.preprocessing import OneHotEncoder,LabelEncoder,StandardScaler\n",
    "from sklearn.decomposition import TruncatedSVD,PCA\n",
    "from sklearn.metrics.pairwise import cosine_similarity, pairwise_distances\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import distance\n",
    "\n",
    "\n",
    "path = '/media/w/1c392724-ecf3-4615-8f3c-79368ec36380/DS Projects/Kaggle/Quora/data/'\n",
    "dst = '/media/w/1c392724-ecf3-4615-8f3c-79368ec36380/DS Projects/Kaggle/Quora/data/features/fm_github/'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv(path+\"train.csv\")\n",
    "test = pd.read_csv(path+\"test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def stem_str(x,stemmer=SnowballStemmer('english')):\n",
    "    x = text.re.sub(\"[^a-zA-Z0-9]\",\" \", x)\n",
    "    x = (\" \").join([stemmer.stem(z) for z in x.split(\" \")])\n",
    "    x = \" \".join(x.split())\n",
    "    return x\n",
    "\n",
    "porter = PorterStemmer()\n",
    "snowball = SnowballStemmer('english')\n",
    "\n",
    "print('Generate porter')\n",
    "train['question1_porter'] = train['question1'].astype(str).apply(lambda x:stem_str(x.lower(),porter))\n",
    "test['question1_porter'] = test['question1'].astype(str).apply(lambda x:stem_str(x.lower(),porter))\n",
    "\n",
    "train['question2_porter'] = train['question2'].astype(str).apply(lambda x:stem_str(x.lower(),porter))\n",
    "test['question2_porter'] = test['question2'].astype(str).apply(lambda x:stem_str(x.lower(),porter))\n",
    "\n",
    "train.to_csv(dst+'train_porter.csv')\n",
    "test.to_csv(dst+'test_porter.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv(dst+\"train_porter.csv\")\n",
    "test = pd.read_csv(dst+\"test_porter.csv\")\n",
    "test['is_duplicated']=[-1]*test.shape[0]\n",
    "\n",
    "len_train = train.shape[0]\n",
    "\n",
    "data_all = pd.concat([train,test])\n",
    "\n",
    "\n",
    "def calc_set_intersection(text_a, text_b):\n",
    "    a = set(text_a.split())\n",
    "    b = set(text_b.split())\n",
    "    return len(a.intersection(b)) *1.0 / len(a)\n",
    "\n",
    "print('Generate intersection')\n",
    "train_interaction = train.astype(str).apply(lambda x:calc_set_intersection(x['question1'],x['question2']),axis=1)\n",
    "test_interaction = test.astype(str).apply(lambda x:calc_set_intersection(x['question1'],x['question2']),axis=1)\n",
    "pd.to_pickle(train_interaction,path+\"train_interaction.pkl\")\n",
    "pd.to_pickle(test_interaction,path+\"test_interaction.pkl\")\n",
    "\n",
    "print('Generate porter intersection')\n",
    "train_porter_interaction = train.astype(str).apply(lambda x:calc_set_intersection(x['question1_porter'],x['question2_porter']),axis=1)\n",
    "test_porter_interaction = test.astype(str).apply(lambda x:calc_set_intersection(x['question1_porter'],x['question2_porter']),axis=1)\n",
    "\n",
    "pd.to_pickle(train_porter_interaction,path+\"train_porter_interaction.pkl\")\n",
    "pd.to_pickle(test_porter_interaction,path+\"test_porter_interaction.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ft = ['question1','question2','question1_porter','question2_porter']\n",
    "\n",
    "train = pd.read_csv(dst+\"train_porter.csv\")[ft]\n",
    "test = pd.read_csv(dst+\"test_porter.csv\")[ft]\n",
    "\n",
    "len_train = train.shape[0]\n",
    "data_all = pd.concat([train,test])\n",
    "\n",
    "max_features = None\n",
    "ngram_range = (1,2)\n",
    "min_df = 3\n",
    "print('Generate tfidf')\n",
    "feats= ['question1','question2']\n",
    "vect_orig = TfidfVectorizer(max_features=max_features,ngram_range=ngram_range, min_df=min_df)\n",
    "\n",
    "corpus = []\n",
    "for f in feats:\n",
    "    data_all[f] = data_all[f].astype(str)\n",
    "    corpus+=data_all[f].values.tolist()\n",
    "\n",
    "vect_orig.fit(\n",
    "    corpus\n",
    "    )\n",
    "\n",
    "for f in feats:\n",
    "    tfidfs = vect_orig.transform(data_all[f].values.tolist())\n",
    "    train_tfidf = tfidfs[:train.shape[0]]\n",
    "    test_tfidf = tfidfs[train.shape[0]:]\n",
    "    pd.to_pickle(train_tfidf,path+'train_%s_tfidf.pkl'%f)\n",
    "    pd.to_pickle(test_tfidf,path+'test_%s_tfidf.pkl'%f)\n",
    "\n",
    "\n",
    "print('Generate porter tfidf')\n",
    "feats= ['question1_porter','question2_porter']\n",
    "vect_orig = TfidfVectorizer(max_features=max_features,ngram_range=ngram_range, min_df=min_df)\n",
    "\n",
    "corpus = []\n",
    "for f in feats:\n",
    "    data_all[f] = data_all[f].astype(str)\n",
    "    corpus+=data_all[f].values.tolist()\n",
    "\n",
    "vect_orig.fit(\n",
    "    corpus\n",
    "    )\n",
    "\n",
    "for f in feats:\n",
    "    tfidfs = vect_orig.transform(data_all[f].values.tolist())\n",
    "    train_tfidf = tfidfs[:train.shape[0]]\n",
    "    test_tfidf = tfidfs[train.shape[0]:]\n",
    "    pd.to_pickle(train_tfidf,path+'train_%s_tfidf.pkl'%f)\n",
    "    pd.to_pickle(test_tfidf,path+'test_%s_tfidf.pkl'%f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv(dst+\"train_porter.csv\")\n",
    "test = pd.read_csv(dst+\"test_porter.csv\")\n",
    "test['is_duplicated']=[-1]*test.shape[0]\n",
    "\n",
    "len_train = train.shape[0]\n",
    "\n",
    "data_all = pd.concat([train,test])\n",
    "\n",
    "def str_jaccard(str1, str2):\n",
    "\n",
    "\n",
    "    str1_list = str1.split(\" \")\n",
    "    str2_list = str2.split(\" \")\n",
    "    res = distance.jaccard(str1_list, str2_list)\n",
    "    return res\n",
    "\n",
    "# shortest alignment\n",
    "def str_levenshtein_1(str1, str2):\n",
    "\n",
    "\n",
    "    str1_list = str1.split(' ')\n",
    "    str2_list = str2.split(' ')\n",
    "    res = distance.nlevenshtein(str1, str2,method=1)\n",
    "    return res\n",
    "\n",
    "# longest alignment\n",
    "def str_levenshtein_2(str1, str2):\n",
    "\n",
    "    str1_list = str1.split(' ')\n",
    "    str2_list = str2.split(' ')\n",
    "    res = distance.nlevenshtein(str1, str2,method=2)\n",
    "    return res\n",
    "\n",
    "def str_sorensen(str1, str2):\n",
    "\n",
    "    str1_list = str1.split(' ')\n",
    "    str2_list = str2.split(' ')\n",
    "    res = distance.sorensen(str1_list, str2_list)\n",
    "    return res\n",
    "\n",
    "print('Generate jaccard')\n",
    "train_jaccard = train.astype(str).apply(lambda x:str_jaccard(x['question1'],x['question2']),axis=1)\n",
    "test_jaccard = test.astype(str).apply(lambda x:str_jaccard(x['question1'],x['question2']),axis=1)\n",
    "pd.to_pickle(train_jaccard,path+\"train_jaccard.pkl\")\n",
    "pd.to_pickle(test_jaccard,path+\"test_jaccard.pkl\")\n",
    "\n",
    "print('Generate porter jaccard')\n",
    "train_porter_jaccard = train.astype(str).apply(lambda x:str_jaccard(x['question1_porter'],x['question2_porter']),axis=1)\n",
    "test_porter_jaccard = test.astype(str).apply(lambda x:str_jaccard(x['question1_porter'],x['question2_porter']),axis=1)\n",
    "\n",
    "pd.to_pickle(train_porter_jaccard,path+\"train_porter_jaccard.pkl\")\n",
    "pd.to_pickle(test_porter_jaccard,path+\"test_porter_jaccard.pkl\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv(dst+\"train_porter.csv\").astype(str)\n",
    "test = pd.read_csv(dst+\"test_porter.csv\").astype(str)\n",
    "\n",
    "def str_abs_diff_len(str1, str2):\n",
    "    return abs(len(str1)-len(str2))\n",
    "\n",
    "def str_len(str1):\n",
    "    return len(str(str1))\n",
    "\n",
    "def char_len(str1):\n",
    "    str1_list = set(str(str1).replace(' ',''))\n",
    "    return len(str1_list)\n",
    "\n",
    "def word_len(str1):\n",
    "    str1_list = str1.split(' ')\n",
    "    return len(str1_list)\n",
    "\n",
    "\n",
    "stop_words = stopwords.words('english')\n",
    "def word_match_share(row):\n",
    "    q1words = {}\n",
    "    q2words = {}\n",
    "    for word in str(row['question1']).lower().split():\n",
    "        if word not in stop_words:\n",
    "            q1words[word] = 1\n",
    "    for word in str(row['question2']).lower().split():\n",
    "        if word not in stop_words:\n",
    "            q2words[word] = 1\n",
    "    if len(q1words) == 0 or len(q2words) == 0:\n",
    "        # The computer-generated chaff includes a few questions that are nothing but stopwords\n",
    "        return 0\n",
    "    shared_words_in_q1 = [w for w in q1words.keys() if w in q2words]\n",
    "    shared_words_in_q2 = [w for w in q2words.keys() if w in q1words]\n",
    "    R = (len(shared_words_in_q1) + len(shared_words_in_q2))*1.0/(len(q1words) + len(q2words))\n",
    "    return R\n",
    "\n",
    "print('Generate len')\n",
    "feats = []\n",
    "\n",
    "train['abs_diff_len'] = train.apply(lambda x:str_abs_diff_len(x['question1'],x['question2']),axis=1)\n",
    "test['abs_diff_len']= test.apply(lambda x:str_abs_diff_len(x['question1'],x['question2']),axis=1)\n",
    "feats.append('abs_diff_len')\n",
    "\n",
    "train['R']=train.apply(word_match_share, axis=1, raw=True)\n",
    "test['R']=test.apply(word_match_share, axis=1, raw=True)\n",
    "feats.append('R')\n",
    "\n",
    "train['common_words'] = train.apply(lambda x: len(set(str(x['question1']).lower().split()).intersection(set(str(x['question2']).lower().split()))), axis=1)\n",
    "test['common_words'] = test.apply(lambda x: len(set(str(x['question1']).lower().split()).intersection(set(str(x['question2']).lower().split()))), axis=1)\n",
    "feats.append('common_words')\n",
    "\n",
    "for c in ['question1','question2']:\n",
    "    train['%s_char_len'%c] = train[c].apply(lambda x:char_len(x))\n",
    "    test['%s_char_len'%c] = test[c].apply(lambda x:char_len(x))\n",
    "    feats.append('%s_char_len'%c)\n",
    "\n",
    "    train['%s_str_len'%c] = train[c].apply(lambda x:str_len(x))\n",
    "    test['%s_str_len'%c] = test[c].apply(lambda x:str_len(x))\n",
    "    feats.append('%s_str_len'%c)\n",
    "    \n",
    "    train['%s_word_len'%c] = train[c].apply(lambda x:word_len(x))\n",
    "    test['%s_word_len'%c] = test[c].apply(lambda x:word_len(x))\n",
    "    feats.append('%s_word_len'%c)\n",
    "\n",
    "pd.to_pickle(train[feats].values,path+\"train_len.pkl\")\n",
    "pd.to_pickle(test[feats].values,path+\"test_len.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dst2 = dst + 'generated_features/'\n",
    "\n",
    "train_feats = [x for x in os.listdir(dst2) if 'pkl' in x and 'train' in x]\n",
    "test_feats = [x for x in os.listdir(dst2) if 'pkl' in x and 'test' in x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from scipy.sparse import hstack, csr_matrix\n",
    "\n",
    "def get_feats_csr(features, train = False, test = False):\n",
    "    feats = []\n",
    "    for i in features:\n",
    "        f = pd.read_pickle(dst2 + i)\n",
    "        print(i, f.shape)\n",
    "        f = csr_matrix(f)\n",
    "        if train:\n",
    "            if f.shape[0] != 404290:\n",
    "                f = f.transpose()\n",
    "        if test:\n",
    "            if f.shape[0] != 2345796:\n",
    "                f = f.transpose()\n",
    "        print('CSR matrix from feature array shape:', f.shape)\n",
    "        feats.append(f)\n",
    "    feats = hstack(feats, format = 'csr')\n",
    "    return feats\n",
    "\n",
    "def get_feats_csr_ones(features, train = False, test = False):\n",
    "    feats = []\n",
    "    for i in features:\n",
    "        f = pd.read_pickle(dst2 + i)\n",
    "        print(i, f.shape)\n",
    "        f = csr_matrix(f)\n",
    "        if train:\n",
    "            if f.shape[0] != 404290:\n",
    "                f = f.transpose()\n",
    "        if test:\n",
    "            if f.shape[0] != 2345796:\n",
    "                f = f.transpose()\n",
    "        print('CSR matrix from feature array shape:', f.shape)\n",
    "        if f.shape[1] > 200:\n",
    "            continue\n",
    "        else:\n",
    "            feats.append(f)\n",
    "    feats = hstack(feats, format = 'csr')\n",
    "    return feats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "feats_train = get_feats_csr(train_feats, train = True)\n",
    "feats_test = get_feats_csr(test_feats, test = True)\n",
    "\n",
    "pd.to_pickle(feats_train, dst + 'FM_fullfeats_train.pkl')\n",
    "pd.to_pickle(feats_test, dst + 'FM_fullfeats_test.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_porter_jaccard.pkl (404290,)\n",
      "CSR matrix from feature array shape: (404290, 1)\n",
      "train_jaccard.pkl (404290,)\n",
      "CSR matrix from feature array shape: (404290, 1)\n",
      "train_question2_tfidf.pkl (404290, 1536763)\n",
      "CSR matrix from feature array shape: (404290, 1536763)\n",
      "train_question1_porter_tfidf.pkl (404290, 1335964)\n",
      "CSR matrix from feature array shape: (404290, 1335964)\n",
      "train_porter_interaction.pkl (404290,)\n",
      "CSR matrix from feature array shape: (404290, 1)\n",
      "train_question1_tfidf.pkl (404290, 1536763)\n",
      "CSR matrix from feature array shape: (404290, 1536763)\n",
      "train_interaction.pkl (404290,)\n",
      "CSR matrix from feature array shape: (404290, 1)\n",
      "train_question2_porter_tfidf.pkl (404290, 1335964)\n",
      "CSR matrix from feature array shape: (404290, 1335964)\n",
      "train_len.pkl (404290, 9)\n",
      "CSR matrix from feature array shape: (404290, 9)\n",
      "test_porter_jaccard.pkl (2345796,)\n",
      "CSR matrix from feature array shape: (2345796, 1)\n",
      "test_len.pkl (2345796, 9)\n",
      "CSR matrix from feature array shape: (2345796, 9)\n",
      "test_jaccard.pkl (2345796,)\n",
      "CSR matrix from feature array shape: (2345796, 1)\n",
      "test_question2_porter_tfidf.pkl (2345796, 1335964)\n",
      "CSR matrix from feature array shape: (2345796, 1335964)\n",
      "test_porter_interaction.pkl (2345796,)\n",
      "CSR matrix from feature array shape: (2345796, 1)\n",
      "test_interaction.pkl (2345796,)\n",
      "CSR matrix from feature array shape: (2345796, 1)\n",
      "test_question1_tfidf.pkl (2345796, 1536763)\n",
      "CSR matrix from feature array shape: (2345796, 1536763)\n",
      "test_question1_porter_tfidf.pkl (2345796, 1335964)\n",
      "CSR matrix from feature array shape: (2345796, 1335964)\n",
      "test_question2_tfidf.pkl (2345796, 1536763)\n",
      "CSR matrix from feature array shape: (2345796, 1536763)\n"
     ]
    }
   ],
   "source": [
    "feats_train = get_feats_csr_ones(train_feats, train = True)\n",
    "feats_test = get_feats_csr_ones(test_feats, test = True)\n",
    "\n",
    "pd.to_pickle(feats_train, dst + 'FM_smallfeats_train.pkl')\n",
    "pd.to_pickle(feats_test, dst + 'FM_smallfeats_test.pkl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
