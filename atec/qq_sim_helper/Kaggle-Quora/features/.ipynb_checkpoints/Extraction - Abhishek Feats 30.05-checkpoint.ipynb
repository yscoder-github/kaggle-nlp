{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle #import cPickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gensim\n",
    "from fuzzywuzzy import fuzz\n",
    "from nltk.corpus import stopwords\n",
    "from tqdm import tqdm, tqdm_pandas\n",
    "from scipy.stats import skew, kurtosis\n",
    "from scipy.spatial.distance import cosine, cityblock, jaccard, canberra, euclidean, minkowski, braycurtis\n",
    "from nltk import word_tokenize\n",
    "import os, time, string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def prnt_updt(what_u_b_updtng, the_start):\n",
    "    kardo_time = '{:*^32.22}'.format(time.strftime(\"%a %b %d %I:%M:%S %p\"))\n",
    "    elpsed = '{:,G}'.format(time.time() - the_start)\n",
    "    print(kardo_time + '{:^32.32}'.format(what_u_b_updtng.upper()) + '{:<32.32}'.format(elpsed))\n",
    "\n",
    "def new_sent2vec(words_smart):\n",
    "    ##### this is still slow as shit\n",
    "    #words = str(s).lower().decode('utf-8') #python 2 crap\n",
    "    #words = word_tokenize(words)\n",
    "    words = [w for w in words_smart if not w in STOP_WORDS]\n",
    "    #words = [w for w in words if w.isalpha()] # obsolete --> pandas str.translate\n",
    "    M = []\n",
    "    for w in words:\n",
    "        try:\n",
    "            M.append(model[w])\n",
    "        except:\n",
    "            continue\n",
    "    M = np.array(M)\n",
    "    v = M.sum(axis=0)\n",
    "    return v / np.sqrt((v ** 2).sum())\n",
    "\n",
    "\n",
    "def smart_do_fuzz(a_dataframe):\n",
    "    #a_dataframe.dropna(axis=0, how='any', inplace=True)\n",
    "    a_dataframe.fillna('dingalingalingalinga', inplace=True)\n",
    "    fuzz_q1_lower = a_dataframe['question1'].str.lower()\n",
    "    fuzz_q2_lower = a_dataframe['question2'].str.lower()\n",
    "    #slice_the_testid = a_dataframe['test_id'] # for test set\n",
    "    slice_the_testid = a_dataframe['id'] # for train set\n",
    "    #slice_is_a_dupe = a_dataframe['is_duplicate']\n",
    "    list_of_dicts_for_pandas = []\n",
    "    with tqdm(total=len(slice_the_testid), desc='doin fuzz') as pbar:\n",
    "        for q1_strings, q2_strings, the_id in zip(fuzz_q1_lower, fuzz_q2_lower, slice_the_testid):\n",
    "            fuzz_qratio = fuzz.QRatio(q1_strings, q2_strings)\n",
    "            fuzz_WRatio = fuzz.WRatio(q1_strings, q2_strings)\n",
    "            fuzz_partial_ratio = fuzz.partial_ratio(q1_strings, q2_strings)\n",
    "            fuzz_partial_token_set_ratio = fuzz.partial_token_set_ratio(q1_strings, q2_strings)\n",
    "            fuzz_partial_token_sort_ratio = fuzz.partial_token_sort_ratio(q1_strings, q2_strings)\n",
    "            fuzz_token_set_ratio = fuzz.token_set_ratio(q1_strings, q2_strings)\n",
    "            fuzz_token_sort_ratio = fuzz.token_sort_ratio(q1_strings, q2_strings)\n",
    "            hdr_list = ['id', 'fuzz_qratio',\n",
    "                        'fuzz_WRatio', 'fuzz_partial_ratio',\n",
    "                        'fuzz_partial_token_set_ratio',\n",
    "                        'fuzz_partial_token_sort_ratio',\n",
    "                        'fuzz_token_set_ratio', 'fuzz_token_sort_ratio']\n",
    "            scores_list = [the_id, fuzz_qratio,\n",
    "                           fuzz_WRatio, fuzz_partial_ratio,\n",
    "                           fuzz_partial_token_set_ratio,\n",
    "                           fuzz_partial_token_sort_ratio,\n",
    "                           fuzz_token_set_ratio, fuzz_token_sort_ratio]\n",
    "\n",
    "            list_of_dicts_for_pandas.append(dict(zip(hdr_list, scores_list)))\n",
    "            pbar.update()\n",
    "    pbar.close()\n",
    "    return pd.DataFrame(list_of_dicts_for_pandas)\n",
    "\n",
    "\n",
    "def better_wmd(df_q1_l_s, df_q2_l_s, df_of_id):\n",
    "    list_of_dicts_for_pandas = []\n",
    "    with tqdm(total=len(df_of_id), desc='doin better WMD') as pbar:\n",
    "        for some_id, ss1, ss2 in zip(df_of_id, df_q1_l_s, df_q2_l_s):\n",
    "            s1 = [w for w in ss1 if w not in STOP_WORDS]\n",
    "            s2 = [w for w in ss2 if w not in STOP_WORDS]\n",
    "            wmd_dist = model.wmdistance(s1, s2)\n",
    "            normwmd_dist = norm_model.wmdistance(s1, s2)\n",
    "            hdr_list = ['id', 'wmd_dist', 'normwmd_dist']\n",
    "            scores_list = [some_id, wmd_dist, normwmd_dist]\n",
    "            list_of_dicts_for_pandas.append(dict(zip(hdr_list, scores_list)))\n",
    "            pbar.update()\n",
    "    return pd.DataFrame(list_of_dicts_for_pandas)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import gensim, time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df_train = pd.read_csv('df_train_spacylemmat_fullclean.csv')\n",
    "df_test = pd.read_csv('df_test_spacylemmat_fullclean.csv')\n",
    "fd = pd.concat([df_train, df_test], ignore_index = True)\n",
    "\n",
    "fd.replace(to_replace=[np.inf, -np.inf], value=np.nan, inplace=True)\n",
    "indices_with_nan_or_inf = pd.isnull(fd).any(1).nonzero()[0]\n",
    "if indices_with_nan_or_inf.any():\n",
    "    print('call the ambulance, this dataset fd u')\n",
    "fd.replace(to_replace=np.nan, value=-911, inplace=True)\n",
    "print('I am a genius.  Look at the code in the kernel please.')\n",
    "\n",
    "embed_src = '/media/w/1c392724-ecf3-4615-8f3c-79368ec36380/DS Projects/Kaggle/Quora/data/embeddings/'\n",
    "start_time = time.time()\n",
    "model = gensim.models.KeyedVectors.load_word2vec_format(embed_src + r'GoogleNews-vectors-negative300.bin', binary=True)\n",
    "elapsed_time = time.time() - start_time\n",
    "print(elapsed_time) # about 180 secs on my old PC\n",
    "\n",
    "\n",
    "##### SAVE THE COMPLETE VERSION\n",
    "start_time = time.time()\n",
    "model.save(r'SmallAssGoogleNews.gnsm') ## saves a memory mapped npy file with it\n",
    "elapsed_time = time.time() - start_time\n",
    "print(elapsed_time) # about 30 secs on my old PC\n",
    "\n",
    "\n",
    "##### NORMALIZE AND SAVE THE NORMALIZED VERSION \n",
    "model.init_sims(replace=True)\n",
    "start_time = time.time()\n",
    "model.save(r'SmallAssGoogleNews_normalized.gnsm') ## saves a memory mapped npy file with it\n",
    "elapsed_time = time.time() - start_time\n",
    "print(elapsed_time) # about 30 secs on my old PC\n",
    "\n",
    "\n",
    "###### LOOK! IT WORKS (IN A SEPARATE INSTANCE)\n",
    "start_time = time.time()\n",
    "model2 = gensim.models.KeyedVectors.load(r'SmallAssGoogleNews.gnsm', mmap='r')\n",
    "elapsed_time = time.time() - start_time\n",
    "print(elapsed_time) # about 18 secs on my old PC\n",
    "\n",
    "s1 = u'I love cash and apples.'.split()\n",
    "s2 = u'Me fondle oranges and money'.split()\n",
    "ohyeah = model2.wmdistance(s1, s2)\n",
    "print(str(ohyeah))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "begin_time = time.time()\n",
    "\n",
    "BASE_DIR = r'/media/w/1c392724-ecf3-4615-8f3c-79368ec36380/DS Projects/Kaggle/Quora/scripts/features/'\n",
    "Q1_W2V_FILE2SAVE = os.path.join(BASE_DIR, r'FeatureEngineering', r'q1_w2v_train.pkl')\n",
    "Q2_W2V_FILE2SAVE = os.path.join(BASE_DIR, r'FeatureEngineering', r'q2_w2v_train.pkl')\n",
    "ABISHEKS_OUTPUT = os.path.join(BASE_DIR, r'FeatureEngineering', r'quora_featuresindian_train_id.csv')\n",
    "WORDMOVEDIST_OUTPUT = os.path.join(BASE_DIR, r'FeatureEngineering', r'quora_featuresindian_3_WMD.csv')\n",
    "FUZZ_WITH_ID_OUTPUT = os.path.join(BASE_DIR, r'FeatureEngineering', r'quora_featuresindian_3_fuzz.csv')\n",
    "GENSIM_LIMIT_ROWS = None\n",
    "\n",
    "# SEE MY OTHER KERNAL FOR THIS MMAP STUFF\n",
    "BIGASS_GOOG_mmap = r'SmallAssGoogleNews.gnsm'\n",
    "BIGASS_GOOG_NORM_mmap = r'SmallAssGoogleNews_normalized.gnsm'\n",
    "\n",
    "STOP_WORDS = stopwords.words('english')\n",
    "CHARS2REPLACE = string.punctuation + string.digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*****Tue May 30 05:07:27 PM*****        STARTING UP NOW         5.5766                          \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Doin common words so: 2750086it [01:31, 30114.76it/s]               \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*****Tue May 30 05:09:24 PM*****      STARTING SMART FUZZ       122.455                         \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "doin fuzz: 100%|██████████| 2750086/2750086 [25:17<00:00, 1812.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*****Tue May 30 05:34:51 PM*****    DONE FUZZ, LOADING GOOG1    1,649.57                        \n"
     ]
    }
   ],
   "source": [
    "GENSIM_LIMIT_ROWS = None\n",
    "prnt_updt('STARTING UP NOW', start_time)\n",
    "\n",
    "df_train = pd.read_csv('df_train_lemmatfullcleanSTEMMED.csv')\n",
    "df_test = pd.read_csv('df_test_lemmatfullcleanSTEMMED.csv')\n",
    "data = pd.concat([df_train, df_test])\n",
    "#data = data.drop(['id', 'qid1', 'qid2'], axis=1) ## SERIOUSLY DUDE DON'T BE A DICK\n",
    "data.fillna('NULL', inplace=True)\n",
    "\n",
    "tqdm.pandas(desc='{:>20.20}'.format('Doin common words so slow...'))\n",
    "df_q1_lower_split = data['question1'].str.lower().str.split()\n",
    "df_q2_lower_split = data['question2'].str.lower().str.split()\n",
    "data['len_q1'] = data['question1'].str.len() # total length of question chars incl whitespace\n",
    "data['len_q2'] = data['question2'].str.len()\n",
    "data['diff_len'] = data.len_q1 - data.len_q2\n",
    "data['len_char_q1'] = df_q1_lower_split.str.join(\"\").str.len() # total length of chars no whitespace\n",
    "data['len_char_q2'] = df_q2_lower_split.str.join(\"\").str.len()\n",
    "data['len_word_q1'] = df_q1_lower_split.str.len() # num of words\n",
    "data['len_word_q2'] = df_q2_lower_split.str.len()\n",
    "data['common_words'] = data.progress_apply(lambda x: len(set(str(x['question1']).lower().split()).intersection(set(str(x['question2']).lower().split()))), axis=1)\n",
    "prnt_updt('STARTING SMART FUZZ', start_time)\n",
    "\n",
    "fuzz_with_id = smart_do_fuzz(data) ## JOIN DF AT THE END\n",
    "prnt_updt('DONE FUZZ, LOADING GOOG1', start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*****Tue May 30 05:34:58 PM*****   DONE GOOG1, LOADING GOOG2    7.46529                         \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "doin better WMD:   0%|          | 1/2750086 [00:00<146:05:33,  5.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*****Tue May 30 05:35:06 PM*****    DONE GOOG2, STARTING WMD    8.21431                         \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "doin better WMD: 100%|██████████| 2750086/2750086 [1:15:54<00:00, 603.75it/s]\n",
      "doin better WMD: 100%|██████████| 2750086/2750086 [1:06:03<00:00, 693.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*****Tue May 30 07:57:13 PM***** DONE WMD, STARTING SENT2VEC Q1 8,535.07                        \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "             Q1 vecs:   0%|          | 0/2750086 [00:00<?, ?it/s]/home/w/anaconda3/envs/idp3/lib/python3.5/site-packages/ipykernel_launcher.py:20: RuntimeWarning: invalid value encountered in double_scalars\n",
      "             Q1 vecs: 100%|██████████| 2750086/2750086 [02:42<00:00, 16939.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*****Tue May 30 08:00:10 PM***** DONE Q1, STARTING SENT2VEC Q2  8,712.05                        \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "        Q2 vecs dawg: 100%|██████████| 2750086/2750086 [02:53<00:00, 15812.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*****Tue May 30 08:03:20 PM*****  DONE Q2, STARTING DIST CALCS  8,902.02                        \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/2750086 [00:58<45052:20:49, 58.98s/it]/home/w/anaconda3/envs/idp3/lib/python3.5/site-packages/scipy/spatial/distance.py:505: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  dist = 1.0 - np.dot(u, v) / (norm(u) * norm(v))\n",
      "100%|██████████| 2750086/2750086 [02:24<00:00, 19086.55it/s]\n",
      "100%|██████████| 2750086/2750086 [00:45<00:00, 60607.72it/s] \n",
      "  0%|          | 1/2750086 [00:13<10027:25:50, 13.13s/it]/home/w/anaconda3/envs/idp3/lib/python3.5/site-packages/scipy/spatial/distance.py:616: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  np.double(np.bitwise_or(u != 0, v != 0).sum()))\n",
      "100%|██████████| 2750086/2750086 [01:01<00:00, 44411.74it/s]\n",
      "100%|██████████| 2750086/2750086 [01:45<00:00, 26132.58it/s]\n",
      "100%|██████████| 2750086/2750086 [00:56<00:00, 48337.31it/s]\n",
      "100%|██████████| 2750086/2750086 [02:07<00:00, 21624.20it/s]\n",
      "  0%|          | 1/2750086 [00:08<6678:49:10,  8.74s/it]/home/w/anaconda3/envs/idp3/lib/python3.5/site-packages/scipy/spatial/distance.py:810: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  return abs(u - v).sum() / abs(u + v).sum()\n",
      "100%|██████████| 2750086/2750086 [00:44<00:00, 62488.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*****Tue May 30 08:14:49 PM*****DONE DIST, DOING SKEW AND KURTOS688.886                         \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2750086/2750086 [09:04<00:00, 5053.85it/s]\n",
      "100%|██████████| 2750086/2750086 [08:56<00:00, 5125.00it/s]\n",
      "100%|██████████| 2750086/2750086 [05:48<00:00, 7889.85it/s]\n",
      "100%|██████████| 2750086/2750086 [05:47<00:00, 7916.32it/s]\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "model = gensim.models.KeyedVectors.load(r'SmallAssGoogleNews.gnsm', mmap='r')\n",
    "prnt_updt('DONE GOOG1, LOADING GOOG2', start_time)\n",
    "   \n",
    "start_time = time.time()\n",
    "norm_model = gensim.models.KeyedVectors.load(r'SmallAssGoogleNews_normalized.gnsm', mmap='r')\n",
    "prnt_updt('DONE GOOG2, STARTING WMD', start_time)\n",
    "df_id_wmd_normwmd = better_wmd(df_q1_lower_split, df_q2_lower_split, data['id']) ## JOIN DF AT THE END\n",
    "df_id_wmd_normwmd = better_wmd(df_q1_lower_split, df_q2_lower_split, data['test_id']) ## JOIN DF AT THE END\n",
    "prnt_updt('DONE WMD, STARTING SENT2VEC Q1', start_time)\n",
    "\n",
    "question1_vectors = np.zeros((data.shape[0], 300))\n",
    "shit5 = data['question1'].str.lower().str.translate(str.maketrans({key: ' ' for key in CHARS2REPLACE})).str.split()\n",
    "for i, q in enumerate(tqdm(shit5, desc='{:>20.20}'.format('Q1 vecs'))):\n",
    "    question1_vectors[i, :] = new_sent2vec(q)\n",
    "prnt_updt('DONE Q1, STARTING SENT2VEC Q2', start_time)\n",
    "\n",
    "shit52 = data['question2'].str.lower().str.translate(str.maketrans({key: ' ' for key in CHARS2REPLACE})).str.split()\n",
    "question2_vectors  = np.zeros((data.shape[0], 300))\n",
    "for i, q in enumerate(tqdm(shit52, desc='{:>20.20}'.format('Q2 vecs dawg'))):\n",
    "    question2_vectors[i, :] = new_sent2vec(q)\n",
    "prnt_updt('DONE Q2, STARTING DIST CALCS', start_time)\n",
    "\n",
    "start_time = time.time()\n",
    "data['cosine_distance'] = [cosine(x, y) for (x, y) in zip(tqdm(np.nan_to_num(question1_vectors)),\n",
    "                                                          np.nan_to_num(question2_vectors))]\n",
    "\n",
    "data['cityblock_distance'] = [cityblock(x, y) for (x, y) in zip(tqdm(np.nan_to_num(question1_vectors)),\n",
    "                                                          np.nan_to_num(question2_vectors))]\n",
    "\n",
    "data['jaccard_distance'] = [jaccard(x, y) for (x, y) in zip(tqdm(np.nan_to_num(question1_vectors)),\n",
    "                                                          np.nan_to_num(question2_vectors))]\n",
    "\n",
    "data['canberra_distance'] = [canberra(x, y) for (x, y) in zip(tqdm(np.nan_to_num(question1_vectors)),\n",
    "                                                          np.nan_to_num(question2_vectors))]\n",
    "\n",
    "data['euclidean_distance'] = [euclidean(x, y) for (x, y) in zip(tqdm(np.nan_to_num(question1_vectors)),\n",
    "                                                          np.nan_to_num(question2_vectors))]\n",
    "\n",
    "data['minkowski_distance'] = [minkowski(x, y, 3) for (x, y) in zip(tqdm(np.nan_to_num(question1_vectors)),\n",
    "                                                          np.nan_to_num(question2_vectors))]\n",
    "\n",
    "data['braycurtis_distance'] = [braycurtis(x, y) for (x, y) in zip(tqdm(np.nan_to_num(question1_vectors)),\n",
    "                                                          np.nan_to_num(question2_vectors))]\n",
    "\n",
    "prnt_updt('DONE DIST, DOING SKEW AND KURTOSIS', start_time)\n",
    "data['skew_q1vec'] = [skew(x) for x in tqdm(np.nan_to_num(question1_vectors))]\n",
    "data['skew_q2vec'] = [skew(x) for x in tqdm(np.nan_to_num(question2_vectors))]\n",
    "data['kur_q1vec'] = [kurtosis(x) for x in tqdm(np.nan_to_num(question1_vectors))]\n",
    "data['kur_q2vec'] = [kurtosis(x) for x in tqdm(np.nan_to_num(question2_vectors))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*****Tue May 30 08:47:06 PM*****     FINISHED PICKLING VECS     2,625.61                        \n"
     ]
    }
   ],
   "source": [
    "#rnt_updt('DONE ALL CALCS, SAVING...', start_time)\n",
    "pickle.dump(question1_vectors, open(Q1_W2V_FILE2SAVE, 'wb'), -1)\n",
    "pickle.dump(question2_vectors, open(Q2_W2V_FILE2SAVE, 'wb'), -1)\n",
    "prnt_updt('FINISHED PICKLING VECS', start_time)\n",
    "\n",
    "df_id_wmd_normwmd.to_csv(WORDMOVEDIST_OUTPUT, header=True, index=True, encoding=\"utf-8\")\n",
    "fuzz_with_id.to_csv(FUZZ_WITH_ID_OUTPUT, header=True, index=True, encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
