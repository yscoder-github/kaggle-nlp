{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import sklearn\n",
    "import random\n",
    "import itertools\n",
    "import nltk\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import gensim\n",
    "import lzma\n",
    "\n",
    "from sklearn.feature_extraction import FeatureHasher\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.manifold import LocallyLinearEmbedding, SpectralEmbedding, TSNE\n",
    "from sklearn.ensemble import RandomTreesEmbedding\n",
    "\n",
    "from scipy import sparse\n",
    "from sklearn.decomposition import *\n",
    "from scipy.stats import skew, kurtosis\n",
    "from textstat.textstat import textstat\n",
    "from gensim import corpora\n",
    "from gensim.models import KeyedVectors\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import *\n",
    "from nltk import word_tokenize, ngrams\n",
    "from nltk.tag import AffixTagger\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from scipy.spatial import distance\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import PCA, TruncatedSVD, LatentDirichletAllocation, NMF\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from scipy.spatial.distance import cosine, cityblock, jaccard, canberra, euclidean, minkowski, braycurtis\n",
    "\n",
    "seed = 1337"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lowercase(df2):\n",
    "    df = df2.copy()\n",
    "    text_feats = df.select_dtypes(include=['object']).columns.values\n",
    "    for i in text_feats:\n",
    "        df[i] = df[i].str.lower()\n",
    "    return df\n",
    "\n",
    "def unidecode(df2):\n",
    "    df = df2.copy()\n",
    "    text_feats = df.select_dtypes(include=['object']).columns.values\n",
    "    for i in text_feats:\n",
    "        df[i] = df[i].str.encode('ascii', 'ignore')\n",
    "    return df\n",
    "\n",
    "def remove_nonalpha(df2):\n",
    "    df = df2.copy()\n",
    "    text_feats = df.select_dtypes(include=['object']).columns.values\n",
    "    for i in text_feats:\n",
    "        df[i] = df[i].str.replace('\\W+', ' ')\n",
    "    return df\n",
    "\n",
    "def repair_words(df2):\n",
    "    df = df2.copy()\n",
    "    text_feats = df.select_dtypes(include=['object']).columns.values\n",
    "    for i in text_feats:\n",
    "        df[i] = df[i].apply(lambda x: (''.join(''.join(s)[:2] for _, s in itertools.groupby(x))))\n",
    "    return df\n",
    "\n",
    "def concat_words(df2):\n",
    "    df = df2.copy()\n",
    "    text_feats = df.select_dtypes(include=['object']).columns.values\n",
    "    for i in text_feats:\n",
    "        df[i] = df[i].apply(lambda x: (' '.join(i for i in x)))\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "def tokenize(df2):\n",
    "    df = df2.copy()\n",
    "    text_feats = df.select_dtypes(include=['object']).columns.values\n",
    "    for i in text_feats:\n",
    "        df[i] = df[i].apply(lambda x: word_tokenize(x))\n",
    "    return df\n",
    "\n",
    "def ngram(df2, n):\n",
    "    df = df2.copy()\n",
    "    text_feats = df.select_dtypes(include=['object']).columns.values\n",
    "    for i in text_feats:\n",
    "        df[i] = df[i].apply(lambda x: [i for i in ngrams(word_tokenize(x), n)])\n",
    "    return df\n",
    "\n",
    "def skipgram(df2, ngram_n, skip_n):\n",
    "    def random_sample(words_list, skip_n):\n",
    "        return [words_list[i] for i in sorted(random.sample(range(len(words_list)), skip_n))]\n",
    "    \n",
    "    df = df2.copy()\n",
    "    text_feats = df.select_dtypes(include=['object']).columns.values\n",
    "    for i in text_feats:\n",
    "        df[i] = df[i].apply(lambda x: [i for i in ngrams(word_tokenize(x), ngram_n)])\n",
    "        df[i] = df[i].apply(lambda x: random_sample(x, skip_n))\n",
    "    return df\n",
    "\n",
    "def chargram(df2, n):\n",
    "    def chargram_generate(string, n):\n",
    "        return [string[i:i+n] for i in range(len(string)-n+1)]\n",
    "    df = df2.copy()\n",
    "    text_feats = df.select_dtypes(include=['object']).columns.values\n",
    "    for i in text_feats:\n",
    "        df[i] = df[i].apply(lambda x: [i for i in chargram_generate(x, 3)])\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "def remove_stops(df2, stopwords):\n",
    "    df = df2.copy()\n",
    "    text_feats = df.select_dtypes(include=['object']).columns.values\n",
    "    for i in text_feats:\n",
    "        df[i] = df[i].apply(lambda x: [i for i in word_tokenize(x) if i not in stopwords])\n",
    "    return df\n",
    "\n",
    "def remove_extremes(df2, stopwords, min_count = 3, max_frequency = 0.75):\n",
    "    df = df2.copy()\n",
    "    text_feats = df.select_dtypes(include=['object']).columns.values\n",
    "    for i in text_feats:\n",
    "        df[i] = df[i].apply(lambda x: [i for i in word_tokenize(x) if i not in stopwords])\n",
    "    tokenized = []\n",
    "    for i in text_feats:\n",
    "        tokenized += df[i].tolist()\n",
    "    dictionary = corpora.Dictionary(tokenized)\n",
    "    dictionary.filter_extremes(no_below = min_count, no_above = max_frequency)\n",
    "    dictionary.compactify()\n",
    "    df = df2.copy()\n",
    "    for i in text_feats:\n",
    "        df[i] = df[i].apply(lambda x: [i for i in word_tokenize(x) if i not in stopwords and i not in \n",
    "                                      list(dictionary.token2id.keys())])\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "def chop(df2, n):\n",
    "    df = df2.copy()\n",
    "    text_feats = df.select_dtypes(include=['object']).columns.values\n",
    "    for i in text_feats:\n",
    "        df[i] = df[i].apply(lambda x: [i[:n] for i in word_tokenize(x)])\n",
    "    return df\n",
    "\n",
    "def stem(df2):\n",
    "    df = df2.copy()\n",
    "    text_feats = df.select_dtypes(include=['object']).columns.values\n",
    "    for i in text_feats:\n",
    "        df[i] = df[i].apply(lambda x: ' '.join([stemmer.stem(i) for i in word_tokenize(x)]))\n",
    "    return df\n",
    "\n",
    "def lemmat(df2):\n",
    "    df = df2.copy()\n",
    "    text_feats = df.select_dtypes(include=['object']).columns.values\n",
    "    for i in text_feats:\n",
    "        df[i] = df[i].apply(lambda x: ' '.join([lemmatizer.lemmatize(i) for i in word_tokenize(x)]))\n",
    "    return df\n",
    "\n",
    "def extract_entity(df2):\n",
    "    df = df2.copy()\n",
    "    text_feats = df.select_dtypes(include=['object']).columns.values\n",
    "    for i in text_feats:\n",
    "        df[i] = df[i].apply(lambda x: word_tokenize(x))\n",
    "        df[i] = df[i].apply(lambda x: nltk.pos_tag(x))\n",
    "        df[i] = df[i].apply(lambda x: [i[1:] for i in x])\n",
    "    return df\n",
    "\n",
    "\n",
    "def doc_features(df2):\n",
    "    df = df2.copy()\n",
    "    text_feats = df.select_dtypes(include=['object']).columns.values\n",
    "    for i, col in enumerate(text_feats):\n",
    "        df['num_characters_{}'.format(i)] = df[col].map(lambda x: len(str(x))) # length of sentence\n",
    "        df['num_words_{}'.format(i)] = df[col].map(lambda x: len(str(x).split())) # number of words\n",
    "        df['num_spaces_{}'.format(i)] = df[col].map(lambda x: x.count(' '))\n",
    "        df['num_alpha_{}'.format(i)] = df[col].apply(lambda x: sum(i.isalpha()for i in x))\n",
    "        df['num_nonalpha_{}'.format(i)] = df[col].apply(lambda x: sum(1-i.isalpha()for i in x))\n",
    "    return df\n",
    "\n",
    "def get_readability(df2):\n",
    "    df = df2.copy()\n",
    "    text_feats = df.select_dtypes(include=['object']).columns.values\n",
    "    for i, col in enumerate(text_feats):\n",
    "        df['flesch_reading_ease{}'.format(i)] = df[col].apply(lambda x: textstat.flesch_reading_ease(x))\n",
    "        df['smog_index{}'.format(i)] = df[col].apply(lambda x: textstat.smog_index(x))\n",
    "        df['flesch_kincaid_grade{}'.format(i)] = df[col].apply(lambda x: textstat.flesch_kincaid_grade(x))\n",
    "        df['coleman_liau_index{}'.format(i)] = df[col].apply(lambda x: textstat.coleman_liau_index(x))\n",
    "        df['automated_readability_index{}'.format(i)] = df[col].apply(lambda x: textstat.automated_readability_index(x))\n",
    "        df['dale_chall_readability_score{}'.format(i)] = df[col].apply(lambda x: textstat.dale_chall_readability_score(x))\n",
    "        df['difficult_words{}'.format(i)] = df[col].apply(lambda x: textstat.difficult_words(x))\n",
    "        df['linsear_write_formula{}'.format(i)] = df[col].apply(lambda x: textstat.linsear_write_formula(x))\n",
    "        df['gunning_fog{}'.format(i)] = df[col].apply(lambda x: textstat.gunning_fog(x))\n",
    "        df['text_standard{}'.format(i)] = df[col].apply(lambda x: textstat.text_standard(x))\n",
    "    return df\n",
    "\n",
    "\n",
    "def tf_idf(df_full, dftr):\n",
    "    tf = TfidfVectorizer(stop_words = 'english', min_df = 1, max_df = 0.999)\n",
    "    tf.fit(df_full.question1 + df_full.question2)\n",
    "    q1_tfidf = tf.transform(dftr.question1.values)\n",
    "    q2_tfidf = tf.transform(dftr.question2.values)\n",
    "    tr_tfidf = sparse.hstack([q1_tfidf, q2_tfidf])\n",
    "    print('Final shape:', tr_tfidf.shape)\n",
    "    return tr_tfidf\n",
    "\n",
    "def SVD_text(df2, ndims):\n",
    "    df = df2.copy()\n",
    "    cv = CountVectorizer(stop_words = 'english', min_df = 2, max_df = 0.99)\n",
    "    svd = TruncatedSVD(ndims, random_state = seed)\n",
    "    text_feats = df.select_dtypes(include=['object']).columns.values\n",
    "    cv.fit(df.question1 + df.question2)\n",
    "    bow = cv.transform(df.question1 + df.question2)\n",
    "    svd.fit(bow)\n",
    "    svds = []\n",
    "    for i in text_feats:\n",
    "        bow_i = cv.transform(df[i])\n",
    "        svd_i = svd.transform(bow_i)\n",
    "        svds.append(svd_i)\n",
    "    return svds\n",
    "\n",
    "def LSA_text(df2, ndims):\n",
    "    cv = CountVectorizer(stop_words = 'english', min_df = 2, max_df = 0.99)\n",
    "    svd = TruncatedSVD(ndims, random_state = 1337)\n",
    "    normalizer = Normalizer(copy = False)\n",
    "    df = df2.copy()\n",
    "    text_feats = df.select_dtypes(include=['object']).columns.values\n",
    "    cv.fit(df.question1 + df.question2)\n",
    "    bow = cv.transform(df.question1 + df.question2)\n",
    "    svd.fit(bow)\n",
    "    transformed_bow = svd.transform(bow)\n",
    "    normed_bow = normalizer.fit(transformed_bow)\n",
    "    svds = []\n",
    "    for i in text_feats:\n",
    "        bow_i = cv.transform(df[i])\n",
    "        svd_i = svd.transform(bow_i)\n",
    "        normed_i = normalizer.transform(svd_i)\n",
    "        svds.append(normed_i)\n",
    "    return svds\n",
    "\n",
    "def SVD_text_tfidf(df2, ndims, gram_range, analyze = 'word'):\n",
    "    df = df2.copy()\n",
    "    tf = TfidfVectorizer(stop_words = 'english', min_df = 1, max_df = 0.999, ngram_range = (1, gram_range),\n",
    "                        analyzer = analyze)\n",
    "    svd = TruncatedSVD(ndims, random_state = seed)\n",
    "    text_feats = df.select_dtypes(include=['object']).columns.values\n",
    "    tf.fit(df.question1 + df.question2)\n",
    "    bow = tf.transform(df.question1 + df.question2)\n",
    "    svd.fit(bow)\n",
    "    svds = []\n",
    "    for i in text_feats:\n",
    "        bow_i = tf.transform(df[i])\n",
    "        svd_i = svd.transform(bow_i)\n",
    "        svds.append(svd_i)\n",
    "    return svds\n",
    "\n",
    "def LSA_text_tfidf(df2, ndims, gram_range, analyze = 'word'):\n",
    "    tf = TfidfVectorizer(stop_words = 'english', min_df = 1, max_df = 0.999, ngram_range = (1, gram_range),\n",
    "                        analyzer = analyze)\n",
    "    svd = TruncatedSVD(ndims, random_state = 1337)\n",
    "    normalizer = Normalizer(copy = False)\n",
    "    df = df2.copy()\n",
    "    text_feats = df.select_dtypes(include=['object']).columns.values\n",
    "    tf.fit(df.question1 + df.question2)\n",
    "    bow = tf.transform(df.question1 + df.question2)\n",
    "    svd.fit(bow)\n",
    "    transformed_bow = svd.transform(bow)\n",
    "    normed_bow = normalizer.fit(transformed_bow)\n",
    "    svds = []\n",
    "    for i in text_feats:\n",
    "        bow_i = tf.transform(df[i])\n",
    "        svd_i = svd.transform(bow_i)\n",
    "        normed_i = normalizer.transform(svd_i)\n",
    "        svds.append(normed_i)\n",
    "    return svds\n",
    "\n",
    "def run_transforms(transformation_name, ndims, gram_range, analyze, test = False):\n",
    "    lsa_dff = LSA_text_tfidf(df_full, ndims, gram_range, analyze)\n",
    "    svd_dff = SVD_text_tfidf(df_full, ndims, gram_range, analyze)\n",
    "    lsa_tr, lsa_te = split_traintest(lsa_dff)\n",
    "    svd_tr, svd_te = split_traintest(svd_dff)\n",
    "    tr_lsa_dist = get_distances('train_LSA_{}'.format(transformation_name), lsa_tr[0], lsa_tr[1])\n",
    "    tr_svd_dist = get_distances('train_SVD_{}'.format(transformation_name), svd_tr[0], svd_tr[1])\n",
    "    tr_lsa_dist.to_csv('train_LSA_{}.csv'.format(transformation_name), index = False)\n",
    "    tr_svd_dist.to_csv('train_SVD_{}'.format(transformation_name), index = False)\n",
    "    if test:\n",
    "        te_lsa_dist = get_distances('test_LSA_{}'.format(transformation_name), lsa_te[0], lsa_te[1])\n",
    "        te_svd_dist = get_distances('test_SVD_{}'.format(transformation_name), svd_te[0], svd_te[1])\n",
    "        te_lsa_dist.to_csv('test_LSA_{}.csv'.format(transformation_name), index = False)\n",
    "        te_svd_dist.to_csv('test_SVD_{}'.format(transformation_name), index = False)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def split_traintest(l):\n",
    "    train = []\n",
    "    test = []\n",
    "    for i in l:\n",
    "        train.append(i[:dftr2.shape[0]])\n",
    "        test.append(i[dftr2.shape[0]:])\n",
    "    return train, test\n",
    "\n",
    "def compression_distance(x,y,l_x=None,l_y=None):\n",
    "    if x==y:\n",
    "        return 0\n",
    "    x_b = x.encode('utf-8')\n",
    "    y_b = y.encode('utf-8')\n",
    "    if l_x is None:\n",
    "        l_x = len(lzma.compress(x_b))\n",
    "        l_y = len(lzma.compress(y_b))\n",
    "    l_xy = len(lzma.compress(x_b+y_b))\n",
    "    l_yx = len(lzma.compress(y_b+x_b))\n",
    "    dist = (min(l_xy,l_yx)-min(l_x,l_y))/max(l_x,l_y)\n",
    "    return dist\n",
    "\n",
    "def norm_wmd(s1, s2):\n",
    "    s1 = str(s1).lower().split()\n",
    "    s2 = str(s2).lower().split()\n",
    "    stop_words = stopwords.words('english')\n",
    "    return norm_model.wmdistance(s1, s2)\n",
    "\n",
    "def wmd(s1, s2):\n",
    "    s1 = str(s1).lower().split()\n",
    "    s2 = str(s2).lower().split()\n",
    "    return model.wmdistance(s1, s2)\n",
    "\n",
    "def extract_WMD(train = True):\n",
    "    if train:\n",
    "        print('Extracting WMD train distances.')\n",
    "        tr_feats = pd.DataFrame()\n",
    "        tr_feats['wmd_clean'] = trdf.apply(lambda x: wmd(x['question1'], x['question2']), axis=1)\n",
    "        tr_feats['norm_wmd_clean'] = trdf.apply(lambda x: norm_wmd(x['question1'], x['question2']), axis=1)\n",
    "        tr_feats['wmd_cleanStemmed'] = trdfs.apply(lambda x: wmd(x['question1'], x['question2']), axis=1)\n",
    "        tr_feats['norm_wmd_cleanStemmed'] = trdfs.apply(lambda x: norm_wmd(x['question1'], x['question2']), axis=1)\n",
    "        tr_feats.to_csv('train_WMD_cleaned_stemmed.csv', index = False)\n",
    "        print('WMD distances extracted.')\n",
    "        print(time.time() - t)\n",
    "    else:\n",
    "        print('Extracting WMD test distances.')\n",
    "        te_feats = pd.DataFrame()\n",
    "        te_feats['wmd_clean'] = tedf.apply(lambda x: wmd(x['question1'], x['question2']), axis=1)\n",
    "        te_feats['norm_wmd_clean'] = tedf.apply(lambda x: norm_wmd(x['question1'], x['question2']), axis=1)\n",
    "        te_feats['wmd_cleanStemmed'] = tedfs.apply(lambda x: wmd(x['question1'], x['question2']), axis=1)\n",
    "        te_feats['norm_wmd_cleanStemmed'] = tedfs.apply(lambda x: norm_wmd(x['question1'], x['question2']), axis=1)\n",
    "        te_feats.to_csv('test_WMD_cleaned_stemmed.csv', index = False)\n",
    "        print('WMD distances extracted.')\n",
    "        print(time.time() - t)\n",
    "    return\n",
    "\n",
    "def get_distances(transformation_name, question1_vectors, question2_vectors):\n",
    "    data = pd.DataFrame()\n",
    "    data['cosine_distance_{}'.format(transformation_name)] = [cosine(x, y) for (x, y) in zip(np.nan_to_num(question1_vectors),\n",
    "                                                              np.nan_to_num(question2_vectors))]\n",
    "\n",
    "    data['cityblock_distance_{}'.format(transformation_name)] = [cityblock(x, y) for (x, y) in zip(np.nan_to_num(question1_vectors),\n",
    "                                                              np.nan_to_num(question2_vectors))]\n",
    "\n",
    "    data['jaccard_distance_{}'.format(transformation_name)] = [jaccard(x, y) for (x, y) in zip(np.nan_to_num(question1_vectors),\n",
    "                                                              np.nan_to_num(question2_vectors))]\n",
    "\n",
    "    data['canberra_distance_{}'.format(transformation_name)] = [canberra(x, y) for (x, y) in zip(np.nan_to_num(question1_vectors),\n",
    "                                                              np.nan_to_num(question2_vectors))]\n",
    "\n",
    "    data['euclidean_distance_{}'.format(transformation_name)] = [euclidean(x, y) for (x, y) in zip(np.nan_to_num(question1_vectors),\n",
    "                                                              np.nan_to_num(question2_vectors))]\n",
    "\n",
    "    data['minkowski_distance_{}'.format(transformation_name)] = [minkowski(x, y, 3) for (x, y) in zip(np.nan_to_num(question1_vectors),\n",
    "                                                              np.nan_to_num(question2_vectors))]\n",
    "\n",
    "    data['braycurtis_distance_{}'.format(transformation_name)] = [braycurtis(x, y) for (x, y) in zip(np.nan_to_num(question1_vectors),\n",
    "    np.nan_to_num(question2_vectors))]\n",
    "    return data\n",
    "\n",
    "def get_moments(transformation_name, question1_vectors, question2_vectors):\n",
    "    data = pd.DataFrame()\n",
    "    data['skew_{}'.format(transformation_name)] = [skew(x) for x in np.nan_to_num(question1_vectors)]\n",
    "    data['skew_{}'.format(transformation_name)] = [skew(x) for x in np.nan_to_num(question2_vectors)]\n",
    "    data['kur_{}'.format(transformation_name)] = [kurtosis(x) for x in np.nan_to_num(question1_vectors)]\n",
    "    data['kur_{}'.format(transformation_name)] = [kurtosis(x) for x in np.nan_to_num(question2_vectors)]\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stemmer = snowball.SnowballStemmer('english')\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stopwords_eng = stopwords.words('english')\n",
    "words = re.compile(r\"\\w+\",re.I)\n",
    "\n",
    "model = KeyedVectors.load_word2vec_format('/media/w/1c392724-ecf3-4615-8f3c-79368ec36380/DS Projects/Kaggle/Quora/data/embeddings/GoogleNews-vectors-negative300.bin',                            \n",
    "                                             binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "src = '/media/w/1c392724-ecf3-4615-8f3c-79368ec36380/DS Projects/Kaggle/Quora/scripts/features/'\n",
    "\n",
    "dftr2 = pd.read_csv('df_train_lemmatfullcleanSTEMMED.csv')\n",
    "dfte2 = pd.read_csv('df_test_lemmatfullcleanSTEMMED.csv')\n",
    "\n",
    "df_full = pd.concat((dftr2, dfte2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF done. (2750086, 3777937)\n",
      "NMF ready.\n"
     ]
    }
   ],
   "source": [
    "t = time.time()\n",
    "tf = TfidfVectorizer(stop_words = 'english', min_df = 3, max_df = 0.99, ngram_range = (1,3))\n",
    "tf.fit(df_full.question1 + df_full.question2)\n",
    "tr_tfidf = tf.transform(df_full.question1 + df_full.question2)\n",
    "print('TF-IDF done.', tr_tfidf.shape)\n",
    "\n",
    "nmf = NMF(n_components = 100)\n",
    "nmf.fit(tr_tfidf)\n",
    "print('NMF ready.')\n",
    "q1_tfidf = tf.transform(dftr2.question1)\n",
    "q2_tfidf = tf.transform(dftr2.question2)\n",
    "tr_nmf_q1 = nmf.transform(q1_tfidf)\n",
    "tr_nmf_q2 = nmf.transform(q2_tfidf)\n",
    "\n",
    "np.save('train_q1_NMF100comp_TFIDF3gram', tr_nmf_q1)\n",
    "np.save('train_q2_NMF100comp_TFIDF3gram', tr_nmf_q2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "q1_tfidf = tf.transform(dfte2.question1)\n",
    "q2_tfidf = tf.transform(dfte2.question2)\n",
    "te_nmf_q1 = nmf.transform(q1_tfidf)\n",
    "te_nmf_q2 = nmf.transform(q2_tfidf)\n",
    "\n",
    "np.save('test_q1_NMF100comp_TFIDF3gram', te_nmf_q1)\n",
    "np.save('test_q2_NMF100comp_TFIDF3gram', te_nmf_q2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "run_transforms('TFIDF_3grams_words_50dim', 50, 3, 'word', test = True)\n",
    "run_transforms('TFIDF_5grams_words_50dim', 50, 5, 'word', test = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Doc2Vec Moments\n",
    "\n",
    "fullclean_d2v_src = '/media/w/1c392724-ecf3-4615-8f3c-79368ec36380/DS Projects/Kaggle/Quora/data/features/lemmatized_fullclean/doc2vec/'\n",
    "lemmat_d2v_src = '/media/w/1c392724-ecf3-4615-8f3c-79368ec36380/DS Projects/Kaggle/Quora/data/features/lemmatized/doc2vec/'\n",
    "\n",
    "fc_q1_quora = np.load(lemmat_d2v_src + 'train_q1_doc2vec_vectors_trainquora.npy')\n",
    "fc_q2_quora = np.load(lemmat_d2v_src + 'train_q2_doc2vec_vectors_trainquora.npy')\n",
    "fc_q1_pre = np.load(lemmat_d2v_src + 'train_q1_doc2vec_vectors_pretrained.npy')\n",
    "fc_q2_pre = np.load(lemmat_d2v_src + 'train_q1_doc2vec_vectors_pretrained.npy')\n",
    "l_q1_pre = np.load(fullclean_d2v_src + 'train_q1_doc2vec_vectors_pretrained_fullcleanDF.npy')\n",
    "l_q2_pre = np.load(fullclean_d2v_src + 'train_q2_doc2vec_vectors_pretrained_fullcleanDF.npy')\n",
    "\n",
    "fc_q1_quora_te = np.load(lemmat_d2v_src + 'test_q1_doc2vec_vectors_trainquora.npy')\n",
    "fc_q2_quora_te = np.load(lemmat_d2v_src + 'test_q2_doc2vec_vectors_trainquora.npy')\n",
    "fc_q1_pre_te = np.load(lemmat_d2v_src + 'test_q1_doc2vec_vectors_pretrained.npy')\n",
    "fc_q2_pre_te = np.load(lemmat_d2v_src + 'test_q1_doc2vec_vectors_pretrained.npy')\n",
    "l_q1_pre_te = np.load(fullclean_d2v_src + 'test_q1_doc2vec_vectors_pretrained_fullcleanDF.npy')\n",
    "l_q2_pre_te = np.load(fullclean_d2v_src + 'test_q2_doc2vec_vectors_pretrained_fullcleanDF.npy')\n",
    "\n",
    "t = time.time()\n",
    "train_quora_lemmat = get_moments('doc2vec_quora_lemmat', fc_q1_quora, fc_q2_quora)\n",
    "train_pre_lemmat = get_moments('doc2vec_pretrained_lemmat', fc_q1_pre, fc_q2_pre)\n",
    "train_pre_clean = get_moments('doc2vec_pretrained_fullclean', l_q1_pre, l_q2_pre)\n",
    "moments_train = pd.concat([train_quora_lemmat, train_pre_lemmat, train_pre_clean], axis = 1)\n",
    "moments_train.to_csv('train_doc2vec_moments.csv', index = False)\n",
    "print('Moments for training data calculated. Time it took:', time.time() - t)\n",
    "\n",
    "t = time.time()\n",
    "test_quora_lemmat = get_moments('doc2vec_quora_lemmat', fc_q1_quora_te, fc_q2_quora_te)\n",
    "test_pre_lemmat = get_moments('doc2vec_pretested_lemmat', fc_q1_pre_te, fc_q2_pre_te)\n",
    "test_pre_clean = get_moments('doc2vec_pretested_fullclean', l_q1_pre_te, l_q2_pre_te)\n",
    "moments_test = pd.concat([test_quora_lemmat, test_pre_lemmat, test_pre_clean], axis = 1)\n",
    "moments_test.to_csv('test_doc2vec_moments.csv', index = False)\n",
    "print('Moments for test data calculated. Time it took:', time.time() - t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# WMD Distances\n",
    "\n",
    "t = time.time()\n",
    "\n",
    "src = '/media/w/1c392724-ecf3-4615-8f3c-79368ec36380/DS Projects/Kaggle/Quora/scripts/features/'\n",
    "trdf =  pd.read_csv(src + 'df_train_spacylemmat_fullclean.csv').iloc[:, :-1]\n",
    "tedf =  pd.read_csv(src + 'df_test_spacylemmat_fullclean.csv').iloc[:, 4:]\n",
    "trdf.fillna('NULL', inplace = True)\n",
    "tedf.fillna('NULL', inplace = True)\n",
    "\n",
    "trdfs =  pd.read_csv(src + 'df_train_lemmatfullcleanSTEMMED.csv').iloc[:, :-1]\n",
    "tedfs =  pd.read_csv(src + 'df_test_lemmatfullcleanSTEMMED.csv').iloc[:, 4:]\n",
    "trdfs.fillna('NULL', inplace = True)\n",
    "tedfs.fillna('NULL', inplace = True)\n",
    "print('Data loaded.')\n",
    "\n",
    "src2 = '/media/w/1c392724-ecf3-4615-8f3c-79368ec36380/DS Projects/Kaggle/Quora/data/embeddings/'\n",
    "model = gensim.models.KeyedVectors.load_word2vec_format(src2 + 'GoogleNews-vectors-negative300.bin', binary=True)\n",
    "norm_model = gensim.models.KeyedVectors.load_word2vec_format(src2 + 'GoogleNews-vectors-negative300.bin', binary=True)\n",
    "norm_model.init_sims(replace=True)\n",
    "print('Models loaded.')\n",
    "\n",
    "extract_WMD(train = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Compression & Edit Distances\n",
    "\n",
    "X = pd.DataFrame()\n",
    "X['compression_distance'] = dftr2.apply(lambda x: compression_distance(x['question1'], x['question2']),\n",
    "                                                axis = 1)               \n",
    "X.to_csv('train_LZMAcompression_distance.csv', index = False)\n",
    "\n",
    "X = pd.DataFrame()\n",
    "X['compression_distance'] = dfte2.apply(lambda x: compression_distance(x['question1'], x['question2']),\n",
    "                                                axis = 1)               \n",
    "X.to_csv('test_LZMAcompression_distance.csv', index = False)\n",
    "\n",
    "src = '/media/w/1c392724-ecf3-4615-8f3c-79368ec36380/DS Projects/Kaggle/Quora/scripts/features/'\n",
    "trdf =  pd.read_csv(src + 'df_train_spacylemmat_fullclean.csv').iloc[:, :-1]\n",
    "tedf =  pd.read_csv(src + 'df_test_spacylemmat_fullclean.csv').iloc[:, 4:]\n",
    "trdf.fillna('NULL', inplace = True)\n",
    "tedf.fillna('NULL', inplace = True)\n",
    "\n",
    "trdfs =  pd.read_csv(src + 'df_train_lemmatfullcleanSTEMMED.csv').iloc[:, :-1]\n",
    "tedfs =  pd.read_csv(src + 'df_test_lemmatfullcleanSTEMMED.csv').iloc[:, 4:]\n",
    "trdfs.fillna('NULL', inplace = True)\n",
    "tedfs.fillna('NULL', inplace = True)\n",
    "print('Data loaded.')\n",
    "\n",
    "\n",
    "Xtr = pd.DataFrame()\n",
    "Xtr['EDITdistance_fullclean'] = trdf.apply(lambda x: 1 - seq_matcher(None, x['question1'], x['question2']).ratio(),\n",
    "                                                axis = 1)   \n",
    "Xtr['EDITdistance_fullcleanSTEM'] = trdfs.apply(lambda x: 1 - seq_matcher(None, x['question1'], x['question2']).ratio(),\n",
    "                                                axis = 1)           \n",
    "print('Training set done.')\n",
    "\n",
    "Xte = pd.DataFrame()\n",
    "Xte['EDITdistance_fullclean'] = tedf.apply(lambda x: 1 - seq_matcher(None, x['question1'], x['question2']).ratio(),\n",
    "                                                axis = 1)     \n",
    "Xte['EDITdistance_fullcleanSTEM'] = tedfs.apply(lambda x: 1 - seq_matcher(None, x['question1'], x['question2']).ratio(),\n",
    "                                                axis = 1)     \n",
    "print('Test set done.')\n",
    "\n",
    "Xtr.to_csv('train_EDITdistance.csv', index = False)\n",
    "Xte.to_csv('test_EDITdistance.csv', index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
