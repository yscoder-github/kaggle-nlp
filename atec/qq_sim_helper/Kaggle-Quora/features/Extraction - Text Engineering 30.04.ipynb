{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import sklearn\n",
    "import random\n",
    "import itertools\n",
    "import nltk\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import gensim\n",
    "import lzma\n",
    "\n",
    "from sklearn.feature_extraction import FeatureHasher\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.manifold import LocallyLinearEmbedding, SpectralEmbedding, TSNE\n",
    "from sklearn.ensemble import RandomTreesEmbedding\n",
    "\n",
    "from textstat.textstat import textstat\n",
    "from gensim import corpora\n",
    "from gensim.models import KeyedVectors\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import *\n",
    "from nltk import word_tokenize, ngrams\n",
    "from nltk.tag import AffixTagger\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from scipy.spatial import distance\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import PCA, TruncatedSVD, LatentDirichletAllocation\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from scipy.spatial.distance import cosine, cityblock, jaccard, canberra, euclidean, minkowski, braycurtis\n",
    "\n",
    "seed = 1337"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def lowercase(df2):\n",
    "    df = df2.copy()\n",
    "    text_feats = df.select_dtypes(include=['object']).columns.values\n",
    "    for i in text_feats:\n",
    "        df[i] = df[i].str.lower()\n",
    "    return df\n",
    "\n",
    "def unidecode(df2):\n",
    "    df = df2.copy()\n",
    "    text_feats = df.select_dtypes(include=['object']).columns.values\n",
    "    for i in text_feats:\n",
    "        df[i] = df[i].str.encode('ascii', 'ignore')\n",
    "    return df\n",
    "\n",
    "def remove_nonalpha(df2):\n",
    "    df = df2.copy()\n",
    "    text_feats = df.select_dtypes(include=['object']).columns.values\n",
    "    for i in text_feats:\n",
    "        df[i] = df[i].str.replace('\\W+', ' ')\n",
    "    return df\n",
    "\n",
    "def repair_words(df2):\n",
    "    df = df2.copy()\n",
    "    text_feats = df.select_dtypes(include=['object']).columns.values\n",
    "    for i in text_feats:\n",
    "        df[i] = df[i].apply(lambda x: (''.join(''.join(s)[:2] for _, s in itertools.groupby(x))))\n",
    "    return df\n",
    "\n",
    "def concat_words(df2):\n",
    "    df = df2.copy()\n",
    "    text_feats = df.select_dtypes(include=['object']).columns.values\n",
    "    for i in text_feats:\n",
    "        df[i] = df[i].apply(lambda x: (' '.join(i for i in x)))\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "def tokenize(df2):\n",
    "    df = df2.copy()\n",
    "    text_feats = df.select_dtypes(include=['object']).columns.values\n",
    "    for i in text_feats:\n",
    "        df[i] = df[i].apply(lambda x: word_tokenize(x))\n",
    "    return df\n",
    "\n",
    "def ngram(df2, n):\n",
    "    df = df2.copy()\n",
    "    text_feats = df.select_dtypes(include=['object']).columns.values\n",
    "    for i in text_feats:\n",
    "        df[i] = df[i].apply(lambda x: [i for i in ngrams(word_tokenize(x), n)])\n",
    "    return df\n",
    "\n",
    "def skipgram(df2, ngram_n, skip_n):\n",
    "    def random_sample(words_list, skip_n):\n",
    "        return [words_list[i] for i in sorted(random.sample(range(len(words_list)), skip_n))]\n",
    "    \n",
    "    df = df2.copy()\n",
    "    text_feats = df.select_dtypes(include=['object']).columns.values\n",
    "    for i in text_feats:\n",
    "        df[i] = df[i].apply(lambda x: [i for i in ngrams(word_tokenize(x), ngram_n)])\n",
    "        df[i] = df[i].apply(lambda x: random_sample(x, skip_n))\n",
    "    return df\n",
    "\n",
    "def chargram(df2, n):\n",
    "    def chargram_generate(string, n):\n",
    "        return [string[i:i+n] for i in range(len(string)-n+1)]\n",
    "    df = df2.copy()\n",
    "    text_feats = df.select_dtypes(include=['object']).columns.values\n",
    "    for i in text_feats:\n",
    "        df[i] = df[i].apply(lambda x: [i for i in chargram_generate(x, 3)])\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "def remove_stops(df2, stopwords):\n",
    "    df = df2.copy()\n",
    "    text_feats = df.select_dtypes(include=['object']).columns.values\n",
    "    for i in text_feats:\n",
    "        df[i] = df[i].apply(lambda x: [i for i in word_tokenize(x) if i not in stopwords])\n",
    "    return df\n",
    "\n",
    "def remove_extremes(df2, stopwords, min_count = 3, max_frequency = 0.75):\n",
    "    df = df2.copy()\n",
    "    text_feats = df.select_dtypes(include=['object']).columns.values\n",
    "    for i in text_feats:\n",
    "        df[i] = df[i].apply(lambda x: [i for i in word_tokenize(x) if i not in stopwords])\n",
    "    tokenized = []\n",
    "    for i in text_feats:\n",
    "        tokenized += df[i].tolist()\n",
    "    dictionary = corpora.Dictionary(tokenized)\n",
    "    dictionary.filter_extremes(no_below = min_count, no_above = max_frequency)\n",
    "    dictionary.compactify()\n",
    "    df = df2.copy()\n",
    "    for i in text_feats:\n",
    "        df[i] = df[i].apply(lambda x: [i for i in word_tokenize(x) if i not in stopwords and i not in \n",
    "                                      list(dictionary.token2id.keys())])\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "def chop(df2, n):\n",
    "    df = df2.copy()\n",
    "    text_feats = df.select_dtypes(include=['object']).columns.values\n",
    "    for i in text_feats:\n",
    "        df[i] = df[i].apply(lambda x: [i[:n] for i in word_tokenize(x)])\n",
    "    return df\n",
    "\n",
    "def stem(df2):\n",
    "    df = df2.copy()\n",
    "    text_feats = df.select_dtypes(include=['object']).columns.values\n",
    "    for i in text_feats:\n",
    "        df[i] = df[i].apply(lambda x: ' '.join([stemmer.stem(i) for i in word_tokenize(x)]))\n",
    "    return df\n",
    "\n",
    "def lemmat(df2):\n",
    "    df = df2.copy()\n",
    "    text_feats = df.select_dtypes(include=['object']).columns.values\n",
    "    for i in text_feats:\n",
    "        df[i] = df[i].apply(lambda x: ' '.join([lemmatizer.lemmatize(i) for i in word_tokenize(x)]))\n",
    "    return df\n",
    "\n",
    "def extract_entity(df2):\n",
    "    df = df2.copy()\n",
    "    text_feats = df.select_dtypes(include=['object']).columns.values\n",
    "    for i in text_feats:\n",
    "        df[i] = df[i].apply(lambda x: word_tokenize(x))\n",
    "        df[i] = df[i].apply(lambda x: nltk.pos_tag(x))\n",
    "        df[i] = df[i].apply(lambda x: [i[1:] for i in x])\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "def doc_features(df2):\n",
    "    df = df2.copy()\n",
    "    text_feats = df.select_dtypes(include=['object']).columns.values\n",
    "    for i, col in enumerate(text_feats):\n",
    "        df['num_characters_{}'.format(i)] = df[col].map(lambda x: len(str(x))) # length of sentence\n",
    "        df['num_words_{}'.format(i)] = df[col].map(lambda x: len(str(x).split())) # number of words\n",
    "        df['num_spaces_{}'.format(i)] = df[col].map(lambda x: x.count(' '))\n",
    "        df['num_alpha_{}'.format(i)] = df[col].apply(lambda x: sum(i.isalpha()for i in x))\n",
    "        df['num_nonalpha_{}'.format(i)] = df[col].apply(lambda x: sum(1-i.isalpha()for i in x))\n",
    "    return df\n",
    "\n",
    "def get_readability(df2):\n",
    "    df = df2.copy()\n",
    "    text_feats = df.select_dtypes(include=['object']).columns.values\n",
    "    for i, col in enumerate(text_feats):\n",
    "        df['flesch_reading_ease{}'.format(i)] = df[col].apply(lambda x: textstat.flesch_reading_ease(x))\n",
    "        df['smog_index{}'.format(i)] = df[col].apply(lambda x: textstat.smog_index(x))\n",
    "        df['flesch_kincaid_grade{}'.format(i)] = df[col].apply(lambda x: textstat.flesch_kincaid_grade(x))\n",
    "        df['coleman_liau_index{}'.format(i)] = df[col].apply(lambda x: textstat.coleman_liau_index(x))\n",
    "        df['automated_readability_index{}'.format(i)] = df[col].apply(lambda x: textstat.automated_readability_index(x))\n",
    "        df['dale_chall_readability_score{}'.format(i)] = df[col].apply(lambda x: textstat.dale_chall_readability_score(x))\n",
    "        df['difficult_words{}'.format(i)] = df[col].apply(lambda x: textstat.difficult_words(x))\n",
    "        df['linsear_write_formula{}'.format(i)] = df[col].apply(lambda x: textstat.linsear_write_formula(x))\n",
    "        df['gunning_fog{}'.format(i)] = df[col].apply(lambda x: textstat.gunning_fog(x))\n",
    "        df['text_standard{}'.format(i)] = df[col].apply(lambda x: textstat.text_standard(x))\n",
    "    return df\n",
    "\n",
    "def bag_of_words(df2):\n",
    "    df = df2.copy()\n",
    "    cv = CountVectorizer(stop_words = 'english', min_df = 1, max_df = 0.999)\n",
    "    bow = cv.fit_transform(df.question1 + df.question2)\n",
    "    return bow\n",
    "\n",
    "def tf_idf(df2):\n",
    "    df = df2.copy()\n",
    "    tf = TfidfVectorizer(stop_words = 'english', min_df = 1, max_df = 0.999)\n",
    "    tfidf = tf.fit_transform(df.question1 + df.question2)\n",
    "    return tfidf\n",
    "\n",
    "def LDA_text2(df2, ntopics):\n",
    "    cv = CountVectorizer(stop_words = 'english', min_df = 2, max_df = 0.99)\n",
    "    lda = LatentDirichletAllocation(ntopics, random_state = seed, n_jobs = 1)\n",
    "    df = df2.copy()\n",
    "    text_feats = df.select_dtypes(include=['object']).columns.values\n",
    "    cv.fit(df.question1 + df.question2)\n",
    "    bow = cv.transform(df.question1 + df.question2)\n",
    "    lda.fit(bow)\n",
    "    ldas = []\n",
    "    for i in text_feats:\n",
    "        bow_i = cv.transform(df[i])\n",
    "        ldas.append(lda.transform(bow_i))\n",
    "    return ldas\n",
    "\n",
    "def SVD_text(df2, ndims):\n",
    "    df = df2.copy()\n",
    "    cv = CountVectorizer(stop_words = 'english', min_df = 2, max_df = 0.99)\n",
    "    svd = TruncatedSVD(ndims, random_state = seed)\n",
    "    text_feats = df.select_dtypes(include=['object']).columns.values\n",
    "    cv.fit(df.question1 + df.question2)\n",
    "    bow = cv.transform(df.question1 + df.question2)\n",
    "    svd.fit(bow)\n",
    "    svds = []\n",
    "    for i in text_feats:\n",
    "        bow_i = cv.transform(df[i])\n",
    "        svd_i = svd.transform(bow_i)\n",
    "        svds.append(svd_i)\n",
    "    return svds\n",
    "\n",
    "def LSA_text(df2, ndims):\n",
    "    cv = CountVectorizer(stop_words = 'english', min_df = 2, max_df = 0.99)\n",
    "    svd = TruncatedSVD(ndims, random_state = 1337)\n",
    "    normalizer = Normalizer(copy = False)\n",
    "    df = df2.copy()\n",
    "    text_feats = df.select_dtypes(include=['object']).columns.values\n",
    "    cv.fit(df.question1 + df.question2)\n",
    "    bow = cv.transform(df.question1 + df.question2)\n",
    "    svd.fit(bow)\n",
    "    transformed_bow = svd.transform(bow)\n",
    "    normed_bow = normalizer.fit(transformed_bow)\n",
    "    svds = []\n",
    "    for i in text_feats:\n",
    "        bow_i = cv.transform(df[i])\n",
    "        svd_i = svd.transform(bow_i)\n",
    "        normed_i = normalizer.transform(svd_i)\n",
    "        svds.append(normed_i)\n",
    "    return svds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stemmer = snowball.SnowballStemmer('english')\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stopwords_eng = stopwords.words('english')\n",
    "words = re.compile(r\"\\w+\",re.I)\n",
    "\n",
    "model = KeyedVectors.load_word2vec_format('/media/w/1c392724-ecf3-4615-8f3c-79368ec36380/DS Projects/Kaggle/Quora/data/embeddings/GoogleNews-vectors-negative300.bin',                            \n",
    "                                             binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "src = '/media/w/1c392724-ecf3-4615-8f3c-79368ec36380/DS Projects/Kaggle/Quora/scripts/features/'\n",
    "\n",
    "dftr2 = pd.read_csv('df_train_lemmatfullcleanSTEMMED.csv')\n",
    "dfte2 = pd.read_csv('df_test_lemmatfullcleanSTEMMED.csv')\n",
    "\n",
    "df_full = pd.concat((dftr2, dfte2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "src = '/media/w/1c392724-ecf3-4615-8f3c-79368ec36380/DS Projects/Kaggle/Quora/scripts/features/'\n",
    "trdf =  pd.read_csv(src + 'df_train_spacylemmat_fullclean.csv').iloc[:, :-1]\n",
    "tedf =  pd.read_csv(src + 'df_test_spacylemmat_fullclean.csv').iloc[:, 4:]\n",
    "trdf.fillna('NULL', inplace = True)\n",
    "tedf.fillna('NULL', inplace = True)\n",
    "\n",
    "trdfs =  pd.read_csv(src + 'df_train_lemmatfullcleanSTEMMED.csv').iloc[:, :-1]\n",
    "tedfs =  pd.read_csv(src + 'df_test_lemmatfullcleanSTEMMED.csv').iloc[:, 4:]\n",
    "trdfs.fillna('NULL', inplace = True)\n",
    "tedfs.fillna('NULL', inplace = True)\n",
    "print('Data loaded.')\n",
    "\n",
    "\n",
    "Xtr = pd.DataFrame()\n",
    "Xtr['EDITdistance_fullclean'] = trdf.apply(lambda x: 1 - seq_matcher(None, x['question1'], x['question2']).ratio(),\n",
    "                                                axis = 1)   \n",
    "Xtr['EDITdistance_fullcleanSTEM'] = trdfs.apply(lambda x: 1 - seq_matcher(None, x['question1'], x['question2']).ratio(),\n",
    "                                                axis = 1)           \n",
    "print('Training set done.')\n",
    "\n",
    "Xte = pd.DataFrame()\n",
    "Xte['EDITdistance_fullclean'] = tedf.apply(lambda x: 1 - seq_matcher(None, x['question1'], x['question2']).ratio(),\n",
    "                                                axis = 1)     \n",
    "Xte['EDITdistance_fullcleanSTEM'] = tedfs.apply(lambda x: 1 - seq_matcher(None, x['question1'], x['question2']).ratio(),\n",
    "                                                axis = 1)     \n",
    "print('Test set done.')\n",
    "\n",
    "Xtr.to_csv('train_EDITdistance.csv', index = False)\n",
    "Xte.to_csv('test_EDITdistance.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>compression_distance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.310345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.322581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.366667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.300000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.285714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.222222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.424242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.285714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.472222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.454545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.347826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.266667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.318182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.433333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.264706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.388889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.360000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.472222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.185185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.307692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.428571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.296296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.585366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.457143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.533333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2345766</th>\n",
       "      <td>0.568182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2345767</th>\n",
       "      <td>0.379310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2345768</th>\n",
       "      <td>0.366667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2345769</th>\n",
       "      <td>0.451613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2345770</th>\n",
       "      <td>0.296296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2345771</th>\n",
       "      <td>0.259259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2345772</th>\n",
       "      <td>0.153846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2345773</th>\n",
       "      <td>0.347826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2345774</th>\n",
       "      <td>0.416667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2345775</th>\n",
       "      <td>0.357143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2345776</th>\n",
       "      <td>0.320000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2345777</th>\n",
       "      <td>0.416667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2345778</th>\n",
       "      <td>0.310345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2345779</th>\n",
       "      <td>0.392857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2345780</th>\n",
       "      <td>0.540541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2345781</th>\n",
       "      <td>0.230769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2345782</th>\n",
       "      <td>0.379310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2345783</th>\n",
       "      <td>0.561404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2345784</th>\n",
       "      <td>0.150000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2345785</th>\n",
       "      <td>0.440000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2345786</th>\n",
       "      <td>0.304348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2345787</th>\n",
       "      <td>0.466667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2345788</th>\n",
       "      <td>0.432432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2345789</th>\n",
       "      <td>0.473684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2345790</th>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2345791</th>\n",
       "      <td>0.407407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2345792</th>\n",
       "      <td>0.360000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2345793</th>\n",
       "      <td>0.321429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2345794</th>\n",
       "      <td>0.166667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2345795</th>\n",
       "      <td>0.346154</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2345796 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         compression_distance\n",
       "0                    0.310345\n",
       "1                    0.322581\n",
       "2                    0.366667\n",
       "3                    0.300000\n",
       "4                    0.285714\n",
       "5                    0.333333\n",
       "6                    0.222222\n",
       "7                    0.424242\n",
       "8                    0.285714\n",
       "9                    0.472222\n",
       "10                   0.200000\n",
       "11                   0.500000\n",
       "12                   0.454545\n",
       "13                   0.347826\n",
       "14                   0.266667\n",
       "15                   0.318182\n",
       "16                   0.433333\n",
       "17                   0.264706\n",
       "18                   0.388889\n",
       "19                   0.360000\n",
       "20                   0.500000\n",
       "21                   0.472222\n",
       "22                   0.185185\n",
       "23                   0.307692\n",
       "24                   0.428571\n",
       "25                   0.296296\n",
       "26                   0.585366\n",
       "27                   0.457143\n",
       "28                   0.333333\n",
       "29                   0.533333\n",
       "...                       ...\n",
       "2345766              0.568182\n",
       "2345767              0.379310\n",
       "2345768              0.366667\n",
       "2345769              0.451613\n",
       "2345770              0.296296\n",
       "2345771              0.259259\n",
       "2345772              0.153846\n",
       "2345773              0.347826\n",
       "2345774              0.416667\n",
       "2345775              0.357143\n",
       "2345776              0.320000\n",
       "2345777              0.416667\n",
       "2345778              0.310345\n",
       "2345779              0.392857\n",
       "2345780              0.540541\n",
       "2345781              0.230769\n",
       "2345782              0.379310\n",
       "2345783              0.561404\n",
       "2345784              0.150000\n",
       "2345785              0.440000\n",
       "2345786              0.304348\n",
       "2345787              0.466667\n",
       "2345788              0.432432\n",
       "2345789              0.473684\n",
       "2345790              0.500000\n",
       "2345791              0.407407\n",
       "2345792              0.360000\n",
       "2345793              0.321429\n",
       "2345794              0.166667\n",
       "2345795              0.346154\n",
       "\n",
       "[2345796 rows x 1 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def compression_distance(x,y,l_x=None,l_y=None):\n",
    "    if x==y:\n",
    "        return 0\n",
    "    x_b = x.encode('utf-8')\n",
    "    y_b = y.encode('utf-8')\n",
    "    if l_x is None:\n",
    "        l_x = len(lzma.compress(x_b))\n",
    "        l_y = len(lzma.compress(y_b))\n",
    "    l_xy = len(lzma.compress(x_b+y_b))\n",
    "    l_yx = len(lzma.compress(y_b+x_b))\n",
    "    dist = (min(l_xy,l_yx)-min(l_x,l_y))/max(l_x,l_y)\n",
    "    return dist\n",
    "\n",
    "X = pd.DataFrame()\n",
    "X['compression_distance'] = dftr2.apply(lambda x: compression_distance(x['question1'], x['question2']),\n",
    "                                                axis = 1)               \n",
    "X.to_csv('train_LZMAcompression_distance.csv', index = False)\n",
    "X\n",
    "\n",
    "X = pd.DataFrame()\n",
    "X['compression_distance'] = dfte2.apply(lambda x: compression_distance(x['question1'], x['question2']),\n",
    "                                                axis = 1)               \n",
    "X.to_csv('test_LZMAcompression_distance.csv', index = False)\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded.\n",
      "Models loaded.\n",
      "Extracting WMD test distances.\n",
      "WMD distances extracted.\n",
      "12495.259793043137\n"
     ]
    }
   ],
   "source": [
    "def split_traintest(l):\n",
    "    train = []\n",
    "    test = []\n",
    "    for i in l:\n",
    "        train.append(i[:dftr2.shape[0]])\n",
    "        test.append(i[dftr2.shape[0]:])\n",
    "    return train, test\n",
    "\n",
    "def norm_wmd(s1, s2):\n",
    "    s1 = str(s1).lower().split()\n",
    "    s2 = str(s2).lower().split()\n",
    "    stop_words = stopwords.words('english')\n",
    "    return norm_model.wmdistance(s1, s2)\n",
    "\n",
    "def wmd(s1, s2):\n",
    "    s1 = str(s1).lower().split()\n",
    "    s2 = str(s2).lower().split()\n",
    "    return model.wmdistance(s1, s2)\n",
    "\n",
    "def extract_WMD(train = True):\n",
    "    if train:\n",
    "        print('Extracting WMD train distances.')\n",
    "        tr_feats = pd.DataFrame()\n",
    "        tr_feats['wmd_clean'] = trdf.apply(lambda x: wmd(x['question1'], x['question2']), axis=1)\n",
    "        tr_feats['norm_wmd_clean'] = trdf.apply(lambda x: norm_wmd(x['question1'], x['question2']), axis=1)\n",
    "        tr_feats['wmd_cleanStemmed'] = trdfs.apply(lambda x: wmd(x['question1'], x['question2']), axis=1)\n",
    "        tr_feats['norm_wmd_cleanStemmed'] = trdfs.apply(lambda x: norm_wmd(x['question1'], x['question2']), axis=1)\n",
    "        tr_feats.to_csv('train_WMD_cleaned_stemmed.csv', index = False)\n",
    "        print('WMD distances extracted.')\n",
    "        print(time.time() - t)\n",
    "    else:\n",
    "        print('Extracting WMD test distances.')\n",
    "        te_feats = pd.DataFrame()\n",
    "        te_feats['wmd_clean'] = tedf.apply(lambda x: wmd(x['question1'], x['question2']), axis=1)\n",
    "        te_feats['norm_wmd_clean'] = tedf.apply(lambda x: norm_wmd(x['question1'], x['question2']), axis=1)\n",
    "        te_feats['wmd_cleanStemmed'] = tedfs.apply(lambda x: wmd(x['question1'], x['question2']), axis=1)\n",
    "        te_feats['norm_wmd_cleanStemmed'] = tedfs.apply(lambda x: norm_wmd(x['question1'], x['question2']), axis=1)\n",
    "        te_feats.to_csv('test_WMD_cleaned_stemmed.csv', index = False)\n",
    "        print('WMD distances extracted.')\n",
    "        print(time.time() - t)\n",
    "    return\n",
    "\n",
    "def get_distances(transformation_name, question1_vectors, question2_vectors):\n",
    "    data = pd.DataFrame()\n",
    "    data['cosine_distance_{}'.format(transformation_name)] = [cosine(x, y) for (x, y) in zip(np.nan_to_num(question1_vectors),\n",
    "                                                              np.nan_to_num(question2_vectors))]\n",
    "\n",
    "    data['cityblock_distance_{}'.format(transformation_name)] = [cityblock(x, y) for (x, y) in zip(np.nan_to_num(question1_vectors),\n",
    "                                                              np.nan_to_num(question2_vectors))]\n",
    "\n",
    "    data['jaccard_distance_{}'.format(transformation_name)] = [jaccard(x, y) for (x, y) in zip(np.nan_to_num(question1_vectors),\n",
    "                                                              np.nan_to_num(question2_vectors))]\n",
    "\n",
    "    data['canberra_distance_{}'.format(transformation_name)] = [canberra(x, y) for (x, y) in zip(np.nan_to_num(question1_vectors),\n",
    "                                                              np.nan_to_num(question2_vectors))]\n",
    "\n",
    "    data['euclidean_distance_{}'.format(transformation_name)] = [euclidean(x, y) for (x, y) in zip(np.nan_to_num(question1_vectors),\n",
    "                                                              np.nan_to_num(question2_vectors))]\n",
    "\n",
    "    data['minkowski_distance_{}'.format(transformation_name)] = [minkowski(x, y, 3) for (x, y) in zip(np.nan_to_num(question1_vectors),\n",
    "                                                              np.nan_to_num(question2_vectors))]\n",
    "\n",
    "    data['braycurtis_distance_{}'.format(transformation_name)] = [braycurtis(x, y) for (x, y) in zip(np.nan_to_num(question1_vectors),\n",
    "    np.nan_to_num(question2_vectors))]\n",
    "    return data\n",
    "\n",
    "def get_moments(transformation_name, question1_vectors, question2_vectors):\n",
    "    data = pd.DataFrame()\n",
    "    data['skew_{}'.format(transformation_name)] = [skew(x) for x in np.nan_to_num(question1_vectors)]\n",
    "    data['skew_{}'.format(transformation_name)] = [skew(x) for x in np.nan_to_num(question2_vectors)]\n",
    "    data['kur_{}'.format(transformation_name)] = [kurtosis(x) for x in np.nan_to_num(question1_vectors)]\n",
    "    data['kur_{}'.format(transformation_name)] = [kurtosis(x) for x in np.nan_to_num(question2_vectors)]\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "t = time.time()\n",
    "\n",
    "src = '/media/w/1c392724-ecf3-4615-8f3c-79368ec36380/DS Projects/Kaggle/Quora/scripts/features/'\n",
    "trdf =  pd.read_csv(src + 'df_train_spacylemmat_fullclean.csv').iloc[:, :-1]\n",
    "tedf =  pd.read_csv(src + 'df_test_spacylemmat_fullclean.csv').iloc[:, 4:]\n",
    "trdf.fillna('NULL', inplace = True)\n",
    "tedf.fillna('NULL', inplace = True)\n",
    "\n",
    "trdfs =  pd.read_csv(src + 'df_train_lemmatfullcleanSTEMMED.csv').iloc[:, :-1]\n",
    "tedfs =  pd.read_csv(src + 'df_test_lemmatfullcleanSTEMMED.csv').iloc[:, 4:]\n",
    "trdfs.fillna('NULL', inplace = True)\n",
    "tedfs.fillna('NULL', inplace = True)\n",
    "print('Data loaded.')\n",
    "\n",
    "src2 = '/media/w/1c392724-ecf3-4615-8f3c-79368ec36380/DS Projects/Kaggle/Quora/data/embeddings/'\n",
    "model = gensim.models.KeyedVectors.load_word2vec_format(src2 + 'GoogleNews-vectors-negative300.bin', binary=True)\n",
    "norm_model = gensim.models.KeyedVectors.load_word2vec_format(src2 + 'GoogleNews-vectors-negative300.bin', binary=True)\n",
    "norm_model.init_sims(replace=True)\n",
    "print('Models loaded.')\n",
    "\n",
    "extract_WMD(train = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def SVD_text_tfidf(df2, ndims, gram_range, analyze = 'word'):\n",
    "    df = df2.copy()\n",
    "    tf = TfidfVectorizer(stop_words = 'english', min_df = 1, max_df = 0.999, ngram_range = (1, gram_range),\n",
    "                        analyzer = analyze)\n",
    "    svd = TruncatedSVD(ndims, random_state = seed)\n",
    "    text_feats = df.select_dtypes(include=['object']).columns.values\n",
    "    tf.fit(df.question1 + df.question2)\n",
    "    bow = tf.transform(df.question1 + df.question2)\n",
    "    svd.fit(bow)\n",
    "    svds = []\n",
    "    for i in text_feats:\n",
    "        bow_i = tf.transform(df[i])\n",
    "        svd_i = svd.transform(bow_i)\n",
    "        svds.append(svd_i)\n",
    "    return svds\n",
    "\n",
    "def LSA_text_tfidf(df2, ndims, gram_range, analyze = 'word'):\n",
    "    tf = TfidfVectorizer(stop_words = 'english', min_df = 1, max_df = 0.999, ngram_range = (1, gram_range),\n",
    "                        analyzer = analyze)\n",
    "    svd = TruncatedSVD(ndims, random_state = 1337)\n",
    "    normalizer = Normalizer(copy = False)\n",
    "    df = df2.copy()\n",
    "    text_feats = df.select_dtypes(include=['object']).columns.values\n",
    "    tf.fit(df.question1 + df.question2)\n",
    "    bow = tf.transform(df.question1 + df.question2)\n",
    "    svd.fit(bow)\n",
    "    transformed_bow = svd.transform(bow)\n",
    "    normed_bow = normalizer.fit(transformed_bow)\n",
    "    svds = []\n",
    "    for i in text_feats:\n",
    "        bow_i = tf.transform(df[i])\n",
    "        svd_i = svd.transform(bow_i)\n",
    "        normed_i = normalizer.transform(svd_i)\n",
    "        svds.append(normed_i)\n",
    "    return svds\n",
    "\n",
    "def run_transforms(transformation_name, ndims, gram_range, analyze, test = False):\n",
    "    lsa_dff = LSA_text_tfidf(df_full, ndims, gram_range, analyze)\n",
    "    svd_dff = SVD_text_tfidf(df_full, ndims, gram_range, analyze)\n",
    "    lsa_tr, lsa_te = split_traintest(lsa_dff)\n",
    "    svd_tr, svd_te = split_traintest(svd_dff)\n",
    "    tr_lsa_dist = get_distances('train_LSA_{}'.format(transformation_name), lsa_tr[0], lsa_tr[1])\n",
    "    tr_svd_dist = get_distances('train_SVD_{}'.format(transformation_name), svd_tr[0], svd_tr[1])\n",
    "    tr_lsa_dist.to_csv('train_LSA_{}.csv'.format(transformation_name), index = False)\n",
    "    tr_svd_dist.to_csv('train_SVD_{}'.format(transformation_name), index = False)\n",
    "    if test:\n",
    "        te_lsa_dist = get_distances('test_LSA_{}'.format(transformation_name), lsa_te[0], lsa_te[1])\n",
    "        te_svd_dist = get_distances('test_SVD_{}'.format(transformation_name), svd_te[0], svd_te[1])\n",
    "        te_lsa_dist.to_csv('test_LSA_{}.csv'.format(transformation_name), index = False)\n",
    "        te_svd_dist.to_csv('test_SVD_{}'.format(transformation_name), index = False)\n",
    "    return\n",
    "\n",
    "run_transforms('TFIDF_3grams_words_50dim', 50, 3, 'word', test = True)\n",
    "run_transforms('TFIDF_5grams_words_50dim', 50, 5, 'word', test = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
